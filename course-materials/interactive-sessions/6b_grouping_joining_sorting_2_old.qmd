---
title: "Interactive Session 6B"
subtitle: "Grouping, Joining, and Sorting (Part II)"
editor_options: 
  chunk_output_type: console
jupyter: eds217_2025
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: show
---

::: {style="width: 60%; margin: auto;"}
![](images/grouping_filtering.jpeg)
:::

:::{.gray-text .center-text}
*A large crowd of pandas dressed in various colors. The pandas are trying to sort themselves based on their clothing color.* [MidJourney 5](https://www.midjourney.com/jobs/a0c058d4-d21b-469c-a71a-9e38e5715ab2?index=0)
:::

## Getting Started

Before we begin our interactive session, please follow these steps to set up your Jupyter Notebook:

1. Open JupyterLab and create a new notebook:
   - Click on the `+` button in the top left corner
   - Select `Python 3.11.0` from the Notebook options

2. Rename your notebook:
   - Right-click on the `Untitled.ipynb` tab
   - Select "Rename"
   - Name your notebook with the format: `Session_XY_Topic.ipynb`
     (Replace X with the day number and Y with the session number)

3. Add a title cell:
   - In the first cell of your notebook, change the cell type to "Markdown"
   - Add the following content (replace the placeholders with the actual information):

```markdown
# Day X: Session Y - [Session Topic]

[Link to session webpage]

Date: [Current Date]
```

4. Add a code cell:
   - Below the title cell, add a new cell
   - Ensure it's set as a "Code" cell
   - This will be where you start writing your Python code for the session

5. Throughout the session:
   - Take notes in Markdown cells
   - Copy or write code in Code cells
   - Run cells to test your code
   - Ask questions if you need clarification

:::{.callout-caution}
Remember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).
:::

Let's begin our interactive session!
<hr>

## Learning Objectives

By the end of this session, you will be able to:

1. Concatenate DataFrames from separate files
2. Handle mismatched column names when merging data
3. Join datasets using a common key
4. Understand different types of joins (inner, outer, left, right)
5. Learn about more advanced sorting methods for organizing data

## Introduction to Merging and Joining

In environmental data science, it's common to work with data from multiple sources. We often need to combine these datasets for comprehensive analysis. Pandas provides powerful tools for merging and joining data.

## Concatenating DataFrames

Concatenation is useful when you have multiple datasets with the same structure, such as data collected over different time periods or from different locations.

Let's start with an example of concatenating weather data from two stations:

<div class="example"> 
✏️ Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true

import pandas as pd
import numpy as np
```

Let's create a couple of simple dataframes containing date, temperature, and humidity data:

<div class="example"> 
✏️ Try it. Copy the cell below to your notebook and run it.
</div>

```{python}
# Create sample DataFrames
station1_data = pd.DataFrame({
    'date': pd.date_range(start='2023-01-01', periods=5),
    'temperature': [20, 22, 21, 23, 22],
    'humidity': [50, 48, 52, 51, 49]
})

station2_data = pd.DataFrame({
    'date': pd.date_range(start='2023-01-06', periods=5),
    'temperature': [19, 20, 22, 21, 23],
    'humidity': [53, 50, 47, 49, 48]
})

print("Station 1 Data:")
print(station1_data)
print("\nStation 2 Data:")
print(station2_data)
```

Use `pd.concat()` to combine these two dataframes into a single one:
<div class="example"> 
✏️ Try it. Add the cell below to your notebook and run it.
</div>

```{python}
# Concatenate the DataFrames
combined_data = pd.concat([station1_data, station2_data], ignore_index=True)

print("\nCombined Data:")
print(combined_data)
```

In this example, we used `pd.concat()` to combine data from two stations. The `ignore_index=True` parameter resets the index of the combined DataFrame. The most common use of concatenation is when downloading or working with multiple datafiles containing the same data and structure (e.g. annual rainfall data, monthly weather data, etc...)

## Handling Mismatched Column Names

Sometimes, datasets may have consistent data types but mismatched column names. Let's look at an example where we have temperature data with different date column names.

<div class="example"> 
✏️ Try it. Copy the cell below to your notebook and run it.
</div>

```{python}
#| echo: true

# Create sample DataFrames with mismatched column names
df1 = pd.DataFrame({
    'date': pd.date_range(start='2023-01-01', periods=3),
    'temp_celsius': [20, 22, 21]
})

df2 = pd.DataFrame({
    'DATE': pd.date_range(start='2023-01-04', periods=3),
    'temp_celsius': [23, 22, 24]
})

print("DataFrame 1:")
print(df1)
print("\nDataFrame 2:")
print(df2)
```

#### Renaming columns to facilitate concatenation
<div class="example"> 
✏️ Try it. Add the cell below to your notebook and run it.
</div>

```{python}
# Rename columns before concatenating
df2 = df2.rename(columns={'DATE': 'date'})

# Concatenate the DataFrames
combined_df = pd.concat([df1, df2], ignore_index=True)

print("\nCombined Data:")
print(combined_df)
```

Here, we used the `rename()` method to standardize the column names before concatenation. 

## Joining Data with a Common Key

Often, we need to combine datasets that share a common key, such as a field site ID or a species name. Pandas provides several join operations: inner, outer, left, and right.

Let's look at an example where we join species occurrence data with site information:

<div class="example"> 
✏️ Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true

# Create sample DataFrames
species_data = pd.DataFrame({
    'site_id': ['A', 'B', 'A', 'C', 'B'],
    'species': ['Oak', 'Pine', 'Maple', 'Birch', 'Oak'],
    'count': [10, 5, 8, 3, 7]
})

site_info = pd.DataFrame({
    'site_id': ['A', 'B', 'C', 'D'],
    'elevation': [100, 200, 150, 180],
    'soil_type': ['loam', 'clay', 'sandy', 'loam']
})

print("Species Data:")
print(species_data)
print("\nSite Info:")
print(site_info)
```

#### Performing an inner join to combine specieis and site data
```{python}
# Perform an inner join
merged_data = pd.merge(species_data, site_info, on='site_id', how='inner')

print("\nMerged Data (Inner Join):")
print(merged_data)
```

In this example, we used `pd.merge()` to perform an inner join on the 'site_id' column. This join keeps only the rows where the site_id exists in both DataFrames.

### Different Types of Joins

Let's explore other types of joins:

- **Left Join**: Keeps all rows from the left DataFrame (species_data) and matching rows from the right DataFrame (site_info).
- **Right Join**: Keeps all rows from the right DataFrame (site_info) and matching rows from the left DataFrame (species_data).
- **Outer Join**: Keeps all rows from both DataFrames, filling in NaN where there's no match.

## Understanding Join Types in Depth

When working with environmental data, understanding different join types is crucial. Let's explore these in more detail, with a focus on their applications in environmental data science.

### Inner Join: The Most Common Join

:::{.callout-note}
Inner join is the most frequently used join type in data analysis, including environmental studies. It returns only the rows that have matching values in both DataFrames.
:::

<div class="example"> 
✏️ Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true

# Example of Inner Join
species_observations = pd.DataFrame({
    'site_id': ['A', 'B', 'C', 'D', 'E'],
    'species': ['Oak', 'Pine', 'Maple', 'Birch', 'Cedar']
})

site_characteristics = pd.DataFrame({
    'site_id': ['A', 'B', 'C', 'F', 'G'],
    'soil_type': ['Loam', 'Clay', 'Sandy', 'Silt', 'Peat']
})

inner_join_result = pd.merge(species_observations, site_characteristics, on='site_id', how='inner')
print("Inner Join Result:")
print(inner_join_result)
```

**When to use Inner Join:**
- When you want to analyze only the data points that have complete information in both datasets.
- For example, studying the relationship between soil types and tree species, but only for sites where you have both pieces of information.

### Left Join: Keeping All Data from the Primary Dataset

Left join keeps all rows from the left (primary) DataFrame and matching rows from the right DataFrame. It's useful when you want to retain all records from your main dataset.

<div class="example"> 
✏️ Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true

# Example of Left Join
left_join_result = pd.merge(species_observations, site_characteristics, on='site_id', how='left')
print("Left Join Result:")
print(left_join_result)
```

**When to use Left Join:**
- When you want to keep all observations from your primary dataset, even if some don't have matching information in the secondary dataset.
- For example, retaining all species observations, even for sites without soil type data.

### Right Join: Less Common, but Useful in Specific Cases

Right join is less common but can be useful in certain scenarios. It keeps all rows from the right DataFrame and matching rows from the left.

<div class="example"> 
✏️ Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true

# Example of Right Join
right_join_result = pd.merge(species_observations, site_characteristics, on='site_id', how='right')
print("Right Join Result:")
print(right_join_result)
```

**When to use Right Join:**
- When you want to ensure all records from a secondary dataset are included.
- For example, including all soil type data, even for sites where no species were observed.

### Outer Join: Combining All Data

Outer join combines all rows from both DataFrames, filling in NaN where there's no match. It's useful for getting a complete view of all available data.

<div class="example"> 
✏️ Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true

# Example of Outer Join
outer_join_result = pd.merge(species_observations, site_characteristics, on='site_id', how='outer')
print("Outer Join Result:")
print(outer_join_result)
```

**When to use Outer Join:**
- When you want to see all available data from both datasets, identifying where information might be missing.
- Useful for data exploration or when preparing a comprehensive dataset for further analysis.

### Key Considerations for Choosing Join Types

1. **Data Completeness**: Inner join for complete cases, outer join for all available data.
2. **Analysis Requirements**: Consider what missing data means for your analysis.
3. **Data Quality**: Outer joins can help identify data gaps or collection inconsistencies.
4. **Primary Focus**: Use left join when focusing on a primary dataset but want additional information where available.

### Common Scenarios in Environmental Data Science

1. **Species Distribution Studies**: Left join species observations (primary data) with environmental factors.
2. **Comprehensive Ecosystem Analysis**: Outer join multiple datasets (species, soil, climate) to get a full picture.
3. **Habitat Suitability Models**: Inner join species presence with complete environmental data.
4. **Long-term Monitoring**: Left join time-series observations with site metadata, keeping all temporal data.

Remember, the choice of join can significantly impact your analysis results. Always consider the nature of your data and the questions you're trying to answer when selecting a join type.

#### Practice Exercise

You have two datasets: one containing river water quality measurements and another with information about sampling sites. Combine these datasets to create a comprehensive view of water quality across different sites.

```{python}
#| echo: true

# Water quality data
water_quality = pd.DataFrame({
    'site_code': ['RV01', 'RV02', 'RV01', 'RV03', 'RV02'],
    'date': pd.date_range(start='2023-01-01', periods=5),
    'pH': [7.2, 7.8, 7.3, 6.9, 7.7],
    'dissolved_oxygen': [8.5, 7.9, 8.3, 7.2, 8.1]
})

# Site information
site_info = pd.DataFrame({
    'site_code': ['RV01', 'RV02', 'RV03', 'RV04'],
    'river_name': ['Blue River', 'Green Creek', 'Red Stream', 'Yellow Brook'],
    'watershed': ['Alpine', 'Lowland', 'Lowland', 'Alpine']
})
```

```{python}

# Your task:
# 1. Merge the water quality data with site information
# 2. Calculate the average pH and dissolved oxygen for each river
# 3. Display the results sorted by average pH

# Your code here
```

```{python}
#| echo: false
#| eval: false

# Solution
# 1. Merge the datasets
merged_data = pd.merge(water_quality, site_info, on='site_code', how='left')

# 2. Calculate averages
avg_quality = merged_data.groupby('river_name').agg({
    'pH': 'mean',
    'dissolved_oxygen': 'mean'
}).reset_index()

# 3. Sort and display results
result = avg_quality.sort_values('pH')
print(result)
```

## Advanced Sorting Techniques

After merging datasets, you often need to sort the resulting DataFrame in more complex ways. Let's explore some advanced sorting techniques that are particularly useful when working with merged data.

### Sorting with a Custom Key or Function

Sometimes you need to sort based on a computed value rather than the raw data in a column. You can use the `key` parameter in `sort_values()` to apply a function to the column before sorting:

```{python}
#| echo: true

import pandas as pd
import numpy as np

# Create a sample DataFrame
df = pd.DataFrame({
    'site': ['A', 'B', 'C', 'D'],
    'species': ['Oak', 'Pine', 'Maple', 'Birch'],
    'height': [5.2, 12.7, 8.1, 9.9]
})

print("Original DataFrame:")
print(df)

# Sort by absolute difference from 10 meters
df_sorted = df.sort_values('height', key=lambda x: abs(x - 10))
print("\nSorted by proximity to 10 meters:")
print(df_sorted)
```

This is useful when you want to sort based on a specific criterion, like proximity to a target value.

### Sorting with NaN Values

When dealing with merged data, you might encounter NaN values. By default, Pandas sorts NaN values at the end:

```{python}
#| echo: true

# Create a DataFrame with NaN values
df_nan = pd.DataFrame({
    'site': ['A', 'B', 'C', 'D', 'E'],
    'value': [5, np.nan, 3, np.nan, 1]
})

print("DataFrame with NaN values:")
print(df_nan)

# Sort with NaN values
df_sorted_nan = df_nan.sort_values('value', na_position='first')
print("\nSorted with NaN values first:")
print(df_sorted_nan)
```

You can control where NaN values appear using the `na_position` parameter.

### Sorting a MultiIndex DataFrame

When you perform certain merge operations or create pivot tables, you might end up with a MultiIndex DataFrame. Sorting these requires specifying which level to sort on:

```{python}
#| echo: true

# Create a MultiIndex DataFrame
multi_df = pd.DataFrame({
    'site': ['A', 'A', 'B', 'B'],
    'year': [2022, 2023, 2022, 2023],
    'value': [10, 12, 9, 11]
}).set_index(['site', 'year'])

print("MultiIndex DataFrame:")
print(multi_df)

# Sort by the 'site' level of the index
sorted_multi_df = multi_df.sort_index(level='site', ascending=False)
print("\nSorted by 'site' level:")
print(sorted_multi_df)

# Sort by the 'value' column within each 'site'
sorted_multi_df_values = multi_df.sort_values('value', ascending=False)
print("\nSorted by 'value' within each 'site':")
print(sorted_multi_df_values)
```

This is particularly useful when you've merged data that creates hierarchical relationships in your DataFrame.

### Stable Sorting

When sorting by multiple columns, you might want to preserve the relative order of rows with the same values. This is called stable sorting:

```{python}
#| echo: true

# Create a DataFrame with duplicate values
df_duplicate = pd.DataFrame({
    'category': ['A', 'B', 'A', 'B', 'A'],
    'subcategory': [1, 1, 2, 2, 3],
    'value': [10, 20, 30, 40, 50]
})

print("DataFrame with duplicate values:")
print(df_duplicate)

# Perform a stable sort
df_stable_sort = df_duplicate.sort_values(['category', 'subcategory'], kind='mergesort')
print("\nStable sort result:")
print(df_stable_sort)
```

Using `kind='mergesort'` ensures a stable sort, which can be important when preserving the original order matters within your sorted groups.

These advanced sorting techniques will help you effectively organize and analyze complex datasets resulting from merge operations or other data manipulations.

## Key Points

- Concatenation (pd.concat()) is useful for combining datasets with the same structure.
- Always check and standardize column names before merging datasets.
- Choose the appropriate join type (inner, left, right, outer) based on your analysis needs.
- Merging on a common key allows you to combine information from different sources.

## Resources

- [Pandas Merging Documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html)
- [Pandas Concat Documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html)

Don't forget to check out our EDS 217 Cheatsheet on [Merging and Joining](../cheatsheets/data_merging.qmd) for quick reference!

::: {.center-text .body-text-xl .teal-text}
End interactive session 6B
:::
