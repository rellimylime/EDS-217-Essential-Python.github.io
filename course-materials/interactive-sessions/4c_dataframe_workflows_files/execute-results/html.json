{
  "hash": "1e0f388ff0453fb8ed0f3ac1f926c1ad",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Interactive Session 4C\"\nsubtitle: \"🔄 Quick Tour: 9-Step Data Science Workflow\"\njupyter: eds217_2025\nformat: \n    html:\n        toc: true\n        toc-depth: 3\n        code-fold: show\n---\n\n\n\n\n## Quick Tour: The Data Science Workflow\n\n**Every data science project follows the same systematic approach.** Today we'll take a **quick tour** through all 9 steps using simple examples. This gives you the big picture before we dive deeper in coming days!\n\n\n\n\n```{mermaid}\nflowchart LR\n    A[\"1. Import<br/>📂\"] --> B[\"2. Explore<br/>🔍\"] --> C[\"3. Clean<br/>🧹\"]\n    C --> D[\"4. Filter<br/>🎯\"] --> E[\"5. Sort<br/>📊\"]\n    E --> F[\"6. Transform<br/>🔄\"] --> G[\"7. Group<br/>👥\"]\n    G --> H[\"8. Aggregate<br/>📈\"] --> I[\"9. Visualize<br/>📊\"]\n\n    style A fill:#e1f5fe\n    style B fill:#e8f5e8\n    style C fill:#fff3e0\n    style D fill:#f3e5f5\n    style E fill:#e0f2f1\n    style F fill:#fce4ec\n    style G fill:#e8eaf6\n    style H fill:#f1f8e9\n    style I fill:#fff8e1\n```\n\n\n\n\n:::{.callout-important title=\"Session Goals\"}\n**Today**: Quick overview of all 9 steps with simple examples  \n**Days 5-7**: Deep dive into specific steps with real data  \n**End-of-day**: Practice the complete workflow yourself!\n:::\n\n## Getting Started\n\nCreate a new notebook called `Session_4C_Workflow_Tour.ipynb` and follow along as we take a quick tour through the data science workflow!\n\n## Setting up our environment\n\nFirst, let's import the libraries we know from previous sessions:\n\n::: {#b6ebd890 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n:::{.callout-note title=\"Libraries We're Using\"}\n- **pandas** (`pd`): For working with DataFrames (from Sessions 4a & 4b)\n- **matplotlib** (`plt`): For creating charts and graphs (from Session 4c)\n:::\n\n## Workflow Progress Tracker\n\nAs we work through each step, we'll track our progress through the complete data science workflow:\n\n:::{.callout-note title=\"Workflow Progress\"}\n**Ocean Temperature Analysis - Workflow Steps**\n\n☐ **Step 1: Import** - Load our ocean data  \n☐ **Step 2: Explore** - Discover what we have  \n☐ **Step 3: Clean** - Fix any problems  \n☐ **Step 4: Filter** - Focus on specific data  \n☐ **Step 5: Sort** - Find temperature patterns  \n☐ **Step 6: Transform** - Create new insights  \n☐ **Step 7: Group** - Organize by categories  \n☐ **Step 8: Aggregate** - Calculate summaries  \n☐ **Step 9: Visualize** - Present our results\n\n**Goal**: Complete systematic data science analysis\n:::\n\n## 📂 Step 1: Import Data\n\n✅ **Workflow Step 1**: Getting our data into Python\n\nThe first step in **every** data science project is getting your data into Python. We'll use `pd.read_csv()` - the same function you learned in Session 4a!\n\n::: {#54f40ace .cell execution_count=2}\n``` {.python .cell-code}\n# Step 1: Import our ocean temperature data\ndf = pd.read_csv('ocean_temperatures_simple.csv')\nprint(\"✅ Step 1 Complete: Data imported successfully!\")\nprint(f\"📊 Loaded {len(df)} rows of ocean temperature data\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n✅ Step 1 Complete: Data imported successfully!\n📊 Loaded 30 rows of ocean temperature data\n```\n:::\n:::\n\n\n:::{.callout-important title=\"Real Data Science Connection\"}\n**Professional data scientists** start every project the same way - importing data! Whether it's:\n- Climate data from NASA\n- User behavior from websites  \n- Financial data from banks\n- Your final project data\n\n**You always start with**: `pd.read_csv()` or similar import functions\n:::\n\n**🔮 Coming Attractions**: Later in the course, you'll learn to import Excel files, JSON data, and even data from databases!\n\n## 🔍 Step 2: Explore Data\n\n✅ **Workflow Step 2**: Discovering what we have\n\nBefore we can analyze data, we need to **understand** what we're working with. Let's use the exploration methods you learned in Session 4a:\n\n::: {#5a8bd694 .cell execution_count=3}\n``` {.python .cell-code}\nprint(\"🔍 EXPLORING OUR OCEAN DATA\")\nprint(\"=\" * 40)\n\nprint(\"\\n📋 First few rows:\")\nprint(df.head())\n\nprint(f\"\\n📊 DataFrame info:\")\ndf.info()\n\nprint(f\"\\n📈 Summary statistics:\")\nprint(df.describe())\n\nprint(\"\\n❓ Missing values check:\")\nprint(df.isna().sum())\n\nprint(\"\\n✅ Step 2 Complete: We now understand our data!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n🔍 EXPLORING OUR OCEAN DATA\n========================================\n\n📋 First few rows:\n         date  location  temperature  salinity  depth\n0  2021-01-15   Pacific         18.5      34.2     50\n1  2021-01-15  Atlantic         22.1      35.1      0\n2  2021-01-15    Indian         20.0      34.8    100\n3  2021-01-15  Southern         15.2      34.0    200\n4  2021-01-15    Arctic         12.1      33.5     50\n\n📊 DataFrame info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 30 entries, 0 to 29\nData columns (total 5 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   date         30 non-null     object \n 1   location     30 non-null     object \n 2   temperature  30 non-null     float64\n 3   salinity     30 non-null     float64\n 4   depth        30 non-null     int64  \ndtypes: float64(2), int64(1), object(2)\nmemory usage: 1.3+ KB\n\n📈 Summary statistics:\n       temperature   salinity       depth\ncount    30.000000  30.000000   30.000000\nmean     19.283333  34.433333   80.000000\nstd       4.621843   0.616068   68.982756\nmin      11.500000  33.300000    0.000000\n25%      15.400000  34.025000   50.000000\n50%      19.200000  34.350000   50.000000\n75%      22.925000  35.000000  100.000000\nmax      27.100000  35.400000  200.000000\n\n❓ Missing values check:\ndate           0\nlocation       0\ntemperature    0\nsalinity       0\ndepth          0\ndtype: int64\n\n✅ Step 2 Complete: We now understand our data!\n```\n:::\n:::\n\n\n:::{.callout-important title=\"What We Discovered\"}\nOur ocean dataset contains:\n- **5 oceans**: Pacific, Atlantic, Indian, Southern, Arctic\n- **Temperature measurements** in degrees Celsius  \n- **Salinity measurements** (salt content)\n- **Depth measurements** where samples were taken\n- **30 total measurements** across different dates\n\n**This is exactly what real data scientists do first!**\n:::\n\n**🔮 Coming Attractions**: In Day 5, you'll learn advanced exploration techniques like correlation analysis and custom statistics!\n\n## 🧹 Step 3: Clean Data\n\n✅ **Workflow Step 3**: Fixing problems in our data\n\nGood news! Our ocean data is already clean - no missing values to worry about. But let's see what cleaning looks like:\n\n::: {#83a81674 .cell execution_count=4}\n``` {.python .cell-code}\nprint(\"🧹 CLEANING OUR DATA\")\nprint(\"=\" * 30)\n\n# Check for missing values (we already did this, but let's confirm)\nmissing_data = df.isna().sum()\nprint(\"Missing values per column:\")\nprint(missing_data)\n\nif missing_data.sum() == 0:\n    print(\"\\n🎉 Great news! Our data is already clean!\")\n    df_cleaned = df.copy()  # Make a copy for consistency\nelse:\n    print(f\"\\n🔧 Cleaning needed...\")\n    df_cleaned = df.dropna().copy()  # Remove rows with missing values\n    print(f\"Removed {len(df) - len(df_cleaned)} rows with missing data\")\n\nprint(f\"\\n✅ Step 3 Complete: Clean dataset with {len(df_cleaned)} rows ready for analysis!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n🧹 CLEANING OUR DATA\n==============================\nMissing values per column:\ndate           0\nlocation       0\ntemperature    0\nsalinity       0\ndepth          0\ndtype: int64\n\n🎉 Great news! Our data is already clean!\n\n✅ Step 3 Complete: Clean dataset with 30 rows ready for analysis!\n```\n:::\n:::\n\n\n:::{.callout-important title=\"Why Cleaning Matters\"}\nIn real data science projects, you'll spend 50-80% of your time cleaning data! Common problems include:\n- **Missing values** (what we just checked for)\n- **Duplicate entries**\n- **Incorrect data types**\n- **Outliers and errors**\n\n**The `.dropna()` method** you just learned will be one of your most-used tools!\n:::\n\n**🔮 Coming Attractions**: In Day 5, you'll learn advanced cleaning techniques like handling duplicates and fixing data types!\n\n## 🎯 Step 4: Filter Data\n\n✅ **Workflow Step 4**: Focusing on what matters for our question\n\nLet's focus on specific data to answer our research question. We'll filter for just the Pacific Ocean to start:\n\n::: {#84923f16 .cell execution_count=5}\n``` {.python .cell-code}\nprint(\"🎯 FILTERING OUR DATA\")\nprint(\"=\" * 30)\n\n# Filter for just Pacific Ocean data (using boolean indexing from Session 4b)\npacific_data = df_cleaned[df_cleaned['location'] == 'Pacific']\n\nprint(\"Pacific Ocean measurements:\")\nprint(pacific_data)\nprint(f\"\\n📊 Found {len(pacific_data)} Pacific Ocean measurements\")\n\n# Let's also look at summer data (June measurements)\nsummer_data = df_cleaned[df_cleaned['date'].str.contains('06-15')]\nprint(f\"\\n🌞 Summer measurements (June): {len(summer_data)} rows\")\n\nprint(\"\\n✅ Step 4 Complete: Focused on specific data for our analysis!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n🎯 FILTERING OUR DATA\n==============================\nPacific Ocean measurements:\n          date location  temperature  salinity  depth\n0   2021-01-15  Pacific         18.5      34.2     50\n5   2021-06-15  Pacific         24.3      34.5     50\n10  2021-12-15  Pacific         19.1      34.3     50\n15  2022-01-15  Pacific         18.2      34.1     50\n20  2022-06-15  Pacific         24.8      34.6     50\n25  2022-12-15  Pacific         19.3      34.4     50\n\n📊 Found 6 Pacific Ocean measurements\n\n🌞 Summer measurements (June): 10 rows\n\n✅ Step 4 Complete: Focused on specific data for our analysis!\n```\n:::\n:::\n\n\n:::{.callout-important title=\"Filtering in Real Data Science\"}\n**Professional data scientists** constantly filter data to focus on specific questions:\n- Netflix: *\"Show me viewing data for comedy movies\"*\n- Climate research: *\"Focus on temperature data from Arctic regions\"*  \n- Your final project: *\"Filter for data relevant to your specific question\"*\n\n**The boolean indexing** you just used (`df[df['column'] == value]`) is a fundamental skill!\n:::\n\n**🔮 Coming Attractions**: In Day 5, you'll learn complex filtering with multiple conditions using `&` and `|` operators!\n\n## 📊 Step 5: Sort Data\n\n✅ **Workflow Step 5**: Organizing data to find patterns\n\nSorting helps us find the highest and lowest values. Let's find the warmest and coldest ocean measurements:\n\n::: {#7062eba7 .cell execution_count=6}\n``` {.python .cell-code}\nprint(\"📊 SORTING OUR DATA\")\nprint(\"=\" * 30)\n\n# Sort by temperature (warmest first) using .sort_values() from Session 4b\nsorted_by_temp = df_cleaned.sort_values('temperature', ascending=False)\n\nprint(\"🔥 TOP 5 WARMEST measurements:\")\nprint(sorted_by_temp[['location', 'temperature', 'date']].head())\n\nprint(\"\\n🧊 TOP 5 COLDEST measurements:\")\nprint(sorted_by_temp[['location', 'temperature', 'date']].tail())\n\nprint(\"\\n✅ Step 5 Complete: Found temperature patterns by sorting!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n📊 SORTING OUR DATA\n==============================\n🔥 TOP 5 WARMEST measurements:\n    location  temperature        date\n21  Atlantic         27.1  2022-06-15\n6   Atlantic         26.8  2021-06-15\n22    Indian         25.5  2022-06-15\n7     Indian         25.2  2021-06-15\n20   Pacific         24.8  2022-06-15\n\n🧊 TOP 5 COLDEST measurements:\n   location  temperature        date\n9    Arctic         14.8  2021-06-15\n4    Arctic         12.1  2021-01-15\n19   Arctic         11.9  2022-01-15\n29   Arctic         11.8  2022-12-15\n14   Arctic         11.5  2021-12-15\n\n✅ Step 5 Complete: Found temperature patterns by sorting!\n```\n:::\n:::\n\n\n:::{.callout-important title=\"Insights from Sorting\"}\n**What we discovered**:\n- 🔥 **Warmest**: Atlantic Ocean (27.1°C in summer)\n- 🧊 **Coldest**: Arctic Ocean (11.5°C in winter)\n- 📈 **Pattern**: Atlantic and Pacific are warmest, Arctic is coldest\n\n**This is how data scientists find patterns** - sorting reveals extremes and trends!\n:::\n\n**🔮 Coming Attractions**: In Day 6, you'll learn to sort by multiple columns and create hierarchical sorting!\n\n## 🔄 Step 6: Transform Data\n\n✅ **Workflow Step 6**: Creating new insights from existing data\n\nLet's create new information that will help answer our research question:\n\n::: {#72f57c9a .cell execution_count=7}\n``` {.python .cell-code}\nprint(\"🔄 TRANSFORMING OUR DATA\")\nprint(\"=\" * 35)\n\n# Create a new column: temperature in Fahrenheit (simple math from Session 4b)\ndf_cleaned['temperature_f'] = (df_cleaned['temperature'] * 9/5) + 32\n\n# Create a season category based on the date\ndef get_season(date_str):\n    if '01-15' in date_str or '12-15' in date_str:\n        return 'Winter'\n    elif '06-15' in date_str:\n        return 'Summer'\n    else:\n        return 'Other'\n\ndf_cleaned['season'] = df_cleaned['date'].apply(get_season)\n\n# Show our new columns\nprint(\"New columns added:\")\nprint(df_cleaned[['location', 'temperature', 'temperature_f', 'season']].head())\n\nprint(f\"\\n📈 Original columns: 5\")\nprint(f\"📈 After transformation: {len(df_cleaned.columns)} columns\")\nprint(\"\\n✅ Step 6 Complete: Created new insights from our data!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n🔄 TRANSFORMING OUR DATA\n===================================\nNew columns added:\n   location  temperature  temperature_f  season\n0   Pacific         18.5          65.30  Winter\n1  Atlantic         22.1          71.78  Winter\n2    Indian         20.0          68.00  Winter\n3  Southern         15.2          59.36  Winter\n4    Arctic         12.1          53.78  Winter\n\n📈 Original columns: 5\n📈 After transformation: 7 columns\n\n✅ Step 6 Complete: Created new insights from our data!\n```\n:::\n:::\n\n\n:::{.callout-important title=\"Why Transform Data?\"}\n**Transformation creates new insights**:\n- 🌡️ **Temperature in Fahrenheit**: Makes data accessible to different audiences\n- 🗓️ **Season categories**: Helps us compare winter vs summer patterns\n- 📊 **New calculations**: Ratios, categories, derived metrics\n\n**Real data scientists** spend lots of time creating these \"feature engineering\" transformations!\n:::\n\n**🔮 Coming Attractions**: In Day 6, you'll learn advanced transformations and custom functions!\n\n\n## 👥 Step 7: Group Data\n\n✅ **Workflow Step 7**: Organizing by categories to find patterns\n\nNow we'll group our data by categories to compare different oceans and seasons:\n\n::: {#58ac921c .cell execution_count=8}\n``` {.python .cell-code}\nprint(\"👥 GROUPING OUR DATA\")\nprint(\"=\" * 30)\n\n# Group by ocean location (using .groupby() from Session 4b)\nby_ocean = df_cleaned.groupby('location')\n\nprint(\"📊 Number of measurements per ocean:\")\nprint(by_ocean.size())\n\n# Group by season to compare winter vs summer\nby_season = df_cleaned.groupby('season')\n\nprint(\"\\n📊 Number of measurements per season:\")\nprint(by_season.size())\n\nprint(\"\\n✅ Step 7 Complete: Data organized by meaningful categories!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n👥 GROUPING OUR DATA\n==============================\n📊 Number of measurements per ocean:\nlocation\nArctic      6\nAtlantic    6\nIndian      6\nPacific     6\nSouthern    6\ndtype: int64\n\n📊 Number of measurements per season:\nseason\nSummer    10\nWinter    20\ndtype: int64\n\n✅ Step 7 Complete: Data organized by meaningful categories!\n```\n:::\n:::\n\n\n:::{.callout-important title=\"Why Group Data?\"}\n**Grouping reveals patterns**:\n- 🌊 **By ocean**: Compare Pacific vs Atlantic vs Arctic temperatures\n- 🗓️ **By season**: See how temperatures change winter to summer  \n- 📊 **By categories**: Any categorical variable can create groups\n\n**This sets up the next step** - calculating summary statistics for each group!\n:::\n\n**🔮 Coming Attractions**: In Day 6, you'll learn to group by multiple columns simultaneously and create complex hierarchical groups!\n\n## 📈 Step 8: Aggregate Data\n\n✅ **Workflow Step 8**: Calculating summary statistics to answer our question\n\nNow for the exciting part - let's calculate averages to answer \"Which ocean is warmest?\"\n\n::: {#3bf4d4bf .cell execution_count=9}\n``` {.python .cell-code}\nprint(\"📈 AGGREGATING OUR DATA\")\nprint(\"=\" * 35)\n\n# Calculate average temperature by ocean (using .mean() from Session 4b)\navg_temp_by_ocean = df_cleaned.groupby('location')['temperature'].mean()\n\nprint(\"🌊 AVERAGE TEMPERATURE BY OCEAN:\")\nprint(avg_temp_by_ocean.sort_values(ascending=False))\n\n# Calculate average temperature by season\navg_temp_by_season = df_cleaned.groupby('season')['temperature'].mean()\n\nprint(\"\\n🗓️ AVERAGE TEMPERATURE BY SEASON:\")\nprint(avg_temp_by_season.sort_values(ascending=False))\n\n# Answer our research question!\nwarmest_ocean = avg_temp_by_ocean.max()\nwarmest_ocean_name = avg_temp_by_ocean.idxmax()\n\nprint(f\"\\n🎉 RESEARCH QUESTION ANSWERED!\")\nprint(f\"🏆 Warmest ocean: {warmest_ocean_name} ({warmest_ocean:.1f}°C)\")\n\nprint(\"\\n✅ Step 8 Complete: Found the answer through aggregation!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n📈 AGGREGATING OUR DATA\n===================================\n🌊 AVERAGE TEMPERATURE BY OCEAN:\nlocation\nAtlantic    24.083333\nIndian      22.133333\nPacific     20.700000\nSouthern    16.616667\nArctic      12.883333\nName: temperature, dtype: float64\n\n🗓️ AVERAGE TEMPERATURE BY SEASON:\nseason\nSummer    22.100\nWinter    17.875\nName: temperature, dtype: float64\n\n🎉 RESEARCH QUESTION ANSWERED!\n🏆 Warmest ocean: Atlantic (24.1°C)\n\n✅ Step 8 Complete: Found the answer through aggregation!\n```\n:::\n:::\n\n\n:::{.callout-important title=\"Key Discovery!\"}\n**Our Research Results**:\n- 🥇 **Warmest Ocean**: Atlantic (24.1°C average)\n- 🥈 **Second Warmest**: Pacific (20.7°C average)  \n- 🥉 **Coldest**: Arctic (12.9°C average)\n- 🌞 **Summer is warmer** than winter (as expected!)\n\n**This is exactly how real data science works** - use aggregation to answer research questions!\n:::\n\n**🔮 Coming Attractions**: In Day 6, you'll learn advanced aggregation functions like `.agg()` to calculate multiple statistics at once!\n\n## 📊 Step 9: Visualize Data\n\n✅ **Workflow Step 9**: Telling our story with charts\n\nThe final step is creating a chart to communicate our findings clearly:\n\n::: {#b67f5414 .cell execution_count=10}\n``` {.python .cell-code}\nprint(\"📊 VISUALIZING OUR RESULTS\")\nprint(\"=\" * 35)\n\n# Create average temperature data for plotting\navg_temps = df_cleaned.groupby('location')['temperature'].mean().sort_values(ascending=False)\n\n# Create a bar chart (using matplotlib from Session 4c)\nplt.figure(figsize=(10, 6))\navg_temps.plot(kind='bar', color=['red', 'orange', 'blue', 'green', 'purple'])\nplt.title('🌊 Average Ocean Temperatures: Research Results', fontsize=16, fontweight='bold')\nplt.xlabel('Ocean Location', fontsize=12)\nplt.ylabel('Average Temperature (°C)', fontsize=12)\nplt.xticks(rotation=45)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\n\n# Add our research conclusion to the plot\nplt.figtext(0.5, 0.02, '🏆 Research Conclusion: Atlantic Ocean is the warmest on average!', \n            ha='center', fontsize=12, fontweight='bold')\n\nplt.show()\n\nprint(\"\\n✅ Step 9 Complete: Story told through visualization!\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/bs/x9tn9jz91cv6hb3q6p4djbmw0000gn/T/ipykernel_96207/1715597996.py:15: UserWarning: Glyph 127754 (\\N{WATER WAVE}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/Users/kellycaylor/mambaforge/envs/eds217_2025/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 127942 (\\N{TROPHY}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/Users/kellycaylor/mambaforge/envs/eds217_2025/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 127754 (\\N{WATER WAVE}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n📊 VISUALIZING OUR RESULTS\n===================================\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](4c_dataframe_workflows_files/figure-html/cell-11-output-3.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n✅ Step 9 Complete: Story told through visualization!\n```\n:::\n:::\n\n\n:::{.callout-important title=\"🎉 CONGRATULATIONS! 🎉\"}\n**You just completed your first full data science project!**\n\n**🌊 Research Question**: *Which ocean has the warmest average temperatures?*  \n**📊 Answer**: **Atlantic Ocean** (24.1°C average)  \n**🏆 Method**: Complete 9-step data science workflow!\n\n**You've completed the full workflow - you're officially a data scientist!** 🎓\n:::\n\n## 🎯 What You Accomplished Today\n\n### **✅ Complete Workflow Mastery**\nYou just used the **exact same process** that professional data scientists use every day:\n\n1. ✅ **Imported** real ocean temperature data\n2. ✅ **Explored** to understand what you had  \n3. ✅ **Cleaned** (lucky us - data was already clean!)\n4. ✅ **Filtered** to focus on specific questions\n5. ✅ **Sorted** to find temperature patterns\n6. ✅ **Transformed** data to create new insights  \n7. ✅ **Grouped** by meaningful categories\n8. ✅ **Aggregated** to calculate summary statistics\n9. ✅ **Visualized** results with a professional chart\n\n### **🔮 Your Data Science Journey Continues**\n\n**Next Week - Individual Step Mastery**:\n- **Day 5**: Advanced filtering and transformation techniques\n- **Day 6**: Complex grouping and aggregation methods  \n- **Day 7**: Professional data visualization with seaborn\n\n**Your Final Project**: Use this **exact 9-step workflow** to answer your own research question!\n\n### **🔄 The Workflow You Can Always Apply**\n\nWhenever you encounter a new dataset or research question, systematically work through these 9 steps:\n1. Import → 2. Explore → 3. Clean → 4. Filter → 5. Sort → 6. Transform → 7. Group → 8. Aggregate → 9. Visualize\n\n**This is your systematic approach to data science success!** 🎯\n\n::: {.center-text .body-text-xl .teal-text}\n🎉 End interactive session 4C - You're now a data scientist! 🎉\n:::\n\n",
    "supporting": [
      "4c_dataframe_workflows_files"
    ],
    "filters": [],
    "includes": {}
  }
}