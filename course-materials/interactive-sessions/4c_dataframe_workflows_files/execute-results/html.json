{
  "hash": "1e0f388ff0453fb8ed0f3ac1f926c1ad",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Interactive Session 4C\"\nsubtitle: \"ğŸ”„ Quick Tour: 9-Step Data Science Workflow\"\njupyter: eds217_2025\nformat: \n    html:\n        toc: true\n        toc-depth: 3\n        code-fold: show\n---\n\n\n\n\n## Quick Tour: The Data Science Workflow\n\n**Every data science project follows the same systematic approach.** Today we'll take a **quick tour** through all 9 steps using simple examples. This gives you the big picture before we dive deeper in coming days!\n\n\n\n\n```{mermaid}\nflowchart LR\n    A[\"1. Import<br/>ğŸ“‚\"] --> B[\"2. Explore<br/>ğŸ”\"] --> C[\"3. Clean<br/>ğŸ§¹\"]\n    C --> D[\"4. Filter<br/>ğŸ¯\"] --> E[\"5. Sort<br/>ğŸ“Š\"]\n    E --> F[\"6. Transform<br/>ğŸ”„\"] --> G[\"7. Group<br/>ğŸ‘¥\"]\n    G --> H[\"8. Aggregate<br/>ğŸ“ˆ\"] --> I[\"9. Visualize<br/>ğŸ“Š\"]\n\n    style A fill:#e1f5fe\n    style B fill:#e8f5e8\n    style C fill:#fff3e0\n    style D fill:#f3e5f5\n    style E fill:#e0f2f1\n    style F fill:#fce4ec\n    style G fill:#e8eaf6\n    style H fill:#f1f8e9\n    style I fill:#fff8e1\n```\n\n\n\n\n:::{.callout-important title=\"Session Goals\"}\n**Today**: Quick overview of all 9 steps with simple examples  \n**Days 5-7**: Deep dive into specific steps with real data  \n**End-of-day**: Practice the complete workflow yourself!\n:::\n\n## Getting Started\n\nCreate a new notebook called `Session_4C_Workflow_Tour.ipynb` and follow along as we take a quick tour through the data science workflow!\n\n## Setting up our environment\n\nFirst, let's import the libraries we know from previous sessions:\n\n::: {#b6ebd890 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n:::{.callout-note title=\"Libraries We're Using\"}\n- **pandas** (`pd`): For working with DataFrames (from Sessions 4a & 4b)\n- **matplotlib** (`plt`): For creating charts and graphs (from Session 4c)\n:::\n\n## Workflow Progress Tracker\n\nAs we work through each step, we'll track our progress through the complete data science workflow:\n\n:::{.callout-note title=\"Workflow Progress\"}\n**Ocean Temperature Analysis - Workflow Steps**\n\nâ˜ **Step 1: Import** - Load our ocean data  \nâ˜ **Step 2: Explore** - Discover what we have  \nâ˜ **Step 3: Clean** - Fix any problems  \nâ˜ **Step 4: Filter** - Focus on specific data  \nâ˜ **Step 5: Sort** - Find temperature patterns  \nâ˜ **Step 6: Transform** - Create new insights  \nâ˜ **Step 7: Group** - Organize by categories  \nâ˜ **Step 8: Aggregate** - Calculate summaries  \nâ˜ **Step 9: Visualize** - Present our results\n\n**Goal**: Complete systematic data science analysis\n:::\n\n## ğŸ“‚ Step 1: Import Data\n\nâœ… **Workflow Step 1**: Getting our data into Python\n\nThe first step in **every** data science project is getting your data into Python. We'll use `pd.read_csv()` - the same function you learned in Session 4a!\n\n::: {#54f40ace .cell execution_count=2}\n``` {.python .cell-code}\n# Step 1: Import our ocean temperature data\ndf = pd.read_csv('ocean_temperatures_simple.csv')\nprint(\"âœ… Step 1 Complete: Data imported successfully!\")\nprint(f\"ğŸ“Š Loaded {len(df)} rows of ocean temperature data\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nâœ… Step 1 Complete: Data imported successfully!\nğŸ“Š Loaded 30 rows of ocean temperature data\n```\n:::\n:::\n\n\n:::{.callout-important title=\"Real Data Science Connection\"}\n**Professional data scientists** start every project the same way - importing data! Whether it's:\n- Climate data from NASA\n- User behavior from websites  \n- Financial data from banks\n- Your final project data\n\n**You always start with**: `pd.read_csv()` or similar import functions\n:::\n\n**ğŸ”® Coming Attractions**: Later in the course, you'll learn to import Excel files, JSON data, and even data from databases!\n\n## ğŸ” Step 2: Explore Data\n\nâœ… **Workflow Step 2**: Discovering what we have\n\nBefore we can analyze data, we need to **understand** what we're working with. Let's use the exploration methods you learned in Session 4a:\n\n::: {#5a8bd694 .cell execution_count=3}\n``` {.python .cell-code}\nprint(\"ğŸ” EXPLORING OUR OCEAN DATA\")\nprint(\"=\" * 40)\n\nprint(\"\\nğŸ“‹ First few rows:\")\nprint(df.head())\n\nprint(f\"\\nğŸ“Š DataFrame info:\")\ndf.info()\n\nprint(f\"\\nğŸ“ˆ Summary statistics:\")\nprint(df.describe())\n\nprint(\"\\nâ“ Missing values check:\")\nprint(df.isna().sum())\n\nprint(\"\\nâœ… Step 2 Complete: We now understand our data!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nğŸ” EXPLORING OUR OCEAN DATA\n========================================\n\nğŸ“‹ First few rows:\n         date  location  temperature  salinity  depth\n0  2021-01-15   Pacific         18.5      34.2     50\n1  2021-01-15  Atlantic         22.1      35.1      0\n2  2021-01-15    Indian         20.0      34.8    100\n3  2021-01-15  Southern         15.2      34.0    200\n4  2021-01-15    Arctic         12.1      33.5     50\n\nğŸ“Š DataFrame info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 30 entries, 0 to 29\nData columns (total 5 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   date         30 non-null     object \n 1   location     30 non-null     object \n 2   temperature  30 non-null     float64\n 3   salinity     30 non-null     float64\n 4   depth        30 non-null     int64  \ndtypes: float64(2), int64(1), object(2)\nmemory usage: 1.3+ KB\n\nğŸ“ˆ Summary statistics:\n       temperature   salinity       depth\ncount    30.000000  30.000000   30.000000\nmean     19.283333  34.433333   80.000000\nstd       4.621843   0.616068   68.982756\nmin      11.500000  33.300000    0.000000\n25%      15.400000  34.025000   50.000000\n50%      19.200000  34.350000   50.000000\n75%      22.925000  35.000000  100.000000\nmax      27.100000  35.400000  200.000000\n\nâ“ Missing values check:\ndate           0\nlocation       0\ntemperature    0\nsalinity       0\ndepth          0\ndtype: int64\n\nâœ… Step 2 Complete: We now understand our data!\n```\n:::\n:::\n\n\n:::{.callout-important title=\"What We Discovered\"}\nOur ocean dataset contains:\n- **5 oceans**: Pacific, Atlantic, Indian, Southern, Arctic\n- **Temperature measurements** in degrees Celsius  \n- **Salinity measurements** (salt content)\n- **Depth measurements** where samples were taken\n- **30 total measurements** across different dates\n\n**This is exactly what real data scientists do first!**\n:::\n\n**ğŸ”® Coming Attractions**: In Day 5, you'll learn advanced exploration techniques like correlation analysis and custom statistics!\n\n## ğŸ§¹ Step 3: Clean Data\n\nâœ… **Workflow Step 3**: Fixing problems in our data\n\nGood news! Our ocean data is already clean - no missing values to worry about. But let's see what cleaning looks like:\n\n::: {#83a81674 .cell execution_count=4}\n``` {.python .cell-code}\nprint(\"ğŸ§¹ CLEANING OUR DATA\")\nprint(\"=\" * 30)\n\n# Check for missing values (we already did this, but let's confirm)\nmissing_data = df.isna().sum()\nprint(\"Missing values per column:\")\nprint(missing_data)\n\nif missing_data.sum() == 0:\n    print(\"\\nğŸ‰ Great news! Our data is already clean!\")\n    df_cleaned = df.copy()  # Make a copy for consistency\nelse:\n    print(f\"\\nğŸ”§ Cleaning needed...\")\n    df_cleaned = df.dropna().copy()  # Remove rows with missing values\n    print(f\"Removed {len(df) - len(df_cleaned)} rows with missing data\")\n\nprint(f\"\\nâœ… Step 3 Complete: Clean dataset with {len(df_cleaned)} rows ready for analysis!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nğŸ§¹ CLEANING OUR DATA\n==============================\nMissing values per column:\ndate           0\nlocation       0\ntemperature    0\nsalinity       0\ndepth          0\ndtype: int64\n\nğŸ‰ Great news! Our data is already clean!\n\nâœ… Step 3 Complete: Clean dataset with 30 rows ready for analysis!\n```\n:::\n:::\n\n\n:::{.callout-important title=\"Why Cleaning Matters\"}\nIn real data science projects, you'll spend 50-80% of your time cleaning data! Common problems include:\n- **Missing values** (what we just checked for)\n- **Duplicate entries**\n- **Incorrect data types**\n- **Outliers and errors**\n\n**The `.dropna()` method** you just learned will be one of your most-used tools!\n:::\n\n**ğŸ”® Coming Attractions**: In Day 5, you'll learn advanced cleaning techniques like handling duplicates and fixing data types!\n\n## ğŸ¯ Step 4: Filter Data\n\nâœ… **Workflow Step 4**: Focusing on what matters for our question\n\nLet's focus on specific data to answer our research question. We'll filter for just the Pacific Ocean to start:\n\n::: {#84923f16 .cell execution_count=5}\n``` {.python .cell-code}\nprint(\"ğŸ¯ FILTERING OUR DATA\")\nprint(\"=\" * 30)\n\n# Filter for just Pacific Ocean data (using boolean indexing from Session 4b)\npacific_data = df_cleaned[df_cleaned['location'] == 'Pacific']\n\nprint(\"Pacific Ocean measurements:\")\nprint(pacific_data)\nprint(f\"\\nğŸ“Š Found {len(pacific_data)} Pacific Ocean measurements\")\n\n# Let's also look at summer data (June measurements)\nsummer_data = df_cleaned[df_cleaned['date'].str.contains('06-15')]\nprint(f\"\\nğŸŒ Summer measurements (June): {len(summer_data)} rows\")\n\nprint(\"\\nâœ… Step 4 Complete: Focused on specific data for our analysis!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nğŸ¯ FILTERING OUR DATA\n==============================\nPacific Ocean measurements:\n          date location  temperature  salinity  depth\n0   2021-01-15  Pacific         18.5      34.2     50\n5   2021-06-15  Pacific         24.3      34.5     50\n10  2021-12-15  Pacific         19.1      34.3     50\n15  2022-01-15  Pacific         18.2      34.1     50\n20  2022-06-15  Pacific         24.8      34.6     50\n25  2022-12-15  Pacific         19.3      34.4     50\n\nğŸ“Š Found 6 Pacific Ocean measurements\n\nğŸŒ Summer measurements (June): 10 rows\n\nâœ… Step 4 Complete: Focused on specific data for our analysis!\n```\n:::\n:::\n\n\n:::{.callout-important title=\"Filtering in Real Data Science\"}\n**Professional data scientists** constantly filter data to focus on specific questions:\n- Netflix: *\"Show me viewing data for comedy movies\"*\n- Climate research: *\"Focus on temperature data from Arctic regions\"*  \n- Your final project: *\"Filter for data relevant to your specific question\"*\n\n**The boolean indexing** you just used (`df[df['column'] == value]`) is a fundamental skill!\n:::\n\n**ğŸ”® Coming Attractions**: In Day 5, you'll learn complex filtering with multiple conditions using `&` and `|` operators!\n\n## ğŸ“Š Step 5: Sort Data\n\nâœ… **Workflow Step 5**: Organizing data to find patterns\n\nSorting helps us find the highest and lowest values. Let's find the warmest and coldest ocean measurements:\n\n::: {#7062eba7 .cell execution_count=6}\n``` {.python .cell-code}\nprint(\"ğŸ“Š SORTING OUR DATA\")\nprint(\"=\" * 30)\n\n# Sort by temperature (warmest first) using .sort_values() from Session 4b\nsorted_by_temp = df_cleaned.sort_values('temperature', ascending=False)\n\nprint(\"ğŸ”¥ TOP 5 WARMEST measurements:\")\nprint(sorted_by_temp[['location', 'temperature', 'date']].head())\n\nprint(\"\\nğŸ§Š TOP 5 COLDEST measurements:\")\nprint(sorted_by_temp[['location', 'temperature', 'date']].tail())\n\nprint(\"\\nâœ… Step 5 Complete: Found temperature patterns by sorting!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nğŸ“Š SORTING OUR DATA\n==============================\nğŸ”¥ TOP 5 WARMEST measurements:\n    location  temperature        date\n21  Atlantic         27.1  2022-06-15\n6   Atlantic         26.8  2021-06-15\n22    Indian         25.5  2022-06-15\n7     Indian         25.2  2021-06-15\n20   Pacific         24.8  2022-06-15\n\nğŸ§Š TOP 5 COLDEST measurements:\n   location  temperature        date\n9    Arctic         14.8  2021-06-15\n4    Arctic         12.1  2021-01-15\n19   Arctic         11.9  2022-01-15\n29   Arctic         11.8  2022-12-15\n14   Arctic         11.5  2021-12-15\n\nâœ… Step 5 Complete: Found temperature patterns by sorting!\n```\n:::\n:::\n\n\n:::{.callout-important title=\"Insights from Sorting\"}\n**What we discovered**:\n- ğŸ”¥ **Warmest**: Atlantic Ocean (27.1Â°C in summer)\n- ğŸ§Š **Coldest**: Arctic Ocean (11.5Â°C in winter)\n- ğŸ“ˆ **Pattern**: Atlantic and Pacific are warmest, Arctic is coldest\n\n**This is how data scientists find patterns** - sorting reveals extremes and trends!\n:::\n\n**ğŸ”® Coming Attractions**: In Day 6, you'll learn to sort by multiple columns and create hierarchical sorting!\n\n## ğŸ”„ Step 6: Transform Data\n\nâœ… **Workflow Step 6**: Creating new insights from existing data\n\nLet's create new information that will help answer our research question:\n\n::: {#72f57c9a .cell execution_count=7}\n``` {.python .cell-code}\nprint(\"ğŸ”„ TRANSFORMING OUR DATA\")\nprint(\"=\" * 35)\n\n# Create a new column: temperature in Fahrenheit (simple math from Session 4b)\ndf_cleaned['temperature_f'] = (df_cleaned['temperature'] * 9/5) + 32\n\n# Create a season category based on the date\ndef get_season(date_str):\n    if '01-15' in date_str or '12-15' in date_str:\n        return 'Winter'\n    elif '06-15' in date_str:\n        return 'Summer'\n    else:\n        return 'Other'\n\ndf_cleaned['season'] = df_cleaned['date'].apply(get_season)\n\n# Show our new columns\nprint(\"New columns added:\")\nprint(df_cleaned[['location', 'temperature', 'temperature_f', 'season']].head())\n\nprint(f\"\\nğŸ“ˆ Original columns: 5\")\nprint(f\"ğŸ“ˆ After transformation: {len(df_cleaned.columns)} columns\")\nprint(\"\\nâœ… Step 6 Complete: Created new insights from our data!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nğŸ”„ TRANSFORMING OUR DATA\n===================================\nNew columns added:\n   location  temperature  temperature_f  season\n0   Pacific         18.5          65.30  Winter\n1  Atlantic         22.1          71.78  Winter\n2    Indian         20.0          68.00  Winter\n3  Southern         15.2          59.36  Winter\n4    Arctic         12.1          53.78  Winter\n\nğŸ“ˆ Original columns: 5\nğŸ“ˆ After transformation: 7 columns\n\nâœ… Step 6 Complete: Created new insights from our data!\n```\n:::\n:::\n\n\n:::{.callout-important title=\"Why Transform Data?\"}\n**Transformation creates new insights**:\n- ğŸŒ¡ï¸ **Temperature in Fahrenheit**: Makes data accessible to different audiences\n- ğŸ—“ï¸ **Season categories**: Helps us compare winter vs summer patterns\n- ğŸ“Š **New calculations**: Ratios, categories, derived metrics\n\n**Real data scientists** spend lots of time creating these \"feature engineering\" transformations!\n:::\n\n**ğŸ”® Coming Attractions**: In Day 6, you'll learn advanced transformations and custom functions!\n\n\n## ğŸ‘¥ Step 7: Group Data\n\nâœ… **Workflow Step 7**: Organizing by categories to find patterns\n\nNow we'll group our data by categories to compare different oceans and seasons:\n\n::: {#58ac921c .cell execution_count=8}\n``` {.python .cell-code}\nprint(\"ğŸ‘¥ GROUPING OUR DATA\")\nprint(\"=\" * 30)\n\n# Group by ocean location (using .groupby() from Session 4b)\nby_ocean = df_cleaned.groupby('location')\n\nprint(\"ğŸ“Š Number of measurements per ocean:\")\nprint(by_ocean.size())\n\n# Group by season to compare winter vs summer\nby_season = df_cleaned.groupby('season')\n\nprint(\"\\nğŸ“Š Number of measurements per season:\")\nprint(by_season.size())\n\nprint(\"\\nâœ… Step 7 Complete: Data organized by meaningful categories!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nğŸ‘¥ GROUPING OUR DATA\n==============================\nğŸ“Š Number of measurements per ocean:\nlocation\nArctic      6\nAtlantic    6\nIndian      6\nPacific     6\nSouthern    6\ndtype: int64\n\nğŸ“Š Number of measurements per season:\nseason\nSummer    10\nWinter    20\ndtype: int64\n\nâœ… Step 7 Complete: Data organized by meaningful categories!\n```\n:::\n:::\n\n\n:::{.callout-important title=\"Why Group Data?\"}\n**Grouping reveals patterns**:\n- ğŸŒŠ **By ocean**: Compare Pacific vs Atlantic vs Arctic temperatures\n- ğŸ—“ï¸ **By season**: See how temperatures change winter to summer  \n- ğŸ“Š **By categories**: Any categorical variable can create groups\n\n**This sets up the next step** - calculating summary statistics for each group!\n:::\n\n**ğŸ”® Coming Attractions**: In Day 6, you'll learn to group by multiple columns simultaneously and create complex hierarchical groups!\n\n## ğŸ“ˆ Step 8: Aggregate Data\n\nâœ… **Workflow Step 8**: Calculating summary statistics to answer our question\n\nNow for the exciting part - let's calculate averages to answer \"Which ocean is warmest?\"\n\n::: {#3bf4d4bf .cell execution_count=9}\n``` {.python .cell-code}\nprint(\"ğŸ“ˆ AGGREGATING OUR DATA\")\nprint(\"=\" * 35)\n\n# Calculate average temperature by ocean (using .mean() from Session 4b)\navg_temp_by_ocean = df_cleaned.groupby('location')['temperature'].mean()\n\nprint(\"ğŸŒŠ AVERAGE TEMPERATURE BY OCEAN:\")\nprint(avg_temp_by_ocean.sort_values(ascending=False))\n\n# Calculate average temperature by season\navg_temp_by_season = df_cleaned.groupby('season')['temperature'].mean()\n\nprint(\"\\nğŸ—“ï¸ AVERAGE TEMPERATURE BY SEASON:\")\nprint(avg_temp_by_season.sort_values(ascending=False))\n\n# Answer our research question!\nwarmest_ocean = avg_temp_by_ocean.max()\nwarmest_ocean_name = avg_temp_by_ocean.idxmax()\n\nprint(f\"\\nğŸ‰ RESEARCH QUESTION ANSWERED!\")\nprint(f\"ğŸ† Warmest ocean: {warmest_ocean_name} ({warmest_ocean:.1f}Â°C)\")\n\nprint(\"\\nâœ… Step 8 Complete: Found the answer through aggregation!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nğŸ“ˆ AGGREGATING OUR DATA\n===================================\nğŸŒŠ AVERAGE TEMPERATURE BY OCEAN:\nlocation\nAtlantic    24.083333\nIndian      22.133333\nPacific     20.700000\nSouthern    16.616667\nArctic      12.883333\nName: temperature, dtype: float64\n\nğŸ—“ï¸ AVERAGE TEMPERATURE BY SEASON:\nseason\nSummer    22.100\nWinter    17.875\nName: temperature, dtype: float64\n\nğŸ‰ RESEARCH QUESTION ANSWERED!\nğŸ† Warmest ocean: Atlantic (24.1Â°C)\n\nâœ… Step 8 Complete: Found the answer through aggregation!\n```\n:::\n:::\n\n\n:::{.callout-important title=\"Key Discovery!\"}\n**Our Research Results**:\n- ğŸ¥‡ **Warmest Ocean**: Atlantic (24.1Â°C average)\n- ğŸ¥ˆ **Second Warmest**: Pacific (20.7Â°C average)  \n- ğŸ¥‰ **Coldest**: Arctic (12.9Â°C average)\n- ğŸŒ **Summer is warmer** than winter (as expected!)\n\n**This is exactly how real data science works** - use aggregation to answer research questions!\n:::\n\n**ğŸ”® Coming Attractions**: In Day 6, you'll learn advanced aggregation functions like `.agg()` to calculate multiple statistics at once!\n\n## ğŸ“Š Step 9: Visualize Data\n\nâœ… **Workflow Step 9**: Telling our story with charts\n\nThe final step is creating a chart to communicate our findings clearly:\n\n::: {#b67f5414 .cell execution_count=10}\n``` {.python .cell-code}\nprint(\"ğŸ“Š VISUALIZING OUR RESULTS\")\nprint(\"=\" * 35)\n\n# Create average temperature data for plotting\navg_temps = df_cleaned.groupby('location')['temperature'].mean().sort_values(ascending=False)\n\n# Create a bar chart (using matplotlib from Session 4c)\nplt.figure(figsize=(10, 6))\navg_temps.plot(kind='bar', color=['red', 'orange', 'blue', 'green', 'purple'])\nplt.title('ğŸŒŠ Average Ocean Temperatures: Research Results', fontsize=16, fontweight='bold')\nplt.xlabel('Ocean Location', fontsize=12)\nplt.ylabel('Average Temperature (Â°C)', fontsize=12)\nplt.xticks(rotation=45)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\n\n# Add our research conclusion to the plot\nplt.figtext(0.5, 0.02, 'ğŸ† Research Conclusion: Atlantic Ocean is the warmest on average!', \n            ha='center', fontsize=12, fontweight='bold')\n\nplt.show()\n\nprint(\"\\nâœ… Step 9 Complete: Story told through visualization!\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/bs/x9tn9jz91cv6hb3q6p4djbmw0000gn/T/ipykernel_96207/1715597996.py:15: UserWarning: Glyph 127754 (\\N{WATER WAVE}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/Users/kellycaylor/mambaforge/envs/eds217_2025/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 127942 (\\N{TROPHY}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/Users/kellycaylor/mambaforge/envs/eds217_2025/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 127754 (\\N{WATER WAVE}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nğŸ“Š VISUALIZING OUR RESULTS\n===================================\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](4c_dataframe_workflows_files/figure-html/cell-11-output-3.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nâœ… Step 9 Complete: Story told through visualization!\n```\n:::\n:::\n\n\n:::{.callout-important title=\"ğŸ‰ CONGRATULATIONS! ğŸ‰\"}\n**You just completed your first full data science project!**\n\n**ğŸŒŠ Research Question**: *Which ocean has the warmest average temperatures?*  \n**ğŸ“Š Answer**: **Atlantic Ocean** (24.1Â°C average)  \n**ğŸ† Method**: Complete 9-step data science workflow!\n\n**You've completed the full workflow - you're officially a data scientist!** ğŸ“\n:::\n\n## ğŸ¯ What You Accomplished Today\n\n### **âœ… Complete Workflow Mastery**\nYou just used the **exact same process** that professional data scientists use every day:\n\n1. âœ… **Imported** real ocean temperature data\n2. âœ… **Explored** to understand what you had  \n3. âœ… **Cleaned** (lucky us - data was already clean!)\n4. âœ… **Filtered** to focus on specific questions\n5. âœ… **Sorted** to find temperature patterns\n6. âœ… **Transformed** data to create new insights  \n7. âœ… **Grouped** by meaningful categories\n8. âœ… **Aggregated** to calculate summary statistics\n9. âœ… **Visualized** results with a professional chart\n\n### **ğŸ”® Your Data Science Journey Continues**\n\n**Next Week - Individual Step Mastery**:\n- **Day 5**: Advanced filtering and transformation techniques\n- **Day 6**: Complex grouping and aggregation methods  \n- **Day 7**: Professional data visualization with seaborn\n\n**Your Final Project**: Use this **exact 9-step workflow** to answer your own research question!\n\n### **ğŸ”„ The Workflow You Can Always Apply**\n\nWhenever you encounter a new dataset or research question, systematically work through these 9 steps:\n1. Import â†’ 2. Explore â†’ 3. Clean â†’ 4. Filter â†’ 5. Sort â†’ 6. Transform â†’ 7. Group â†’ 8. Aggregate â†’ 9. Visualize\n\n**This is your systematic approach to data science success!** ğŸ¯\n\n::: {.center-text .body-text-xl .teal-text}\nğŸ‰ End interactive session 4C - You're now a data scientist! ğŸ‰\n:::\n\n",
    "supporting": [
      "4c_dataframe_workflows_files"
    ],
    "filters": [],
    "includes": {}
  }
}