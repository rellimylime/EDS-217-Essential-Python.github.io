---
title: "Interactive Session 5B"
subtitle: "Cleaning Data"
editor_options: 
  chunk_output_type: console
jupyter: eds217_2025
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: show
---

::: {style="width: 60%; margin: auto;"}
![](images/cleaning.png)
:::

:::{.gray-text .center-text}
*A cartoon panda is getting a bubble bath.* [MidJourney 5](https://www.midjourney.com/jobs/2a08bcac-24e3-4c73-ad14-4733c1ffd363?index=0)
:::

## Getting Started

Before we begin our interactive session, please follow these steps to set up your Jupyter Notebook:

1. Open JupyterLab and create a new notebook:
   - Click on the `+` button in the top left corner
   - Select `Python 3.11.0` from the Notebook options

2. Rename your notebook:
   - Right-click on the `Untitled.ipynb` tab
   - Select "Rename"
   - Name your notebook with the format: `Session_XY_Topic.ipynb`
     (Replace X with the day number and Y with the session number)

3. Add a title cell:
   - In the first cell of your notebook, change the cell type to "Markdown"
   - Add the following content (replace the placeholders with the actual information):

```markdown
# Day 5: Session B - Cleaning Data

[Link to session webpage](https://eds-217-essential-python.github.io/course-materials/interactive-sessions/5b_cleaning_data.html)

Date: 09/08/2025

```

4. Add a code cell:
   - Below the title cell, add a new cell
   - Ensure it's set as a "Code" cell
   - This will be where you start writing your Python code for the session

5. Throughout the session:
   - Take notes in Markdown cells
   - Copy or write code in Code cells
   - Run cells to test your code
   - Ask questions if you need clarification

:::{.callout-caution}
Remember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).
:::

Let's begin our interactive session!
<hr>


## Introduction to Data Cleaning

Data cleaning is a crucial step in the data science workflow. It involves identifying and correcting errors, inconsistencies, and inaccuracies in datasets to ensure the quality and reliability of your analysis.

In this session, we'll explore common issues in dataframes and learn how to address them using pandas. 


## Instructions
We will work through this material together, writing a new notebook as we go.


<p style="color:#008C96; font-weight: bold"> ‚úèÔ∏è &nbsp; &nbsp; This symbol designates code you should add to your notebook and run.  </p>

ü§ì Where useful, this session contains links to [Pandas Tutor](https://pandastutor.com/index.html), which helps you to visualize the chained functions in the accompanying code block.

<hr style="border-top: 1px solid gray; margin-top: 24px; margin-bottom: 1px"></hr>


Let's start by importing pandas and creating a sample dataframe with some issues we'll need to clean:

<div class="example"> 
‚úèÔ∏è Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true
import pandas as pd
import numpy as np

# Create a sample dataframe with issues
data = {
    'species': ['Oak', 'Pine', 'Maple', 'Oak', 'Pine', None],
    'height_m': [5.2, 12.0, '7.5', 5.2, 15.0, 8.1],
    'diameter_cm': [20, 35, 25, 20, 40, np.nan],
    'location': ['Park A', 'Park B', 'Park A', 'Park A', 'Park B', 'Park C '],
    'date_planted': ['2020-01-15', '2019-05-20', '2020-03-10', '2020-01-15', '2018-11-30', '2021-07-05']
}

df = pd.DataFrame(data)
print(df)
```

## Handling Missing Values

### Identifying Missing Values

First, let's check for missing values in our dataframe. For this we use the `isnull()` method on the dataframe.

<div class="example"> 
‚úèÔ∏è Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true
print(df.isnull())
```

You can see that the `isnull()` command returns a Booelan (`True` or `False`) value for each item in the dataframe. If the location (row, column) is empty, then the `isnull()` command will return `True`, otherwise it returns `False`.

We can apply the `sum()` method to the result of `df.isnull()` to see what columns have empty values in them. 

:::{.callout-important}
The `axis` argument is often used in pandas and numpy to indicate how an aggregation (e.g. `sum()`) should be applied. You should read this argument as an answer to the question:

> What should I apply this aggregation across, `rows` (axis `0`) or `columns` (axis `1`)?

`df.sum(axis=0)` adds up all the rows and <i>returns a single sum for each column</i>.

`df.sum(axis=1)` adds up all the columns and <i>returns a single sum for each row</i>. 

Generally, aggregations over all rows are more useful than aggregations across all columns, so the _default_ for pandas and numpy aggregations is to apply aggregations and dataframe operations assuming `axis=0`. However, as we'll see, other commands default to `axis=1`.

Some commands allow you to use alias string arguments (`rows` and `columns`), but this isn't universal across the libarary.

:::

<div class="example"> 
‚úèÔ∏è Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true

# Use the axis argument as an integer (0)
null_values = df.isnull()
print("Using `axis=0`:\n",
        null_values.sum(axis=0))

# Show that this is the same as using the `axis='rows'` argument:
print("\nUsing `axis='rows':\n",
        null_values.sum(axis='rows'))

# And that this is the same as the default behavior:
print("\nUsing default arguments:\n",
        null_values.sum())

```

As we requested, this command sums up all the rows in each column of `null_values`. Any `False` is a `0` and any `True` is a `1`, so the result is the number of null values in each column of the dataframe.

:::{.callout-info}
Method chaining allows us to do both the finding of null values and the summing of values for all rows in each column with a single line of code
:::

<div class="example"> 
‚úèÔ∏è Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true
# Use method chaining to make our code more concise.
df.isnull().sum(axis='rows')
```

ü§ì [Pandas Tutor](https://pandastutor.com/vis.html#code=import%20pandas%20as%20pd%0Aimport%20numpy%20as%20np%0A%23%20Create%20a%20sample%20dataframe%20with%20issues%0Adata%20%3D%20%7B%0A%20%20%20%20'species'%3A%20%5B'Oak',%20'Pine',%20'Maple',%20'Oak',%20'Pine',%20None%5D,%0A%20%20%20%20'height_m'%3A%20%5B5.2,%2012.0,%20'7.5',%205.2,%2015.0,%208.1%5D,%0A%20%20%20%20'diameter_cm'%3A%20%5B20,%2035,%2025,%2020,%2040,%20np.nan%5D,%0A%20%20%20%20'location'%3A%20%5B'Park%20A',%20'Park%20B',%20'Park%20A',%20'Park%20A',%20'Park%20B',%20'Park%20C'%5D,%0A%20%20%20%20'date_planted'%3A%20%5B'2020-01-15',%20'2019-05-20',%20'2020-03-10',%20'2020-01-15',%20'2018-11-30',%20'2021-07-05'%5D%0A%7D%0Adf%20%3D%20pd.DataFrame%28data%29%0A%0Adf.isnull%28%29.sum%28axis%3D'rows'%29&d=2024-08-28&lang=py&v=v1)

### Dropping Missing Values

We can drop rows with missing values using the `dropna()` function:

```{python}
#| echo: true
df_dropped = df.dropna()
print(df_dropped)
```

Notice how we didn't need to specify an axis - by default, `dropna()` operates on each row and removes and rows that are any missing values (the default is `axis='rows'`). 

### Filling Missing Values (Imputation)

When you drop data (using methods like `dropna()` or `drop()`), you're permanently removing information from your dataset. 

This can potentially lead to:

- Loss of important insights
- Biased results
- Reduced statistical power
- Smaller sample size, which can affect the reliability of your analysis

[Imputation](https://towardsdatascience.com/different-imputation-methods-to-handle-missing-data-8dd5bce97583) is the process of replacing missing values with substituted values. Instead of dropping rows or columns with missing data, you fill in the gaps. 

#### Common imputation techniques include:

- Mean/median/mode imputation
- Forward fill or backward fill
- Interpolation
- Using machine learning models to predict missing values

#### Other techniques:

- Creating a "missing" category for categorical variables
- Using algorithms that can handle missing data (like some decision tree-based methods)
- Multiple imputation for more rigorous statistical analysis

When to consider alternatives:

- When missing data is not completely at random ([MCAR](https://en.wikipedia.org/wiki/Missing_data))
- When you have a small dataset and can't afford to lose samples
- When the missing data might contain important information about your problem

You can use any of the pandas Series aggregation commands to fill missing values instead of dropping the data.

<div class="example"> 
‚úèÔ∏è Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true
# Fill missing values with a specific value
df['species'] = df['species'].fillna('unknown')

print(df)
```

We can even use aggregations to fill with values derived from our dataframe. 

For example, let's replace missing values of `diameter_cm` with the average value across all the rows.

<div class="example"> 
‚úèÔ∏è Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true
# Fill missing numeric values with the mean of the column
df['diameter_cm'] = df['diameter_cm'].fillna(df['diameter_cm'].mean())
```

ü§ì [Pandas Tutor](https://pandastutor.com/vis.html#code=import%20pandas%20as%20pd%0Aimport%20numpy%20as%20np%0A%23%20Create%20a%20sample%20dataframe%20with%20issues%0Adata%20%3D%20%7B%0A%20%20%20%20'species'%3A%20%5B'Oak',%20'Pine',%20'Maple',%20'Oak',%20'Pine',%20None%5D,%0A%20%20%20%20'height_m'%3A%20%5B5.2,%2012.0,%20'7.5',%205.2,%2015.0,%208.1%5D,%0A%20%20%20%20'diameter_cm'%3A%20%5B20,%2035,%2025,%2020,%2040,%20np.nan%5D,%0A%20%20%20%20'location'%3A%20%5B'Park%20A',%20'Park%20B',%20'Park%20A',%20'Park%20A',%20'Park%20B',%20'Park%20C'%5D,%0A%20%20%20%20'date_planted'%3A%20%5B'2020-01-15',%20'2019-05-20',%20'2020-03-10',%20'2020-01-15',%20'2018-11-30',%20'2021-07-05'%5D%0A%7D%0Adf%20%3D%20pd.DataFrame%28data%29%0A%0Adf%5B'diameter_cm'%5D.fillna%28df%5B'diameter_cm'%5D.mean%28%29%29&d=2024-08-28&lang=py&v=v1)

## Dealing with Duplicates

### Identifying and Removing Duplicate Rows

Let's check for and remove duplicate rows:

<div class="example"> 
‚úèÔ∏è Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true
# Check for duplicates
print(df.duplicated())
```

It looks like row `3` is a duplicate (it is the same as row `0`). As before, we can see how many rows are duplicated by applying the sum command to the result of `df.duplicated()`

<div class="example"> 
‚úèÔ∏è Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true
df.duplicated().sum()
```

ü§ì [Pandas Tutor](https://pandastutor.com/vis.html#code=import%20pandas%20as%20pd%0Aimport%20numpy%20as%20np%0A%23%20Create%20a%20sample%20dataframe%20with%20issues%0Adata%20%3D%20%7B%0A%20%20%20%20'species'%3A%20%5B'Oak',%20'Pine',%20'Maple',%20'Oak',%20'Pine',%20None%5D,%0A%20%20%20%20'height_m'%3A%20%5B5.2,%2012.0,%20'7.5',%205.2,%2015.0,%208.1%5D,%0A%20%20%20%20'diameter_cm'%3A%20%5B20,%2035,%2025,%2020,%2040,%20np.nan%5D,%0A%20%20%20%20'location'%3A%20%5B'Park%20A',%20'Park%20B',%20'Park%20A',%20'Park%20A',%20'Park%20B',%20'Park%20C'%5D,%0A%20%20%20%20'date_planted'%3A%20%5B'2020-01-15',%20'2019-05-20',%20'2020-03-10',%20'2020-01-15',%20'2018-11-30',%20'2021-07-05'%5D%0A%7D%0Adf%20%3D%20pd.DataFrame%28data%29%0A%0Adf.duplicated%28%29.sum%28%29&d=2024-08-28&lang=py&v=v1)

The `drop_duplicates()` method returns a new dataframe that only contains the first row of any duplicated rows.

<div class="example"> 
‚úèÔ∏è Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true
df_no_duplicates = df.drop_duplicates()
print(df_no_duplicates)
```

The extra entry for `Oak` no longer appears in `df_no_duplicates`.

:::{.callout-info}
What if we wanted to simply get rid of the duplicates in our original `df` without having to make an entirely new dataframe? the `inplace` option allows for this with many pandas methods:

```python
df.drop_duplicates(inplace=True)
```
:::{.callout-important}
While `inplace=True` can be useful when making changes to a dataframe without having to worry about creating a copy, you can't do method chaining when using this argument.
:::

:::

<div class="example"> 
‚úèÔ∏è Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true
# Make a copy of our dataframe
df2 = df.copy()
# Remove the duplicates from df2 without making a new dataframe (save results back into df2)
df2.drop_duplicates(inplace=True)
print(df2)
```


### Handling Duplicates Based on Specific Columns

We can also remove duplicates based on specific columns, in this case removing any rows that share the same `species` and `location`.

<div class="example"> 
‚úèÔ∏è Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true
df_unique_species = df.drop_duplicates(subset=['species', 'location'])
print(df_unique_species)
```

Although our two Pines weren't duplciates (their `height_m`, `diameter_cm`, and `date_planted` were different), we still dropped them from the dataframe based on the subset of columns (`species` and `location`)

ü§ì [Pandas Tutor](https://pandastutor.com/vis.html#code=import%20pandas%20as%20pd%0Aimport%20numpy%20as%20np%0A%23%20Create%20a%20sample%20dataframe%20with%20issues%0Adata%20%3D%20%7B%0A%20%20%20%20'species'%3A%20%5B'Oak',%20'Pine',%20'Maple',%20'Oak',%20'Pine',%20None%5D,%0A%20%20%20%20'height_m'%3A%20%5B5.2,%2012.0,%20'7.5',%205.2,%2015.0,%208.1%5D,%0A%20%20%20%20'diameter_cm'%3A%20%5B20,%2035,%2025,%2020,%2040,%20np.nan%5D,%0A%20%20%20%20'location'%3A%20%5B'Park%20A',%20'Park%20B',%20'Park%20A',%20'Park%20A',%20'Park%20B',%20'Park%20C'%5D,%0A%20%20%20%20'date_planted'%3A%20%5B'2020-01-15',%20'2019-05-20',%20'2020-03-10',%20'2020-01-15',%20'2018-11-30',%20'2021-07-05'%5D%0A%7D%0Adf%20%3D%20pd.DataFrame%28data%29%0A%0Adf.drop_duplicates%28subset%3D%5B'species',%20'location'%5D%29&d=2024-08-28&lang=py&v=v1)

## Data Type Conversion and Consistency

### Checking and Changing Data Types

Often datasets - especially those collected by surveys or forms - contain a mixture of data types (i.e. some strings mixed in with mostly numbers)

Let's check the data types of our columns and convert them as needed:

<div class="example"> 
‚úèÔ∏è Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true
print(df.dtypes)

```

The `object` datatype is a generic term meaning "something, I don't know what" in python (remember, in python _everything is an object_).

Generally, we want our data types to be something more specific, like a floating point number, an integer, a string, or a date. We can use the `astype()` method to coerce our data into a specific kind of thing.

:::{.callout-info}
In older versions of pandas, string columns were always still listed as type `object`. They are functionally `str` objects, but pandas isn't storing them in any special "pandas" way, so they are just generic python `object`s. Newer versions of pandas allow you to create `string` (note: not the same as `str`) data types. They are optimized for use in pandas, although you will rarely see any difference in performance, it's good practice to use them when you can.
:::

Let's convert `height_m` to `float`.

<div class="example"> 
‚úèÔ∏è Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true
# Convert 'height_m' to float
df['height_m'] = df['height_m'].astype(float)
print(df.dtypes)
```

Converting generic objects to `datetime` is more complicated. In fact, we'll have an entire session later this class on working with dates. Pandas has a helper function - `pd.to_datetime()` - that tries to infer dates from values in columns.  

<div class="example"> 
‚úèÔ∏è Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true
# Convert 'date_planted' to datetime
df['date_planted'] = pd.to_datetime(df['date_planted'])

print(df.dtypes)
```

## String Manipulation and Formatting

We can use string methods to clean text data. We access these methods using the `.str` attribute that is part of every pandas `Series`.

:::{.callout-note}
Remember, every column in a `DataFrame` is a `Series`
:::

<div class="example"> 
‚úèÔ∏è Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true
print("'unkown' should be capitalized")
print(df)

# Capitalize species names (unknown -> Unknown)
df['species'] = df['species'].str.capitalize()

print("\nFixed it!")
print(df)
```

<div class="example"> 
‚úèÔ∏è Try it. Add the cell below to your notebook and run it.
</div>

```{python}
#| echo: true
print("'Park C ' should be 'Park C'")
print(df)

# Remove leading/trailing whitespace from location
# "Park C " -> "Park C"
df['location'] = df['location'].str.strip()

print("\nFixed it!")
print(df)
```

## Wrap-up and Best Practices

In this session, we've covered essential techniques for cleaning dataframes in pandas:
- Handling missing values
- Dealing with duplicates
- Converting data types
- String manipulation and formatting

Remember these best practices:

1. Always examine your data before and after cleaning steps.
2. Remember that the default for most operations is to act across all rows (`axis=0`). 
3. Document your cleaning steps for reproducibility.
4. Be cautious when dropping data - sometimes imputation or other techniques might be more appropriate.

For more advanced cleaning techniques and in-depth explanations, refer to the pandas documentation [Pandas Documentation](https://pandas.pydata.org/docs/), the master [Pandas Cheat Sheet](Pandas_Cheat_Sheet.pdf), or our class [Cleaning Data Cheatsheet](../cheatsheets/data_cleaning.qmd)

::: {.center-text .body-text-xl .teal-text}
End interactive session 5B
:::
