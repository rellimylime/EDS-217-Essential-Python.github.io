---
title: "Live Coding Session"
subtitle: "Mastering pd.read_csv() in Pandas"
jupyter: eds217_2025
search: false
format:
    html:
        toc: true
        toc-depth: 3
        code-fold: show
---

## Introduction (5 minutes)

Good morning/afternoon, everyone! Today, we're going to dive deep into one of the most fundamental functions in pandas: `pd.read_csv()`. This function is your gateway to working with CSV data in Python, and mastering it will significantly boost your data analysis capabilities.

Let's start by importing pandas and setting up our environment:

```{python}
#| echo: true
import pandas as pd
import numpy as np
```

## Basic Usage and Column Selection (10 minutes)

Let's begin with the basics. We'll use the 'basic_data.csv' file for this part.

```{python}
#| echo: true
# URL for the basic data CSV
url_basic = 'https://bit.ly/eds217-basic'

# Read the CSV file
df_basic = pd.read_csv(url_basic)

# Display the first few rows
print(df_basic.head())

# Get information about the dataframe
print(df_basic.info())
```

Now, let's see how we can select specific columns when reading the file:

```{python}
#| echo: true
# Read only the 'Name' and 'Age' columns
df_selected = pd.read_csv(url_basic, usecols=['Name', 'Age'])
print(df_selected.head())
```

## Handling Missing Data (10 minutes)

Next, we'll look at how to handle missing data using the 'missing_values.csv' file.

```{python}
#| echo: true
# URL for the missing values CSV
url_missing = 'https://bit.ly/eds217-missing'

# Read the CSV file, treating 'N/A' and 'Unknown' as NA values
df_missing = pd.read_csv(url_missing, na_values=['N/A', 'Unknown'])

# Display the first few rows and info
print(df_missing.head())
print(df_missing.info())

# Count the number of missing values in each column
print(df_missing.isnull().sum())
```

Let's see how we can handle these missing values:

```{python}
#| echo: true
# Fill NA values in the 'Age' column with the median age
median_age = df_missing['Age'].median()
df_missing['Age'] = df_missing['Age'].fillna(median_age)

# Drop rows where 'Salary' is missing
df_missing = df_missing.dropna(subset=['Salary'])

print(df_missing.isnull().sum())
```

## Parsing Dates (10 minutes)

Now, let's work with dates using the 'date_data.csv' file.

```{python}
#| echo: true
# URL for the date data CSV
url_dates = 'https://bit.ly/eds217-dates'

# Read the CSV file, parsing the 'Date' column
df_dates = pd.read_csv(url_dates, parse_dates=['Date'])

print(df_dates.head())
print(df_dates.dtypes)
```

Let's do some date-based analysis:

```{python}
#| echo: true
# Extract year and month from the Date column
df_dates['Year'] = df_dates['Date'].dt.year
df_dates['Month'] = df_dates['Date'].dt.month

# Group by year and calculate mean value
yearly_mean = df_dates.groupby('Year')['Value'].mean()
print(yearly_mean)
```

## Working with Files Without Headers (5 minutes)

Now, let's look at how to handle files without headers using the 'no_header.csv' file.

```{python}
#| echo: true
# URL for the no header CSV
url_no_header = 'https://bit.ly/eds217-noheader'

# Read the CSV file without headers
df_no_header = pd.read_csv(url_no_header, header=None, names=['A', 'B', 'C', 'D'])

print(df_no_header.head())
```

## Working with Tab-Separated Values (TSV) Files (5 minutes)

Sometimes, you'll encounter files that use tabs as separators instead of commas. Let's see how to handle TSV files:

```{python}
#| echo: true
# URL for the TSV data
url_tsv = 'https://bit.ly/eds217-tabs'

# Read the TSV file
df_tsv = pd.read_csv(url_tsv, sep='\t')

print(df_tsv.head())
print(df_tsv.info())
```

Notice how we used the `sep='\t'` parameter to specify that the file is tab-separated. This flexibility allows `pd.read_csv()` to work with various types of delimited files, not just comma-separated ones.

## Handling Large Files: Reading a Subset of Data (5 minutes)

When dealing with very large files, you might want to read only a portion of the data to get a quick overview or to work with a manageable subset. Let's see how to do this:

```{python}
#| echo: true
# URL for the large data file
url_large = 'https://bit.ly/eds217-large'

# Read only the first 1000 lines of the file
df_large_subset = pd.read_csv(url_large, nrows=1000)

print(df_large_subset.head())
print(f"Number of rows loaded: {len(df_large_subset)}")
```

In this example, we used the `nrows` parameter to limit the number of rows read from the file. This is particularly useful when you're working with very large datasets and want to quickly explore the data structure or perform some initial analysis without loading the entire file into memory.

You can also use the `skiprows` parameter to skip a certain number of rows:

```{python}
#| echo: true
# Read 1000 rows, starting from row 5001
df_large_middle = pd.read_csv(url_large, skiprows=5000, nrows=1000)

print(df_large_middle.head())
print(f"Number of rows loaded: {len(df_large_middle)}")
```

This technique allows you to sample different parts of a large file without loading the entire dataset.

## Conclusion and Q&A (5 minutes)

Let's recap the key points we've covered:
1. Basic usage of `pd.read_csv()`
2. Selecting specific columns
3. Handling missing data
4. Parsing dates
5. Working with files that lack headers
6. Reading TSV (tab-separated values) files
7. Handling large files by reading subsets of data

Are there any questions about what we've covered or any specific scenarios you'd like to explore further?

## Bonus: Combining Techniques (if time permits)

If we have extra time, let's combine some of these techniques:

```{python}
#| echo: true
# Read the first 100 rows of the dates CSV file, selecting specific columns and parsing dates
df_combined = pd.read_csv(url_dates, 
                          nrows=100, 
                          usecols=['Date', 'Value', 'Category'], 
                          parse_dates=['Date'])

# Group by category and calculate mean value
category_mean = df_combined.groupby('Category')['Value'].mean().sort_values(ascending=False)
print(category_mean)

# Find the earliest and latest dates in the subset
print(f"Date range: {df_combined['Date'].min()} to {df_combined['Date'].max()}")
```

This exercise demonstrates how powerful `pd.read_csv()` can be when combined with other pandas functions for data analysis, even when working with subsets of larger datasets.
