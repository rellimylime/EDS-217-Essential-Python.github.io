[
  {
    "objectID": "course-materials/cheatsheets/seaborn.html",
    "href": "course-materials/cheatsheets/seaborn.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Seaborn is a powerful Python data visualization library built on top of Matplotlib. It provides a high-level interface for drawing attractive statistical graphics. Seaborn is particularly useful for creating complex visualizations with just a few lines of code.\nKey features: - Built-in themes for attractive plots - Statistical plot types - Integration with Pandas DataFrames - Automatic estimation and plotting of statistical models"
  },
  {
    "objectID": "course-materials/cheatsheets/seaborn.html#introduction-to-seaborn",
    "href": "course-materials/cheatsheets/seaborn.html#introduction-to-seaborn",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Seaborn is a powerful Python data visualization library built on top of Matplotlib. It provides a high-level interface for drawing attractive statistical graphics. Seaborn is particularly useful for creating complex visualizations with just a few lines of code.\nKey features: - Built-in themes for attractive plots - Statistical plot types - Integration with Pandas DataFrames - Automatic estimation and plotting of statistical models"
  },
  {
    "objectID": "course-materials/cheatsheets/seaborn.html#setting-up-seaborn",
    "href": "course-materials/cheatsheets/seaborn.html#setting-up-seaborn",
    "title": "EDS 217 Cheatsheet",
    "section": "Setting up Seaborn",
    "text": "Setting up Seaborn\nTo use Seaborn, you need to import it along with other necessary libraries:\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Set the style for all plots\nsns.set_style(\"whitegrid\")"
  },
  {
    "objectID": "course-materials/cheatsheets/seaborn.html#common-plot-types-in-environmental-data-science",
    "href": "course-materials/cheatsheets/seaborn.html#common-plot-types-in-environmental-data-science",
    "title": "EDS 217 Cheatsheet",
    "section": "Common Plot Types in Environmental Data Science",
    "text": "Common Plot Types in Environmental Data Science\nFor these examples, we’ll use various built-in datasets provided by Seaborn. These datasets are included with the library and are commonly used for demonstration purposes.\n\n1. Scatter Plots\nUseful for showing relationships between two continuous variables.\n\n\nCode\n# Load the tips dataset\ntips = sns.load_dataset(\"tips\")\nprint(tips.head())\n\n\n   total_bill   tip     sex smoker  day    time  size\n0       16.99  1.01  Female     No  Sun  Dinner     2\n1       10.34  1.66    Male     No  Sun  Dinner     3\n2       21.01  3.50    Male     No  Sun  Dinner     3\n3       23.68  3.31    Male     No  Sun  Dinner     2\n4       24.59  3.61  Female     No  Sun  Dinner     4\n\n\n\n\nCode\n# Basic scatter plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='total_bill', y='tip', data=tips)\nplt.title('Relationship between Total Bill and Tip')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Add hue for a third variable\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='total_bill', y='tip', hue='time', data=tips)\nplt.title('Relationship between Total Bill and Tip, colored by Time of Day')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe ‘tips’ dataset contains information about restaurant bills and tips. It’s a classic dataset used for demonstrating various plotting techniques.\n\n\n2. Line Plots\nIdeal for time series data or showing trends.\n\n\nCode\n# Load the flights dataset\nflights = sns.load_dataset(\"flights\")\nprint(flights.head())\n\n\n   year month  passengers\n0  1949   Jan         112\n1  1949   Feb         118\n2  1949   Mar         132\n3  1949   Apr         129\n4  1949   May         121\n\n\n\n\nCode\n# Basic line plot (uncertainty bounds calculated auto-magically by grouping rows containing the same year!)\nplt.figure(figsize=(12, 6))\nsns.lineplot(x='year', y='passengers', data=flights)\nplt.title('Number of Passengers over Time')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Multiple lines with confidence intervals\nplt.figure(figsize=(12, 6))\nsns.lineplot(x='year', y='passengers', hue='month', data=flights)\nplt.title('Number of Passengers over Time, by Month')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe ‘flights’ dataset contains information about passenger numbers for flights over time. It’s useful for demonstrating time series visualizations.\n\n\n3. Bar Plots\nGreat for comparing quantities across different categories.\n\n\nCode\n# Load the titanic dataset\ntitanic = sns.load_dataset(\"titanic\")\nprint(titanic.head())\n\n\n   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n0         0       3    male  22.0      1      0   7.2500        S  Third   \n1         1       1  female  38.0      1      0  71.2833        C  First   \n2         1       3  female  26.0      0      0   7.9250        S  Third   \n3         1       1  female  35.0      1      0  53.1000        S  First   \n4         0       3    male  35.0      0      0   8.0500        S  Third   \n\n     who  adult_male deck  embark_town alive  alone  \n0    man        True  NaN  Southampton    no  False  \n1  woman       False    C    Cherbourg   yes  False  \n2  woman       False  NaN  Southampton   yes   True  \n3  woman       False    C  Southampton   yes  False  \n4    man        True  NaN  Southampton    no   True  \n\n\n\n\nCode\n# Basic bar plot\nplt.figure(figsize=(10, 6))\nsns.barplot(x='class', y='fare', data=titanic)\nplt.title('Average Fare by Passenger Class')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Grouped bar plot\nplt.figure(figsize=(10, 6))\nsns.barplot(x='class', y='fare', hue='sex', data=titanic)\nplt.title('Average Fare by Passenger Class and Sex')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe ‘titanic’ dataset contains information about passengers on the Titanic, including their class, sex, age, and survival status.\n\n\n4. Box Plots\nUseful for showing distribution of data across categories.\n\n\nCode\n# Basic box plot\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='class', y='age', data=titanic)\nplt.title('Distribution of Age by Passenger Class')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Add individual data points\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='class', y='age', data=titanic)\nsns.swarmplot(x='class', y='age', data=titanic, color=\".25\", size=3)\nplt.title('Distribution of Age by Passenger Class with Individual Points')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n5. Violin Plots\nSimilar to box plots but show the full distribution of data.\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.violinplot(x='class', y='age', data=titanic)\nplt.title('Distribution of Age by Passenger Class (Violin Plot)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n6. Heatmaps\nExcellent for visualizing correlation matrices or gridded data.\n\n\nCode\n# Load the penguins dataset\npenguins = sns.load_dataset(\"penguins\")\nprint(penguins.head(5))\n\n\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0  Adelie  Torgersen            39.1           18.7              181.0   \n1  Adelie  Torgersen            39.5           17.4              186.0   \n2  Adelie  Torgersen            40.3           18.0              195.0   \n3  Adelie  Torgersen             NaN            NaN                NaN   \n4  Adelie  Torgersen            36.7           19.3              193.0   \n\n   body_mass_g     sex  \n0       3750.0    Male  \n1       3800.0  Female  \n2       3250.0  Female  \n3          NaN     NaN  \n4       3450.0  Female  \n\n\n\n\nCode\n# Correlation heatmap\nplt.figure(figsize=(10, 8))\ncorrelation = penguins.select_dtypes(include=[np.number]).corr()\nsns.heatmap(correlation, annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap of Penguin Measurements')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe ‘penguins’ dataset contains size measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica."
  },
  {
    "objectID": "course-materials/cheatsheets/seaborn.html#using-seaborn-for-data-exploration-with-dataframes",
    "href": "course-materials/cheatsheets/seaborn.html#using-seaborn-for-data-exploration-with-dataframes",
    "title": "EDS 217 Cheatsheet",
    "section": "Using Seaborn for Data Exploration with DataFrames",
    "text": "Using Seaborn for Data Exploration with DataFrames\nSeaborn is particularly powerful when working with Pandas DataFrames, as it can automatically infer variable types and choose appropriate plot types.\n\nQuick Data Overview\nRecall the structure of the penguins dataframe, which has a combination of measured and categorical values:\n\n\nCode\nprint(penguins.head())\n\n\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0  Adelie  Torgersen            39.1           18.7              181.0   \n1  Adelie  Torgersen            39.5           17.4              186.0   \n2  Adelie  Torgersen            40.3           18.0              195.0   \n3  Adelie  Torgersen             NaN            NaN                NaN   \n4  Adelie  Torgersen            36.7           19.3              193.0   \n\n   body_mass_g     sex  \n0       3750.0    Male  \n1       3800.0  Female  \n2       3250.0  Female  \n3          NaN     NaN  \n4       3450.0  Female  \n\n\nWe can explore the distribution of every numerical variable as well as the pair-wise relationship between all the variables in a dataframe using pairplot and can use a categorical variable to further organize the data within each plot using the hue argument.\n\n\nCode\n# Get a quick overview of numerical variables\nsns.pairplot(penguins, hue='species')\nplt.suptitle('Overview of Penguin Measurements by Species', y=1.02)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Visualize distributions of all numerical variables\nsns.displot(penguins['bill_length_mm'], kind='kde')\nplt.suptitle('Distribution of Penguin Bill Lengths',y=1.02)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nExploring Relationships\n\n\nCode\n# Explore relationship between variables\nsns.relplot(data=penguins, x='bill_length_mm', y='bill_depth_mm', hue='species', style='sex')\nplt.title('Relationship between Bill Length and Depth')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Facet plots for multi-dimensional exploration\ng = sns.FacetGrid(penguins, col='species', row='sex')\ng.map(sns.scatterplot, 'bill_length_mm', 'bill_depth_mm')\ng.add_legend()\nplt.suptitle('Bill Measurements by Species and Sex', y=1.02)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCategorical Data Exploration\n\n\nCode\n# Compare distributions across categories\nsns.catplot(data=penguins, kind='box', x='species', y='body_mass_g', hue='sex')\nplt.title('Body Mass Distribution by Species and Sex')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Count plots for categorical variables\nsns.countplot(data=penguins, x='island', hue='species')\nplt.title('Penguin Species Count by Island')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTime Series Exploration\n\n\nCode\n# Visualize trends over time\nsns.lineplot(data=flights, x='year', y='passengers', hue='month')\nplt.title('Passenger Numbers by Year and Month')\nplt.show()"
  },
  {
    "objectID": "course-materials/cheatsheets/seaborn.html#key-points",
    "href": "course-materials/cheatsheets/seaborn.html#key-points",
    "title": "EDS 217 Cheatsheet",
    "section": "Key Points",
    "text": "Key Points\n\nSeaborn works seamlessly with Pandas DataFrames, making it easy to create plots directly from your data.\nThe library offers a wide range of plot types suitable for various environmental data science tasks.\nSeaborn’s statistical plotting functions (like regplot and lmplot) can automatically fit and visualize linear regressions.\nThe pairplot function and FacetGrid class are powerful for creating multi-panel plots to explore complex datasets.\nSeaborn’s themes and color palettes help create publication-quality figures with minimal customization."
  },
  {
    "objectID": "course-materials/cheatsheets/seaborn.html#resources",
    "href": "course-materials/cheatsheets/seaborn.html#resources",
    "title": "EDS 217 Cheatsheet",
    "section": "Resources",
    "text": "Resources\n\nSeaborn Official Documentation\nSeaborn Tutorial\nSeaborn Example Gallery\nSeaborn Examples at the Python Graph Gallery\nVisualization in Seaborn (Jake VanderPlas)\n\nRemember, while Seaborn is powerful, it’s built on matplotlib. For more customization options, you might need to use matplotlib functions directly."
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_series.html",
    "href": "course-materials/cheatsheets/pandas_series.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "This cheatsheet provides a quick reference for common operations on Pandas Series. It’s designed for beginning data science students who are just starting to work with Pandas."
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_series.html#introduction",
    "href": "course-materials/cheatsheets/pandas_series.html#introduction",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "This cheatsheet provides a quick reference for common operations on Pandas Series. It’s designed for beginning data science students who are just starting to work with Pandas."
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_series.html#importing-pandas",
    "href": "course-materials/cheatsheets/pandas_series.html#importing-pandas",
    "title": "EDS 217 Cheatsheet",
    "section": "Importing Pandas",
    "text": "Importing Pandas\nAlways start by importing pandas:\n\n\nCode\nimport pandas as pd"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_series.html#creating-a-series",
    "href": "course-materials/cheatsheets/pandas_series.html#creating-a-series",
    "title": "EDS 217 Cheatsheet",
    "section": "Creating a Series",
    "text": "Creating a Series\n\nFrom a list\n\n\nCode\ndata = [1, 2, 3, 4, 5]\ns = pd.Series(data)\nprint(s)\n\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\n\n\n\nFrom a dictionary\n\n\nCode\ndata = {'a': 0., 'b': 1., 'c': 2.}\ns = pd.Series(data)\nprint(s)\n\n\na    0.0\nb    1.0\nc    2.0\ndtype: float64\n\n\n\n\nWith custom index\n\n\nCode\ns = pd.Series([1, 2, 3, 4, 5], index=['a', 'b', 'c', 'd', 'e'])\nprint(s)\n\n\na    1\nb    2\nc    3\nd    4\ne    5\ndtype: int64"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_series.html#basic-series-information",
    "href": "course-materials/cheatsheets/pandas_series.html#basic-series-information",
    "title": "EDS 217 Cheatsheet",
    "section": "Basic Series Information",
    "text": "Basic Series Information\n\n\nCode\n# Display the first few elements\nprint(s.head())\n\n# Get basic information about the Series\nprint(s.info())\n\n# Get summary statistics\nprint(s.describe())\n\n# Get index\nprint(s.index)\n\n# Get values\nprint(s.values)\n\n\na    1\nb    2\nc    3\nd    4\ne    5\ndtype: int64\n&lt;class 'pandas.core.series.Series'&gt;\nIndex: 5 entries, a to e\nSeries name: None\nNon-Null Count  Dtype\n--------------  -----\n5 non-null      int64\ndtypes: int64(1)\nmemory usage: 80.0+ bytes\nNone\ncount    5.000000\nmean     3.000000\nstd      1.581139\nmin      1.000000\n25%      2.000000\n50%      3.000000\n75%      4.000000\nmax      5.000000\ndtype: float64\nIndex(['a', 'b', 'c', 'd', 'e'], dtype='object')\n[1 2 3 4 5]"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_series.html#selecting-data",
    "href": "course-materials/cheatsheets/pandas_series.html#selecting-data",
    "title": "EDS 217 Cheatsheet",
    "section": "Selecting Data",
    "text": "Selecting Data\n\nBy label\n\n\nCode\n# Select a single element\nelement = s['a']\n\n# Select multiple elements\nsubset = s[['a', 'c', 'e']]\n\n\n\n\nBy position\n\n\nCode\n# Select by integer index (direct selection is being deprecated)\nfirst_element = s.iloc[0]\n\n# Select a slice by index (still okay):\nsubset = s[1:4]\n\n# Select a slice (using iloc)\nsubset = s.iloc[1:4]\n\n\n\n\nBy condition\n\n\nCode\n# Select elements greater than 2\ngreater_than_two = s[s &gt; 2]"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_series.html#basic-data-manipulation",
    "href": "course-materials/cheatsheets/pandas_series.html#basic-data-manipulation",
    "title": "EDS 217 Cheatsheet",
    "section": "Basic Data Manipulation",
    "text": "Basic Data Manipulation\n\nUpdating values\n\n\nCode\ns['a'] = 10\n\n\n\n\nRemoving elements\n\n\nCode\ns=s.drop(labels=['a'])\n\n\n\n\nAdding elements to a Series\n\n\nCode\nanother_series = pd.Series(\n    [3, 4, 5], \n    index=['f', 'g', 'h']\n)\ns = pd.concat([s, another_series])\n\n\nThe pd.concat() command (short for concatenate) is now the preferred method for extending series.\n\n\n\n\n\n\nImportant\n\n\n\nThe pd.concat() command takes a list of pd.Series objects to concatenate. This means you must create a pd.Series of new values to extend an existing pd.Series.\n\n\n\n\nUpdating elements based on their value using mask\n\n\nCode\nprint(s)\ns = s.mask(s &gt; 2, s * 2)\nprint(s)\n\n\nb    2\nc    3\nd    4\ne    5\nf    3\ng    4\nh    5\ndtype: int64\nb     2\nc     6\nd     8\ne    10\nf     6\ng     8\nh    10\ndtype: int64\n\n\nThis line uses the greater than (&gt;) logical operator within the mask() function to update the series. It will double the values in series where the condition s &gt; 5 is True, while leaving other values unchanged.\n\n\nReplacing elements based on their value using where\n\n\nCode\nprint(s)\ns = s.where(s &lt; 8, 12)\nprint(s)\n\n\nb     2\nc     6\nd     8\ne    10\nf     6\ng     8\nh    10\ndtype: int64\nb     2\nc     6\nd    12\ne    12\nf     6\ng    12\nh    12\ndtype: int64\n\n\nThis line of code will update the values in series where condition is False (i.e. where s is not less than 8), replacing them with 12. The values where condition is True will remain unchanged.\n\n\nApplying functions\n\nApplying a newly-defined function\n\n\nCode\ndef squared(x):\n    ''' Square the value of x '''\n    return x**2\n\n# Apply a function to each element\ns_squared = s.apply(squared)\n\n\n\n\nApplying a lambda (temporary) function\n\n\nCode\ns_squared = s.apply(lambda x: x**2)\n\n\nLambda functions allow for doing transformations with temporary functions instead of having to define functions seperately. They are good for quick, 1-off transformations.\n\n\n\nHandling missing values\n\n\nCode\n# Drop missing values\ns_cleaned = s.dropna()\n\n# Fill missing values\ns_filled = s.fillna(0)  # Fills with 0"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_series.html#basic-calculations",
    "href": "course-materials/cheatsheets/pandas_series.html#basic-calculations",
    "title": "EDS 217 Cheatsheet",
    "section": "Basic Calculations",
    "text": "Basic Calculations\n\n\nCode\n# Calculate mean\nmean_value = s.mean()\n\n# Calculate sum\nsum_value = s.sum()\n\n# Calculate maximum\nmax_value = s.max()"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_series.html#sorting",
    "href": "course-materials/cheatsheets/pandas_series.html#sorting",
    "title": "EDS 217 Cheatsheet",
    "section": "Sorting",
    "text": "Sorting\n\n\nCode\nprint(s)\n# Sort by values\ns_sorted = s.sort_values()\n\n# Sort by index\ns_sorted_by_index = s.sort_index()\n\n\nb     2\nc     6\nd    12\ne    12\nf     6\ng    12\nh    12\ndtype: int64"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_series.html#reindexing",
    "href": "course-materials/cheatsheets/pandas_series.html#reindexing",
    "title": "EDS 217 Cheatsheet",
    "section": "Reindexing",
    "text": "Reindexing\n\n\nCode\nprint(f\"Original Series:\\n{s}\\n\", sep='')\nnew_index = ['a', 'c', 'b', 'f', 'e', 'd', 'g', 'h', 'i', 'b']\ns_reindexed = s.reindex(new_index)\nprint(f\"Re-indexed series:\\n{s_reindexed}\")\n\n\nOriginal Series:\nb     2\nc     6\nd    12\ne    12\nf     6\ng    12\nh    12\ndtype: int64\n\nRe-indexed series:\na     NaN\nc     6.0\nb     2.0\nf     6.0\ne    12.0\nd    12.0\ng    12.0\nh    12.0\ni     NaN\nb     2.0\ndtype: float64\n\n\n\nthe reindex() command takes an ordered list that specifies the indicies that should be used to make a new pd.Series object. The list of indicies supplied to reindex() must have some indicies in common with the existing index. Indicies that do not appear in the existing Series will be set to NaN in the new Series. Repetition of indicies is allowed."
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_series.html#combining-series",
    "href": "course-materials/cheatsheets/pandas_series.html#combining-series",
    "title": "EDS 217 Cheatsheet",
    "section": "Combining Series",
    "text": "Combining Series\n\n\nCode\ns1 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\ns2 = pd.Series([4, 5, 6], index=['d', 'e', 'f'])\ns_combined = pd.concat([s1, s2])"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_series.html#converting-to-other-data-types",
    "href": "course-materials/cheatsheets/pandas_series.html#converting-to-other-data-types",
    "title": "EDS 217 Cheatsheet",
    "section": "Converting to Other Data Types",
    "text": "Converting to Other Data Types\n\n\nCode\n# To list\nlist_data = s.tolist()\n\n# To dictionary\ndict_data = s.to_dict()\n\n# To DataFrame\ndf = s.to_frame()"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_series.html#further-learning",
    "href": "course-materials/cheatsheets/pandas_series.html#further-learning",
    "title": "EDS 217 Cheatsheet",
    "section": "Further Learning",
    "text": "Further Learning\nFor more advanced operations and in-depth explanations, check out these resources:\n\nPandas Official Documentation\n10 Minutes to Pandas\nPython for Data Analysis by Wes McKinney\n\nRemember, practice is key! Try these operations with different datasets to become more comfortable with Pandas Series."
  },
  {
    "objectID": "course-materials/lectures/pandas_workflow.html#data-loading",
    "href": "course-materials/lectures/pandas_workflow.html#data-loading",
    "title": "Efficient Data Workflows in Pandas",
    "section": "1. Data Loading",
    "text": "1. Data Loading\nImportance\nLoading data properly is essential! It serves as the foundation for all subsequent operations.\nCommon Operations\n\nread_csv()\nread_excel()\nread_sql()"
  },
  {
    "objectID": "course-materials/lectures/pandas_workflow.html#data-cleaning",
    "href": "course-materials/lectures/pandas_workflow.html#data-cleaning",
    "title": "Efficient Data Workflows in Pandas",
    "section": "2. Data Cleaning",
    "text": "2. Data Cleaning\nImportance\nData cleaning is crucial for correcting or removing inaccurate, corrupted, or irrelevant data from the dataset.\nCommon Operations\n\ndropna(): Removes rows or columns with missing values.\nfillna(): Fills missing values.\nastype(): Converts column data types.\nrename(): Renames columns or index names.\ndrop(): Removes rows or columns that match given labels."
  },
  {
    "objectID": "course-materials/lectures/pandas_workflow.html#data-transformation",
    "href": "course-materials/lectures/pandas_workflow.html#data-transformation",
    "title": "Efficient Data Workflows in Pandas",
    "section": "3. Data Transformation",
    "text": "3. Data Transformation\nImportance\nData transformation involves modifying data to prepare it for analysis, which may include filtering, sorting, or adding new columns.\nCommon Operations\n\nquery(): Filter DataFrame using a query expression string.\nassign(): Add new columns or overwrite existing ones.\napply(): Apply a function to rows or columns.\nsort_values(): Sort by the values of columns."
  },
  {
    "objectID": "course-materials/lectures/pandas_workflow.html#combining-data",
    "href": "course-materials/lectures/pandas_workflow.html#combining-data",
    "title": "Efficient Data Workflows in Pandas",
    "section": "4. Combining Data",
    "text": "4. Combining Data\nImportance\nCombining data is essential when you need to enrich or expand your dataset through additions from other data sources.\nCommon Operations\n\nmerge(): Combines DataFrames based on keys.\njoin(): Joins DataFrames using index or key.\nconcat(): Concatenates DataFrames along an axis."
  },
  {
    "objectID": "course-materials/lectures/pandas_workflow.html#grouping-and-aggregation",
    "href": "course-materials/lectures/pandas_workflow.html#grouping-and-aggregation",
    "title": "Efficient Data Workflows in Pandas",
    "section": "5. Grouping and Aggregation",
    "text": "5. Grouping and Aggregation\nImportance\nGrouping and aggregation are critical for summarizing data, which can help in identifying patterns or performing segment-wise analysis.\nCommon Operations\n\ngroupby(): Group data by columns for aggregation.\nsum(): Sum values across rows/columns.\nmean(): Calculate mean of values across rows/columns.\naggregate(): Apply functions to groups, reducing dimensions."
  },
  {
    "objectID": "course-materials/lectures/pandas_workflow.html#data-summarization",
    "href": "course-materials/lectures/pandas_workflow.html#data-summarization",
    "title": "Efficient Data Workflows in Pandas",
    "section": "6. Data Summarization",
    "text": "6. Data Summarization\nImportance\nData summarization provides a quick look into the dataset, which is helpful for initial analyses and decision-making.\nCommon Operations\n\ndescribe()\nvalue_counts()"
  },
  {
    "objectID": "course-materials/lectures/pandas_workflow.html#outputexport",
    "href": "course-materials/lectures/pandas_workflow.html#outputexport",
    "title": "Efficient Data Workflows in Pandas",
    "section": "7. Output/Export",
    "text": "7. Output/Export\nImportance\nThe final step in any data analysis workflow is to save or export your results, making them available for sharing or further processing.\nCommon Operations\n\nto_csv()\nto_excel()\nto_json()"
  },
  {
    "objectID": "course-materials/lectures/pandas_workflow.html#combining-the-workflow-with-method-chaining",
    "href": "course-materials/lectures/pandas_workflow.html#combining-the-workflow-with-method-chaining",
    "title": "Efficient Data Workflows in Pandas",
    "section": "Combining the Workflow with Method Chaining",
    "text": "Combining the Workflow with Method Chaining\nIntroduction to Method Chaining\nMethod chaining allows combining multiple operations into a single, coherent expression. It enhances readability and efficiency."
  },
  {
    "objectID": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#getting-started",
    "href": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#getting-started",
    "title": "Interactive Session 6B",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#learning-objectives",
    "href": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#learning-objectives",
    "title": "Interactive Session 6B",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, you will be able to:\n\nConcatenate DataFrames from separate files\nHandle mismatched column names when merging data\nJoin datasets using a common key\nUnderstand different types of joins (inner, outer, left, right)\nLearn about more advanced sorting methods for organizing data"
  },
  {
    "objectID": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#introduction-to-merging-and-joining",
    "href": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#introduction-to-merging-and-joining",
    "title": "Interactive Session 6B",
    "section": "Introduction to Merging and Joining",
    "text": "Introduction to Merging and Joining\nIn environmental data science, it’s common to work with data from multiple sources. We often need to combine these datasets for comprehensive analysis. Pandas provides powerful tools for merging and joining data."
  },
  {
    "objectID": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#concatenating-dataframes",
    "href": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#concatenating-dataframes",
    "title": "Interactive Session 6B",
    "section": "Concatenating DataFrames",
    "text": "Concatenating DataFrames\nConcatenation is useful when you have multiple datasets with the same structure, such as data collected over different time periods or from different locations.\nLet’s start with an example of concatenating weather data from two stations:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\nLet’s create a couple of simple dataframes containing date, temperature, and humidity data:\n\n✏️ Try it. Copy the cell below to your notebook and run it.\n\n\n\nCode\n# Create sample DataFrames\nstation1_data = pd.DataFrame({\n    'date': pd.date_range(start='2023-01-01', periods=5),\n    'temperature': [20, 22, 21, 23, 22],\n    'humidity': [50, 48, 52, 51, 49]\n})\n\nstation2_data = pd.DataFrame({\n    'date': pd.date_range(start='2023-01-06', periods=5),\n    'temperature': [19, 20, 22, 21, 23],\n    'humidity': [53, 50, 47, 49, 48]\n})\n\nprint(\"Station 1 Data:\")\nprint(station1_data)\nprint(\"\\nStation 2 Data:\")\nprint(station2_data)\n\n\nStation 1 Data:\n        date  temperature  humidity\n0 2023-01-01           20        50\n1 2023-01-02           22        48\n2 2023-01-03           21        52\n3 2023-01-04           23        51\n4 2023-01-05           22        49\n\nStation 2 Data:\n        date  temperature  humidity\n0 2023-01-06           19        53\n1 2023-01-07           20        50\n2 2023-01-08           22        47\n3 2023-01-09           21        49\n4 2023-01-10           23        48\n\n\nUse pd.concat() to combine these two dataframes into a single one:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Concatenate the DataFrames\ncombined_data = pd.concat([station1_data, station2_data], ignore_index=True)\n\nprint(\"\\nCombined Data:\")\nprint(combined_data)\n\n\n\nCombined Data:\n        date  temperature  humidity\n0 2023-01-01           20        50\n1 2023-01-02           22        48\n2 2023-01-03           21        52\n3 2023-01-04           23        51\n4 2023-01-05           22        49\n5 2023-01-06           19        53\n6 2023-01-07           20        50\n7 2023-01-08           22        47\n8 2023-01-09           21        49\n9 2023-01-10           23        48\n\n\nIn this example, we used pd.concat() to combine data from two stations. The ignore_index=True parameter resets the index of the combined DataFrame. The most common use of concatenation is when downloading or working with multiple datafiles containing the same data and structure (e.g. annual rainfall data, monthly weather data, etc…)"
  },
  {
    "objectID": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#handling-mismatched-column-names",
    "href": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#handling-mismatched-column-names",
    "title": "Interactive Session 6B",
    "section": "Handling Mismatched Column Names",
    "text": "Handling Mismatched Column Names\nSometimes, datasets may have consistent data types but mismatched column names. Let’s look at an example where we have temperature data with different date column names.\n\n✏️ Try it. Copy the cell below to your notebook and run it.\n\n\n\nCode\n# Create sample DataFrames with mismatched column names\ndf1 = pd.DataFrame({\n    'date': pd.date_range(start='2023-01-01', periods=3),\n    'temp_celsius': [20, 22, 21]\n})\n\ndf2 = pd.DataFrame({\n    'DATE': pd.date_range(start='2023-01-04', periods=3),\n    'temp_celsius': [23, 22, 24]\n})\n\nprint(\"DataFrame 1:\")\nprint(df1)\nprint(\"\\nDataFrame 2:\")\nprint(df2)\n\n\nDataFrame 1:\n        date  temp_celsius\n0 2023-01-01            20\n1 2023-01-02            22\n2 2023-01-03            21\n\nDataFrame 2:\n        DATE  temp_celsius\n0 2023-01-04            23\n1 2023-01-05            22\n2 2023-01-06            24\n\n\n\nRenaming columns to facilitate concatenation\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Rename columns before concatenating\ndf2 = df2.rename(columns={'DATE': 'date'})\n\n# Concatenate the DataFrames\ncombined_df = pd.concat([df1, df2], ignore_index=True)\n\nprint(\"\\nCombined Data:\")\nprint(combined_df)\n\n\n\nCombined Data:\n        date  temp_celsius\n0 2023-01-01            20\n1 2023-01-02            22\n2 2023-01-03            21\n3 2023-01-04            23\n4 2023-01-05            22\n5 2023-01-06            24\n\n\nHere, we used the rename() method to standardize the column names before concatenation."
  },
  {
    "objectID": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#joining-data-with-a-common-key",
    "href": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#joining-data-with-a-common-key",
    "title": "Interactive Session 6B",
    "section": "Joining Data with a Common Key",
    "text": "Joining Data with a Common Key\nOften, we need to combine datasets that share a common key, such as a field site ID or a species name. Pandas provides several join operations: inner, outer, left, and right.\nLet’s look at an example where we join species occurrence data with site information:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Create sample DataFrames\nspecies_data = pd.DataFrame({\n    'site_id': ['A', 'B', 'A', 'C', 'B'],\n    'species': ['Oak', 'Pine', 'Maple', 'Birch', 'Oak'],\n    'count': [10, 5, 8, 3, 7]\n})\n\nsite_info = pd.DataFrame({\n    'site_id': ['A', 'B', 'C', 'D'],\n    'elevation': [100, 200, 150, 180],\n    'soil_type': ['loam', 'clay', 'sandy', 'loam']\n})\n\nprint(\"Species Data:\")\nprint(species_data)\nprint(\"\\nSite Info:\")\nprint(site_info)\n\n\nSpecies Data:\n  site_id species  count\n0       A     Oak     10\n1       B    Pine      5\n2       A   Maple      8\n3       C   Birch      3\n4       B     Oak      7\n\nSite Info:\n  site_id  elevation soil_type\n0       A        100      loam\n1       B        200      clay\n2       C        150     sandy\n3       D        180      loam\n\n\n\nPerforming an inner join to combine specieis and site data\n\n\nCode\n# Perform an inner join\nmerged_data = pd.merge(species_data, site_info, on='site_id', how='inner')\n\nprint(\"\\nMerged Data (Inner Join):\")\nprint(merged_data)\n\n\n\nMerged Data (Inner Join):\n  site_id species  count  elevation soil_type\n0       A     Oak     10        100      loam\n1       B    Pine      5        200      clay\n2       A   Maple      8        100      loam\n3       C   Birch      3        150     sandy\n4       B     Oak      7        200      clay\n\n\nIn this example, we used pd.merge() to perform an inner join on the ‘site_id’ column. This join keeps only the rows where the site_id exists in both DataFrames.\n\n\nDifferent Types of Joins\nLet’s explore other types of joins:\n\nLeft Join: Keeps all rows from the left DataFrame (species_data) and matching rows from the right DataFrame (site_info).\nRight Join: Keeps all rows from the right DataFrame (site_info) and matching rows from the left DataFrame (species_data).\nOuter Join: Keeps all rows from both DataFrames, filling in NaN where there’s no match."
  },
  {
    "objectID": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#understanding-join-types-in-depth",
    "href": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#understanding-join-types-in-depth",
    "title": "Interactive Session 6B",
    "section": "Understanding Join Types in Depth",
    "text": "Understanding Join Types in Depth\nWhen working with environmental data, understanding different join types is crucial. Let’s explore these in more detail, with a focus on their applications in environmental data science.\n\nInner Join: The Most Common Join\n\n\n\n\n\n\nNote\n\n\n\nInner join is the most frequently used join type in data analysis, including environmental studies. It returns only the rows that have matching values in both DataFrames.\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Example of Inner Join\nspecies_observations = pd.DataFrame({\n    'site_id': ['A', 'B', 'C', 'D', 'E'],\n    'species': ['Oak', 'Pine', 'Maple', 'Birch', 'Cedar']\n})\n\nsite_characteristics = pd.DataFrame({\n    'site_id': ['A', 'B', 'C', 'F', 'G'],\n    'soil_type': ['Loam', 'Clay', 'Sandy', 'Silt', 'Peat']\n})\n\ninner_join_result = pd.merge(species_observations, site_characteristics, on='site_id', how='inner')\nprint(\"Inner Join Result:\")\nprint(inner_join_result)\n\n\nInner Join Result:\n  site_id species soil_type\n0       A     Oak      Loam\n1       B    Pine      Clay\n2       C   Maple     Sandy\n\n\nWhen to use Inner Join: - When you want to analyze only the data points that have complete information in both datasets. - For example, studying the relationship between soil types and tree species, but only for sites where you have both pieces of information.\n\n\nLeft Join: Keeping All Data from the Primary Dataset\nLeft join keeps all rows from the left (primary) DataFrame and matching rows from the right DataFrame. It’s useful when you want to retain all records from your main dataset.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Example of Left Join\nleft_join_result = pd.merge(species_observations, site_characteristics, on='site_id', how='left')\nprint(\"Left Join Result:\")\nprint(left_join_result)\n\n\nLeft Join Result:\n  site_id species soil_type\n0       A     Oak      Loam\n1       B    Pine      Clay\n2       C   Maple     Sandy\n3       D   Birch       NaN\n4       E   Cedar       NaN\n\n\nWhen to use Left Join: - When you want to keep all observations from your primary dataset, even if some don’t have matching information in the secondary dataset. - For example, retaining all species observations, even for sites without soil type data.\n\n\nRight Join: Less Common, but Useful in Specific Cases\nRight join is less common but can be useful in certain scenarios. It keeps all rows from the right DataFrame and matching rows from the left.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Example of Right Join\nright_join_result = pd.merge(species_observations, site_characteristics, on='site_id', how='right')\nprint(\"Right Join Result:\")\nprint(right_join_result)\n\n\nRight Join Result:\n  site_id species soil_type\n0       A     Oak      Loam\n1       B    Pine      Clay\n2       C   Maple     Sandy\n3       F     NaN      Silt\n4       G     NaN      Peat\n\n\nWhen to use Right Join: - When you want to ensure all records from a secondary dataset are included. - For example, including all soil type data, even for sites where no species were observed.\n\n\nOuter Join: Combining All Data\nOuter join combines all rows from both DataFrames, filling in NaN where there’s no match. It’s useful for getting a complete view of all available data.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Example of Outer Join\nouter_join_result = pd.merge(species_observations, site_characteristics, on='site_id', how='outer')\nprint(\"Outer Join Result:\")\nprint(outer_join_result)\n\n\nOuter Join Result:\n  site_id species soil_type\n0       A     Oak      Loam\n1       B    Pine      Clay\n2       C   Maple     Sandy\n3       D   Birch       NaN\n4       E   Cedar       NaN\n5       F     NaN      Silt\n6       G     NaN      Peat\n\n\nWhen to use Outer Join: - When you want to see all available data from both datasets, identifying where information might be missing. - Useful for data exploration or when preparing a comprehensive dataset for further analysis.\n\n\nKey Considerations for Choosing Join Types\n\nData Completeness: Inner join for complete cases, outer join for all available data.\nAnalysis Requirements: Consider what missing data means for your analysis.\nData Quality: Outer joins can help identify data gaps or collection inconsistencies.\nPrimary Focus: Use left join when focusing on a primary dataset but want additional information where available.\n\n\n\nCommon Scenarios in Environmental Data Science\n\nSpecies Distribution Studies: Left join species observations (primary data) with environmental factors.\nComprehensive Ecosystem Analysis: Outer join multiple datasets (species, soil, climate) to get a full picture.\nHabitat Suitability Models: Inner join species presence with complete environmental data.\nLong-term Monitoring: Left join time-series observations with site metadata, keeping all temporal data.\n\nRemember, the choice of join can significantly impact your analysis results. Always consider the nature of your data and the questions you’re trying to answer when selecting a join type.\n\nPractice Exercise\nYou have two datasets: one containing river water quality measurements and another with information about sampling sites. Combine these datasets to create a comprehensive view of water quality across different sites.\n\n\nCode\n# Water quality data\nwater_quality = pd.DataFrame({\n    'site_code': ['RV01', 'RV02', 'RV01', 'RV03', 'RV02'],\n    'date': pd.date_range(start='2023-01-01', periods=5),\n    'pH': [7.2, 7.8, 7.3, 6.9, 7.7],\n    'dissolved_oxygen': [8.5, 7.9, 8.3, 7.2, 8.1]\n})\n\n# Site information\nsite_info = pd.DataFrame({\n    'site_code': ['RV01', 'RV02', 'RV03', 'RV04'],\n    'river_name': ['Blue River', 'Green Creek', 'Red Stream', 'Yellow Brook'],\n    'watershed': ['Alpine', 'Lowland', 'Lowland', 'Alpine']\n})\n\n\n\n\nCode\n# Your task:\n# 1. Merge the water quality data with site information\n# 2. Calculate the average pH and dissolved oxygen for each river\n# 3. Display the results sorted by average pH\n\n# Your code here"
  },
  {
    "objectID": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#advanced-sorting-techniques",
    "href": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#advanced-sorting-techniques",
    "title": "Interactive Session 6B",
    "section": "Advanced Sorting Techniques",
    "text": "Advanced Sorting Techniques\nAfter merging datasets, you often need to sort the resulting DataFrame in more complex ways. Let’s explore some advanced sorting techniques that are particularly useful when working with merged data.\n\nSorting with a Custom Key or Function\nSometimes you need to sort based on a computed value rather than the raw data in a column. You can use the key parameter in sort_values() to apply a function to the column before sorting:\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Create a sample DataFrame\ndf = pd.DataFrame({\n    'site': ['A', 'B', 'C', 'D'],\n    'species': ['Oak', 'Pine', 'Maple', 'Birch'],\n    'height': [5.2, 12.7, 8.1, 9.9]\n})\n\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Sort by absolute difference from 10 meters\ndf_sorted = df.sort_values('height', key=lambda x: abs(x - 10))\nprint(\"\\nSorted by proximity to 10 meters:\")\nprint(df_sorted)\n\n\nOriginal DataFrame:\n  site species  height\n0    A     Oak     5.2\n1    B    Pine    12.7\n2    C   Maple     8.1\n3    D   Birch     9.9\n\nSorted by proximity to 10 meters:\n  site species  height\n3    D   Birch     9.9\n2    C   Maple     8.1\n1    B    Pine    12.7\n0    A     Oak     5.2\n\n\nThis is useful when you want to sort based on a specific criterion, like proximity to a target value.\n\n\nSorting with NaN Values\nWhen dealing with merged data, you might encounter NaN values. By default, Pandas sorts NaN values at the end:\n\n\nCode\n# Create a DataFrame with NaN values\ndf_nan = pd.DataFrame({\n    'site': ['A', 'B', 'C', 'D', 'E'],\n    'value': [5, np.nan, 3, np.nan, 1]\n})\n\nprint(\"DataFrame with NaN values:\")\nprint(df_nan)\n\n# Sort with NaN values\ndf_sorted_nan = df_nan.sort_values('value', na_position='first')\nprint(\"\\nSorted with NaN values first:\")\nprint(df_sorted_nan)\n\n\nDataFrame with NaN values:\n  site  value\n0    A    5.0\n1    B    NaN\n2    C    3.0\n3    D    NaN\n4    E    1.0\n\nSorted with NaN values first:\n  site  value\n1    B    NaN\n3    D    NaN\n4    E    1.0\n2    C    3.0\n0    A    5.0\n\n\nYou can control where NaN values appear using the na_position parameter.\n\n\nSorting a MultiIndex DataFrame\nWhen you perform certain merge operations or create pivot tables, you might end up with a MultiIndex DataFrame. Sorting these requires specifying which level to sort on:\n\n\nCode\n# Create a MultiIndex DataFrame\nmulti_df = pd.DataFrame({\n    'site': ['A', 'A', 'B', 'B'],\n    'year': [2022, 2023, 2022, 2023],\n    'value': [10, 12, 9, 11]\n}).set_index(['site', 'year'])\n\nprint(\"MultiIndex DataFrame:\")\nprint(multi_df)\n\n# Sort by the 'site' level of the index\nsorted_multi_df = multi_df.sort_index(level='site', ascending=False)\nprint(\"\\nSorted by 'site' level:\")\nprint(sorted_multi_df)\n\n# Sort by the 'value' column within each 'site'\nsorted_multi_df_values = multi_df.sort_values('value', ascending=False)\nprint(\"\\nSorted by 'value' within each 'site':\")\nprint(sorted_multi_df_values)\n\n\nMultiIndex DataFrame:\n           value\nsite year       \nA    2022     10\n     2023     12\nB    2022      9\n     2023     11\n\nSorted by 'site' level:\n           value\nsite year       \nB    2023     11\n     2022      9\nA    2023     12\n     2022     10\n\nSorted by 'value' within each 'site':\n           value\nsite year       \nA    2023     12\nB    2023     11\nA    2022     10\nB    2022      9\n\n\nThis is particularly useful when you’ve merged data that creates hierarchical relationships in your DataFrame.\n\n\nStable Sorting\nWhen sorting by multiple columns, you might want to preserve the relative order of rows with the same values. This is called stable sorting:\n\n\nCode\n# Create a DataFrame with duplicate values\ndf_duplicate = pd.DataFrame({\n    'category': ['A', 'B', 'A', 'B', 'A'],\n    'subcategory': [1, 1, 2, 2, 3],\n    'value': [10, 20, 30, 40, 50]\n})\n\nprint(\"DataFrame with duplicate values:\")\nprint(df_duplicate)\n\n# Perform a stable sort\ndf_stable_sort = df_duplicate.sort_values(['category', 'subcategory'], kind='mergesort')\nprint(\"\\nStable sort result:\")\nprint(df_stable_sort)\n\n\nDataFrame with duplicate values:\n  category  subcategory  value\n0        A            1     10\n1        B            1     20\n2        A            2     30\n3        B            2     40\n4        A            3     50\n\nStable sort result:\n  category  subcategory  value\n0        A            1     10\n2        A            2     30\n4        A            3     50\n1        B            1     20\n3        B            2     40\n\n\nUsing kind='mergesort' ensures a stable sort, which can be important when preserving the original order matters within your sorted groups.\nThese advanced sorting techniques will help you effectively organize and analyze complex datasets resulting from merge operations or other data manipulations."
  },
  {
    "objectID": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#key-points",
    "href": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#key-points",
    "title": "Interactive Session 6B",
    "section": "Key Points",
    "text": "Key Points\n\nConcatenation (pd.concat()) is useful for combining datasets with the same structure.\nAlways check and standardize column names before merging datasets.\nChoose the appropriate join type (inner, left, right, outer) based on your analysis needs.\nMerging on a common key allows you to combine information from different sources."
  },
  {
    "objectID": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#resources",
    "href": "course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#resources",
    "title": "Interactive Session 6B",
    "section": "Resources",
    "text": "Resources\n\nPandas Merging Documentation\nPandas Concat Documentation\n\nDon’t forget to check out our EDS 217 Cheatsheet on Merging and Joining for quick reference!\n\nEnd interactive session 6B"
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#getting-started",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#getting-started",
    "title": "Interactive Session 6A",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#introduction",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#introduction",
    "title": "Interactive Session 6A",
    "section": "Introduction",
    "text": "Introduction\nIn this session, we’ll explore essential data manipulation and analysis techniques in pandas, focusing on some simple examples. We’ll cover sorting, grouping, joining, working with dates, and applying custom transformations to data."
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#setup",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#setup",
    "title": "Interactive Session 6A",
    "section": "Setup",
    "text": "Setup\nLet’s start by importing the necessary libraries and creating a sample dataset:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a sample dataset of species observations\nnp.random.seed(42) \ndates = pd.date_range(start='2023-01-01', periods=100)\ndata = {\n    'date': dates,\n    'site': np.random.choice(['Forest', 'Grassland', 'Wetland'], 100),\n    'species': np.random.choice(['Oak', 'Maple', 'Pine', 'Birch'], 100),\n    'count': np.random.randint(1, 20, 100),\n    'temperature': np.random.normal(15, 5, 100)\n}\ndf = pd.DataFrame(data)\nprint(df.head())\n\n\n        date     site species  count  temperature\n0 2023-01-01  Wetland   Birch      7     9.043483\n1 2023-01-02   Forest   Birch      1    18.282768\n2 2023-01-03  Wetland   Birch      1    10.126592\n3 2023-01-04  Wetland    Pine     13    18.935423\n4 2023-01-05   Forest    Pine      9    20.792978"
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#sorting-data",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#sorting-data",
    "title": "Interactive Session 6A",
    "section": "1. Sorting Data",
    "text": "1. Sorting Data\nSorting is crucial in data analysis for identifying extremes, patterns, and organizing your data for subsequent analyses.\n\nBasic Sorting\n\n\nCode\n# Sort by species count\ndf_sorted = df.sort_values('count', ascending=False)\nprint(df_sorted.head())\n\n\n         date       site species  count  temperature\n12 2023-01-13     Forest     Oak     19    10.552428\n33 2023-02-03  Grassland   Maple     19    19.281994\n53 2023-02-23     Forest   Maple     19    20.677828\n55 2023-02-25    Wetland   Birch     19    18.256956\n81 2023-03-23     Forest   Birch     19    19.262167\n\n\n\n\nMulti-column Sorting\n\n\nCode\n# Sort by site and then by species count\ndf_multi_sorted = df.sort_values(['site', 'count'], ascending=[True, False])\nprint(df_multi_sorted.head())\n\n\n         date    site species  count  temperature\n12 2023-01-13  Forest     Oak     19    10.552428\n53 2023-02-23  Forest   Maple     19    20.677828\n81 2023-03-23  Forest   Birch     19    19.262167\n61 2023-03-03  Forest     Oak     18    15.409371\n95 2023-04-06  Forest   Maple     18    20.162326"
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#grouping-and-aggregating-data",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#grouping-and-aggregating-data",
    "title": "Interactive Session 6A",
    "section": "2. Grouping and Aggregating Data",
    "text": "2. Grouping and Aggregating Data\nGrouping allows us to analyze data at different ecological levels, from individual species to entire ecosystems.\n\nBasic Groupby\n\n\nCode\n# Sum of species counts by site\nsite_counts = df.groupby('site')['count'].sum()\nprint(site_counts)\n\n\nsite\nForest       311\nGrassland    336\nWetland      248\nName: count, dtype: int64\n\n\n\n\nMultiple Aggregations\n\n\nCode\n# Multiple stats by site\nsite_stats = df.groupby('site').agg({\n    'count': ['sum', 'mean', 'max'],\n    'species': 'nunique',\n    'temperature': 'mean'\n})\nprint(site_stats)\n\n\n          count               species temperature\n            sum      mean max nunique        mean\nsite                                             \nForest      311  9.424242  19       4   16.527332\nGrassland   336  9.333333  19       4   15.540037\nWetland     248  8.000000  19       4   14.528127"
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#joining-data",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#joining-data",
    "title": "Interactive Session 6A",
    "section": "3. Joining Data",
    "text": "3. Joining Data\nJoining data allows us to combine information from datasets.\n\n\nCode\n# Create a second DataFrame with site characteristics\nsite_data = pd.DataFrame({\n    'site': ['Forest', 'Grassland', 'Wetland'],\n    'soil_pH': [6.5, 7.2, 6.8],\n    'annual_rainfall': [1200, 800, 1500]\n})\n\n# Perform an inner join\nmerged_df = pd.merge(df, site_data, on='site', how='inner')\nprint(merged_df.head())\n\n\n        date     site species  count  temperature  soil_pH  annual_rainfall\n0 2023-01-01  Wetland   Birch      7     9.043483      6.8             1500\n1 2023-01-02   Forest   Birch      1    18.282768      6.5             1200\n2 2023-01-03  Wetland   Birch      1    10.126592      6.8             1500\n3 2023-01-04  Wetland    Pine     13    18.935423      6.8             1500\n4 2023-01-05   Forest    Pine      9    20.792978      6.5             1200\n\n\nTypes of joins and when to use them:\n\nInner Join: Use when you want to combine two datasets based on a common key, keeping only the records that have matches in both datasets. This is useful when you’re interested in analyzing only the data points that have complete information across both sources.\nLeft Join: Use when you want to keep all records from the left (primary) dataset and match them with records from the right (secondary) dataset where possible. This is helpful when you want to preserve all information from your main dataset while enriching it with additional data where available.\nRight Join: Similar to a left join, but keeps all records from the right dataset. This is less common but can be useful when you want to ensure all records from a secondary dataset are included, even if they don’t have corresponding entries in the primary dataset.\nOuter Join: Use when you want to combine all unique records from both datasets, regardless of whether they have matches or not. This creates a comprehensive dataset that includes all information from both sources, filling in with null values where there’s no match.\n\nUse cases: - Combining observation data with site characteristics - Merging disparate datasets that share a location or timestamp."
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#working-with-dates",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#working-with-dates",
    "title": "Interactive Session 6A",
    "section": "4. Working with Dates",
    "text": "4. Working with Dates\nDate manipulation is crucial for analyzing seasonal patterns, long-term trends, and time-sensitive events.\n\n\nCode\n# Set date as index\ndf.set_index('date', inplace=True)\nprint(df.head())\n\n# Resample to monthly data\nmonthly_counts = df.resample('M')['count'].sum()\nprint(monthly_counts.head())\n\n\n               site species  count  temperature\ndate                                           \n2023-01-01  Wetland   Birch      7     9.043483\n2023-01-02   Forest   Birch      1    18.282768\n2023-01-03  Wetland   Birch      1    10.126592\n2023-01-04  Wetland    Pine     13    18.935423\n2023-01-05   Forest    Pine      9    20.792978\ndate\n2023-01-31    275\n2023-02-28    232\n2023-03-31    297\n2023-04-30     91\nFreq: ME, Name: count, dtype: int64\n\n\n/var/folders/bs/x9tn9jz91cv6hb3q6p4djbmw0000gn/T/ipykernel_60482/881548494.py:6: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n  monthly_counts = df.resample('M')['count'].sum()\n\n\n\nUnderstanding inplace=True\nThe inplace=True parameter modifies the original DataFrame directly:\n\n\nCode\n# Without inplace=True (creates a new DataFrame)\ndf_new = df.reset_index()\nprint(\"\\nNew DataFrame with reset index:\")\nprint(df_new.head())\nprint(\"\\nOriginal DataFrame (unchanged):\")\nprint(df.head())\n\n# With inplace=True (modifies the original DataFrame)\ndf.reset_index(inplace=True)\nprint(\"\\nOriginal DataFrame after reset_index(inplace=True):\")\nprint(df.head())\n\n\n\nNew DataFrame with reset index:\n        date     site species  count  temperature\n0 2023-01-01  Wetland   Birch      7     9.043483\n1 2023-01-02   Forest   Birch      1    18.282768\n2 2023-01-03  Wetland   Birch      1    10.126592\n3 2023-01-04  Wetland    Pine     13    18.935423\n4 2023-01-05   Forest    Pine      9    20.792978\n\nOriginal DataFrame (unchanged):\n               site species  count  temperature\ndate                                           \n2023-01-01  Wetland   Birch      7     9.043483\n2023-01-02   Forest   Birch      1    18.282768\n2023-01-03  Wetland   Birch      1    10.126592\n2023-01-04  Wetland    Pine     13    18.935423\n2023-01-05   Forest    Pine      9    20.792978\n\nOriginal DataFrame after reset_index(inplace=True):\n        date     site species  count  temperature\n0 2023-01-01  Wetland   Birch      7     9.043483\n1 2023-01-02   Forest   Birch      1    18.282768\n2 2023-01-03  Wetland   Birch      1    10.126592\n3 2023-01-04  Wetland    Pine     13    18.935423\n4 2023-01-05   Forest    Pine      9    20.792978\n\n\nWhen to use inplace=True: - When preprocessing large datasets to save memory - In data cleaning pipelines for time series - When you’re sure you won’t need the original version of the data\nWhen not to use inplace=True: - When you need to preserve the original dataset for comparison - In functions where you want to return a modified copy without altering the input - When working with shared datasets that other parts of your analysis might depend on\n\n\nDate Filtering and Analysis\n\n\nCode\n# Filter by date range (e.g., spring season)\nspring_data = df[(df['date'] &gt;= '2023-03-01') & (df['date'] &lt; '2023-06-01')]\nprint(spring_data.head())\n\n# Extract date components\ndf['month'] = df['date'].dt.month\ndf['day_of_year'] = df['date'].dt.dayofyear\nprint(df.head())\n\n\n         date       site species  count  temperature\n59 2023-03-01    Wetland   Birch      8    13.815907\n60 2023-03-02  Grassland    Pine      7    12.573182\n61 2023-03-03     Forest     Oak     18    15.409371\n62 2023-03-04  Grassland   Birch      8    26.573293\n63 2023-03-05  Grassland     Oak      1     5.663674\n        date     site species  count  temperature  month  day_of_year\n0 2023-01-01  Wetland   Birch      7     9.043483      1            1\n1 2023-01-02   Forest   Birch      1    18.282768      1            2\n2 2023-01-03  Wetland   Birch      1    10.126592      1            3\n3 2023-01-04  Wetland    Pine     13    18.935423      1            4\n4 2023-01-05   Forest    Pine      9    20.792978      1            5\n\n\nWhen to use date manipulation: - Analyzing seasonal patterns - Studying time-specific events (e.g., flowering times, migration patterns) - Creating time-based features for models - Aligning climate data with other observations (resampling)"
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#using-df.apply-to-transform-data",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#using-df.apply-to-transform-data",
    "title": "Interactive Session 6A",
    "section": "5. Using df.apply() to transform data",
    "text": "5. Using df.apply() to transform data\nThe apply() function allows you to apply custom calculations to your data.\n\n\nCode\n# Apply a function to categorize temperature\ndef categorize_temperature(value):\n    if value &lt; 10:\n        return 'Cold'\n    elif value &lt; 20:\n        return 'Moderate'\n    else:\n        return 'Warm'\n\ndf['temp_category'] = df['temperature'].apply(categorize_temperature)\nprint(df.head())\n\n# Apply a function to calculate biodiversity index\ndef simpson_diversity(row):\n    n = row['count']\n    N = df.loc[df['site'] == row['site'], 'count'].sum()\n    return 1 - (n * (n - 1)) / (N * (N - 1))\n\ndf['simpson_index'] = df.apply(simpson_diversity, axis=1)\nprint(df.head())\n\n\n        date     site species  count  temperature  month  day_of_year  \\\n0 2023-01-01  Wetland   Birch      7     9.043483      1            1   \n1 2023-01-02   Forest   Birch      1    18.282768      1            2   \n2 2023-01-03  Wetland   Birch      1    10.126592      1            3   \n3 2023-01-04  Wetland    Pine     13    18.935423      1            4   \n4 2023-01-05   Forest    Pine      9    20.792978      1            5   \n\n  temp_category  \n0          Cold  \n1      Moderate  \n2      Moderate  \n3      Moderate  \n4          Warm  \n        date     site species  count  temperature  month  day_of_year  \\\n0 2023-01-01  Wetland   Birch      7     9.043483      1            1   \n1 2023-01-02   Forest   Birch      1    18.282768      1            2   \n2 2023-01-03  Wetland   Birch      1    10.126592      1            3   \n3 2023-01-04  Wetland    Pine     13    18.935423      1            4   \n4 2023-01-05   Forest    Pine      9    20.792978      1            5   \n\n  temp_category  simpson_index  \n0          Cold       0.999314  \n1      Moderate       1.000000  \n2      Moderate       1.000000  \n3      Moderate       0.997453  \n4          Warm       0.999253  \n\n\nWhen to use apply(): - Calculating complex indices (e.g., biodiversity measures) - Applying models to observational data - Implementing custom data cleaning rules - Performing category-specific calculations\nRemember, while apply() is versatile, it can be slower than vectorized operations for large datasets. Always consider if there’s a vectorized alternative, especially when working with big datasets."
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#reshaping-dataframes-with-pivot-tables",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#reshaping-dataframes-with-pivot-tables",
    "title": "Interactive Session 6A",
    "section": "6. Reshaping DataFrames with Pivot Tables",
    "text": "6. Reshaping DataFrames with Pivot Tables\nPivot tables provide a way to reshape data and calculate aggregations in one step.\n\nHow Pivot Tables Work\n\nReshaping Data: Pivot tables reshape data by turning unique values from one column into multiple columns.\nAggregation: They perform aggregations on a specified value column for each unique group created by the new columns.\nIndex and Columns: You specify which column to use as the new index, which to use as new columns, and which to aggregate.\n\nThe idea is very similar to the df.pivot command: \nThe main difference between df.pivot and df.pivot_table is that df.pivot_table includes aggregation.\nLet’s see an example:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Create a sample DataFrame\ndata = {\n    'date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02'],\n    'city': ['New York', 'Los Angeles', 'New York', 'Los Angeles'],\n    'temperature': [32, 68, 28, 72]\n}\ndf = pd.DataFrame(data)\nprint(\"Original DataFrame:\")\nprint(df)\n\n\nOriginal DataFrame:\n         date         city  temperature\n0  2023-01-01     New York           32\n1  2023-01-01  Los Angeles           68\n2  2023-01-02     New York           28\n3  2023-01-02  Los Angeles           72\n\n\n\nUsing df.pivot to rotate the dataframe:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\npivot = pd.pivot(df, values='temperature', index='date', columns='city')\nprint(\"\\nPivot:\")\nprint(pivot)\n\n\n\nPivot:\ncity        Los Angeles  New York\ndate                             \n2023-01-01           68        32\n2023-01-02           72        28\n\n\n\n\nUsing df.pivot_table to create a pivot table:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Create a pivot table\npivot_table = df.pivot_table(values='temperature', index='date', columns='city', aggfunc='mean')\nprint(\"\\nPivot Table:\")\nprint(pivot_table)\n\n\n\nPivot Table:\ncity        Los Angeles  New York\ndate                             \n2023-01-01         68.0      32.0\n2023-01-02         72.0      28.0\n\n\nIn this example: - ‘date’ becomes the index - ‘city’ values become new columns - ‘temperature’ values are aggregated (mean) for each date-city combination\n\n\n\n\n\n\nNote\n\n\n\nIn this example, the result of our pivot and pivot_table commands are essentially the same. Why is that the case? When would we expect different results from these two commands?\n\n\n\n\n\nKey Features of Pivot Tables\n\nHandling Duplicates: If there are multiple values for a given index-column combination, an aggregation function (like mean, sum, count) must be specified.\nMissing Data: Pivot tables can reveal missing data, often filling these gaps with NaN.\nMulti-level Index: You can create multi-level indexes and columns for more complex reorganizations.\nFlexibility: You can pivot on multiple columns and use multiple value columns.\n\nPivot tables are especially useful for: - Summarizing large datasets - Creating cross-tabulations - Preparing data for visualization - Identifying patterns or trends across categories\nRemember, while pivot tables are powerful, they work best with well-structured data and clear categorical variables.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Create a pivot table\npivot = df.pivot_table(values='temperature', index='city', columns='date', aggfunc='mean')\nprint(pivot)\n\n\ndate         2023-01-01  2023-01-02\ncity                               \nLos Angeles        68.0        72.0\nNew York           32.0        28.0\n\n\nTry creating a pivot table that shows the maximum humidity for each city and date.\n\n✏️ Try it. Add the cell below to your notebook and then provide your code.\n\n\n\nCode\n# Your code here"
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#key-points",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#key-points",
    "title": "Interactive Session 6A",
    "section": "Key Points",
    "text": "Key Points\n\nGrouping allows us to split data based on categories and perform operations on each group.\nThe groupby() function is the primary tool for grouping in Pandas.\nWe can apply various aggregation functions to grouped data.\nMulti-level grouping creates a hierarchical index.\nPivot tables offer a way to reshape and aggregate data simultaneously."
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#conclusion",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting.html#conclusion",
    "title": "Interactive Session 6A",
    "section": "Conclusion",
    "text": "Conclusion\nThese techniques form the foundation of data manipulation and analysis in pandas. By understanding when and how to use each method, you can efficiently process and gain insights from datasets. Next you’ll applying these concepts to different scenarios to strengthen your skills in environmental data science."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day4.html#introduction",
    "href": "course-materials/eod-practice/eod-day4.html#introduction",
    "title": "Day 4: Tasks & activities",
    "section": "Introduction",
    "text": "Introduction\nThis end-of-day session is focused on using pandas for loading, visualizing, and analyzing marine microplastics data. This session is designed to help you become more comfortable with the pandas library, equipping you with the skills needed to perform data analysis effectively.\nThe National Oceanic and Atmospheric Administration, via its National Centers for Environmental Information has an entire section related to marine microplastics – that is, microplastics found in water — at https://www.ncei.noaa.gov/products/microplastics.\nWe will be working with a recent download of the entire marine microplastics dataset. The url for this data is located here:\n\n\nCode\nurl = 'https://ucsb.box.com/shared/static/dnnu59jsnkymup6o8aaovdywrtxiy3a9.csv'\n\n\nObjective: Write your own notebook that contains a simple DataFrame exploration as well as some basic grouping, filtering, and aggregation, and visualization… all within the pandas library."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day4.html#loading-the-data",
    "href": "course-materials/eod-practice/eod-day4.html#loading-the-data",
    "title": "Day 4: Tasks & activities",
    "section": "1. Loading the Data",
    "text": "1. Loading the Data\nObjective: Learn to load data into a pandas DataFrame and display the first few records.\n\nTask 1.1 Import the pandas library.\n\n\nTask 1.2: Load the dataset into a dataframe named df from the provided URL into a pandas DataFrame.\n\n\n\n\n\n\nTip\n\n\n\nI’ve already taken a look at this data set and noticed there was a column with sample date called Date. We can use the parse_date option of the read_csv() function to convert values in the Dates column of the csv into datetime objects in pandas while reading the file.\n\n\n\n\nTask 1.3:\n\nDisplay the first five rows of the DataFrame to get an initial understanding of the data structure."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day4.html#exploring-the-data",
    "href": "course-materials/eod-practice/eod-day4.html#exploring-the-data",
    "title": "Day 4: Tasks & activities",
    "section": "2. Exploring the Data",
    "text": "2. Exploring the Data\n\nTask 2.1:\n\nDisplay summary statistics of the dataset to understand the central tendency and variability.\n\n\n\nTask 2.2:\n\nCheck the data types of each column and identify if there are any missing values.\nRemove any records that are missing a value in the Oceans\n\n\n\n\n\n\n\nUsing ~ to invert built-in function results\n\n\n\nThe ~ operator inverts a list of Boolean values (switches True to False and vice versa).\nThis operator isn’t useful for most selection operations because you can just use == and != to invert selection criteria. However, the ~ operator becomes very handy when there is a need to invert the results of a built-in function.\nFor example, the use of the ~ operator and isnull() combine to create an efficient way to filter dataframes where the value of a df[column] is not isnull():\ndf_valid = df[~(df['column'].isnull())].copy()\nNote that the results of the built-in function - df['column'].isnull() need to be wrapped in ( ) for the ~ operator to work properly."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day4.html#grouping-the-data",
    "href": "course-materials/eod-practice/eod-day4.html#grouping-the-data",
    "title": "Day 4: Tasks & activities",
    "section": "3. Grouping the Data",
    "text": "3. Grouping the Data\n\nTask 3.1:\n\nCreate a groupby object called oceans that groups the data in df according to the value of the Oceans column.\n\n\n\nTask 3.2:\n\nDetermine the total number of Measurements taken from each Ocean.\n\n\n\nTask 3.3:\n\nDetermine the average value of Measurement taken from each Ocean."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day4.html#filtering-the-data-and-aggregating-the-data",
    "href": "course-materials/eod-practice/eod-day4.html#filtering-the-data-and-aggregating-the-data",
    "title": "Day 4: Tasks & activities",
    "section": "4. Filtering the Data and Aggregating the data",
    "text": "4. Filtering the Data and Aggregating the data\n\nTask 4.1:\n\nFilter the data to a new df (called df2) that only contains rows where the Unit of measurement is pieces/m3\n\n\n\nTask 4.2:\n\nUse the groupby and the max() command to determine the Maximum value of pieces/m3 measured for each Ocean"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day4.html#simple-plots-in-pandas",
    "href": "course-materials/eod-practice/eod-day4.html#simple-plots-in-pandas",
    "title": "Day 4: Tasks & activities",
    "section": "5. Simple Plots in pandas",
    "text": "5. Simple Plots in pandas\n\nTask 5.1:\n\nMake a histogram of the latitude of every sample in your filtered dataframe using the DataFrame plot command.\n\n\n\nTask 5.2:\n\nMake a new dataframe (df3) from your filtered dataframe (df2) that contains only rows where Measurement is greater than zero.\n\n\n\n\n\n\n\nImportant\n\n\n\nUsing .copy() when filtering a dataframe ensures that you’re working with a new DataFrame, not a view of the original. This is especially important when you’re filtering data and then modifying the result, which is common in data science workflows.\n\n\n\n\nTask 5.3\n\nCreate a new column in df3 that contains the log10 of Measurements.\n\n\n\n\n\n\n\nTip\n\n\n\nThe numpy library has a log10() function that you will find useful for this step!\n\n\n\n\nTask 5.4\n\nMake a histogram of the log-transformed values in df3"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day4.html#conclusion",
    "href": "course-materials/eod-practice/eod-day4.html#conclusion",
    "title": "Day 4: Tasks & activities",
    "section": "Conclusion",
    "text": "Conclusion\n🎉 Congratulations, you’re officially doing python data science! 🎉\nBe sure to save your notebook and add comments and reflections at the end of your notebook before heading out for the day.\n\nEnd Activity Session (Day 4)"
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes_original.html#getting-started",
    "href": "course-materials/interactive-sessions/4a_dataframes_original.html#getting-started",
    "title": "Interactive Session 4A",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes_original.html#introduction-to-pandas-dataframes",
    "href": "course-materials/interactive-sessions/4a_dataframes_original.html#introduction-to-pandas-dataframes",
    "title": "Interactive Session 4A",
    "section": "Introduction to pandas DataFrames",
    "text": "Introduction to pandas DataFrames\nIn this interactive session, we’ll explore the fundamental concepts of pandas DataFrames, their relationship to Series, and some essential methods for working with them."
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes_original.html#instructions",
    "href": "course-materials/interactive-sessions/4a_dataframes_original.html#instructions",
    "title": "Interactive Session 4A",
    "section": "Instructions",
    "text": "Instructions\nWe will work through this material together, writing a new notebook as we go.\n\n\n\n\n\n\nNote\n\n\n\n🐍     This symbol designates an important note about Python structure, syntax, or another quirk.\n\n\n\n✏️     This symbol designates code you should add to your notebook and run."
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes_original.html#setting-up-our-environment",
    "href": "course-materials/interactive-sessions/4a_dataframes_original.html#setting-up-our-environment",
    "title": "Interactive Session 4A",
    "section": "Setting up our environment",
    "text": "Setting up our environment\nLet’s start by importing the necessary libraries:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes_original.html#understanding-series",
    "href": "course-materials/interactive-sessions/4a_dataframes_original.html#understanding-series",
    "title": "Interactive Session 4A",
    "section": "Understanding Series",
    "text": "Understanding Series\nBefore diving into DataFrames, let’s briefly review pandas Series, as they form the building blocks of DataFrames.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Creating a Series\ns = pd.Series([1, 3, 5, np.nan, 6, 8])\nprint(s)\n\n\n0    1.0\n1    3.0\n2    5.0\n3    NaN\n4    6.0\n5    8.0\ndtype: float64\n\n\nA Series is a one-dimensional labeled array that can hold data of any type."
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes_original.html#introduction-to-dataframes",
    "href": "course-materials/interactive-sessions/4a_dataframes_original.html#introduction-to-dataframes",
    "title": "Interactive Session 4A",
    "section": "Introduction to DataFrames",
    "text": "Introduction to DataFrames\nA DataFrame is a two-dimensional labeled data structure with columns of potentially different types. You can think of it as a table or a spreadsheet-like structure.\nLet’s create a simple DataFrame:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Creating a DataFrame\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4],\n    'B': pd.date_range(start='2023-01-01', periods=4),\n    'C': pd.Series(1, index=list(range(4)), dtype='float32'),\n    'D': np.array([3] * 4, dtype='int32'),\n    'E': pd.Categorical([\"test\", \"train\", \"test\", \"train\"]),\n    'F': 'foo'\n})\n\nprint(df)\n\n\n   A          B    C  D      E    F\n0  1 2023-01-01  1.0  3   test  foo\n1  2 2023-01-02  1.0  3  train  foo\n2  3 2023-01-03  1.0  3   test  foo\n3  4 2023-01-04  1.0  3  train  foo\n\n\nHere, we’ve created a DataFrame with different types of data: integers, dates, floats, categories, and strings."
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes_original.html#dataframes-and-series-relationship",
    "href": "course-materials/interactive-sessions/4a_dataframes_original.html#dataframes-and-series-relationship",
    "title": "Interactive Session 4A",
    "section": "DataFrames and Series Relationship",
    "text": "DataFrames and Series Relationship\nEach column in a DataFrame is a Series. You can access a column like this:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(df['A'])\n\n\n0    1\n1    2\n2    3\n3    4\nName: A, dtype: int64\n\n\nThis returns a Series object. You can confirm this:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(type(df['A']))\n\n\n&lt;class 'pandas.core.series.Series'&gt;"
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes_original.html#investigating-dataframe-structure",
    "href": "course-materials/interactive-sessions/4a_dataframes_original.html#investigating-dataframe-structure",
    "title": "Interactive Session 4A",
    "section": "Investigating DataFrame Structure",
    "text": "Investigating DataFrame Structure\npandas provides several methods to investigate the structure of a DataFrame:\n\n1. Shape\nTo get the dimensions of the DataFrame:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(df.shape)\n\n\n(4, 6)\n\n\nThis returns a tuple where the first element is the number of rows, and the second is the number of columns.\n\n\n2. Info\nFor a concise summary of the DataFrame:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 4 entries, 0 to 3\nData columns (total 6 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   A       4 non-null      int64         \n 1   B       4 non-null      datetime64[ns]\n 2   C       4 non-null      float32       \n 3   D       4 non-null      int32         \n 4   E       4 non-null      category      \n 5   F       4 non-null      object        \ndtypes: category(1), datetime64[ns](1), float32(1), int32(1), int64(1), object(1)\nmemory usage: 288.0+ bytes\n\n\nThis method prints information about the DataFrame including the index dtype and columns, non-null values and memory usage.\n\n\n3. Dtypes\nTo see the data types of each column:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(df.dtypes)\n\n\nA             int64\nB    datetime64[ns]\nC           float32\nD             int32\nE          category\nF            object\ndtype: object\n\n\n\n\n4. Describe\nTo see some basic statistical details of numerical columns:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(df.describe())\n\n\n              A                    B    C    D\ncount  4.000000                    4  4.0  4.0\nmean   2.500000  2023-01-02 12:00:00  1.0  3.0\nmin    1.000000  2023-01-01 00:00:00  1.0  3.0\n25%    1.750000  2023-01-01 18:00:00  1.0  3.0\n50%    2.500000  2023-01-02 12:00:00  1.0  3.0\n75%    3.250000  2023-01-03 06:00:00  1.0  3.0\nmax    4.000000  2023-01-04 00:00:00  1.0  3.0\nstd    1.290994                  NaN  0.0  0.0"
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes_original.html#accessing-dataframe-contents",
    "href": "course-materials/interactive-sessions/4a_dataframes_original.html#accessing-dataframe-contents",
    "title": "Interactive Session 4A",
    "section": "Accessing DataFrame Contents",
    "text": "Accessing DataFrame Contents\nThere are multiple ways to access data within a DataFrame. The easiest and most direct way is to use the head() and tail() command. The head() method shows the first (tail() shows the last) 10 values by default (if no argument is provided). If you provide a number to the command, then that is the number of values returned.\n\n1. Head and Tail\nTo view the first or last few rows:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(df.head(2))  # First 2 rows\nprint(\"\\n\")\nprint(df.tail(2))  # Last 2 rows\n\n\n   A          B    C  D      E    F\n0  1 2023-01-01  1.0  3   test  foo\n1  2 2023-01-02  1.0  3  train  foo\n\n\n   A          B    C  D      E    F\n2  3 2023-01-03  1.0  3   test  foo\n3  4 2023-01-04  1.0  3  train  foo\n\n\n\n\n2. Indexing\nYou can access rows and columns using various indexing methods:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Selecting a single column\nprint(df['A'])\n\n# Selecting multiple columns\nprint(df[['A', 'B']])\n\n# Selecting rows by position\nprint(df.iloc[0])  # First row\n\n# Selecting rows by label\nprint(df.loc[0])  # First row (assuming default integer index)\n\n# Selecting both rows and columns\nprint(df.loc[0, 'A'])  # Value at first row of column 'A'\n\n\n0    1\n1    2\n2    3\n3    4\nName: A, dtype: int64\n   A          B\n0  1 2023-01-01\n1  2 2023-01-02\n2  3 2023-01-03\n3  4 2023-01-04\nA                      1\nB    2023-01-01 00:00:00\nC                    1.0\nD                      3\nE                   test\nF                    foo\nName: 0, dtype: object\nA                      1\nB    2023-01-01 00:00:00\nC                    1.0\nD                      3\nE                   test\nF                    foo\nName: 0, dtype: object\n1"
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes_original.html#basic-dataframe-operations",
    "href": "course-materials/interactive-sessions/4a_dataframes_original.html#basic-dataframe-operations",
    "title": "Interactive Session 4A",
    "section": "Basic DataFrame Operations",
    "text": "Basic DataFrame Operations\nLet’s look at some basic operations you can perform on DataFrames:\n\n1. Adding a new column\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\ndf['G'] = df['A'] + df['D']\nprint(df)\n\n\n   A          B    C  D      E    F  G\n0  1 2023-01-01  1.0  3   test  foo  4\n1  2 2023-01-02  1.0  3  train  foo  5\n2  3 2023-01-03  1.0  3   test  foo  6\n3  4 2023-01-04  1.0  3  train  foo  7\n\n\n\n\n2. Applying functions to columns\nLet’s define a simpe function and use apply to apply that function to every row in a column:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\ndef square(x):\n    return x ** 2\n\ndf['A_squared'] = df['A'].apply(square)\nprint(df)\n\n\n   A          B    C  D      E    F  G  A_squared\n0  1 2023-01-01  1.0  3   test  foo  4          1\n1  2 2023-01-02  1.0  3  train  foo  5          4\n2  3 2023-01-03  1.0  3   test  foo  6          9\n3  4 2023-01-04  1.0  3  train  foo  7         16\n\n\n\n\n3. Basic statistics\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(df['A'].mean())  # Mean of column A\nprint(df['D'].sum())   # Sum of column D\n\n\n2.5\n12"
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes_original.html#conclusion",
    "href": "course-materials/interactive-sessions/4a_dataframes_original.html#conclusion",
    "title": "Interactive Session 4A",
    "section": "Conclusion",
    "text": "Conclusion\nThis session introduced you to the basics of pandas DataFrames, their relationship to Series, and some fundamental methods for exploring and manipulating them.\nIn future sessions, we’ll dive deeper into more advanced operations like selection, filtering, grouping, and data cleaning.\nRemember, DataFrames are powerful tools for data manipulation and analysis, built upon the concept of Series.\nUnderstanding their structure and basic operations is crucial for effective data analysis with pandas.\n\nPandas Cheat Sheet\nDataFrame Cheatsheet\n\n\nEnd interactive session 4A"
  },
  {
    "objectID": "course-materials/interactive-sessions/1c_variables_strings.html",
    "href": "course-materials/interactive-sessions/1c_variables_strings.html",
    "title": "Interactive Session 1C",
    "section": "",
    "text": "Understanding the types of variables and the methods available for different objects is crucial for effective programming in Python. This guide will walk you through how to determine the type of a variable and explore the methods you can use with various objects."
  },
  {
    "objectID": "course-materials/interactive-sessions/1c_variables_strings.html#getting-started",
    "href": "course-materials/interactive-sessions/1c_variables_strings.html#getting-started",
    "title": "Interactive Session 1C",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab (see the note below if you have trouble!)\nSelect “Rename”\nName your notebook with the format: Session_1C_Variables_and_Strings.ipynb\n\n\n\n\n\n\n\n\nRight-clicking in JupyterLab\n\n\n\nSome browsers and operating system combinations will not conceded right-clicking to the JupyterLab interface and will show a system menu when you try to right click. In those cases, usually CTRL-Right Click or OPTION-Right Click will bring up the Jupyter menu.\n\n\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day 1: Session C - Variables & Strings\n\n[Session Webpage](https://eds-217-essential-python.github.io/course-materials/interactive-sessions/1c_variables_strings.html)\n\nDate: 09/03/2024\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "course-materials/interactive-sessions/1c_variables_strings.html#variable-types",
    "href": "course-materials/interactive-sessions/1c_variables_strings.html#variable-types",
    "title": "Interactive Session 1C",
    "section": "Variable Types",
    "text": "Variable Types\nIn Python, everything is an object. Each object has a specific type, and knowing the type of a variable helps you understand what operations and methods you can perform on it.\n\nDetermining Variable Type\nTo find out the type of a variable, you can use the type() function. This function returns the type of the object passed to it.\n\n\nCode\n# Define some variables\nnumber = 42\ntext = \"Hello, World!\"\npi = 3.14159\ndata = [1, 2, 3, 4, 5]\n\n# Determine the type of each variable\nprint(type(number))\nprint(type(text))\nprint(type(pi))\nprint(type(data))\n\n\n&lt;class 'int'&gt;\n&lt;class 'str'&gt;\n&lt;class 'float'&gt;\n&lt;class 'list'&gt;\n\n\n\n\nBuilt-in Types\nHere are some common built-in types in Python:\n\nint: Represents integers.\nfloat: Represents floating-point numbers.\nstr: Represents strings.\nlist: Represents lists, which are ordered collections of items."
  },
  {
    "objectID": "course-materials/interactive-sessions/1c_variables_strings.html#exploring-methods",
    "href": "course-materials/interactive-sessions/1c_variables_strings.html#exploring-methods",
    "title": "Interactive Session 1C",
    "section": "Exploring Methods",
    "text": "Exploring Methods\nOnce you know the type of an object, you can discover the methods available for that object. Methods are functions that belong to an object and can be used to perform operations on the data contained within the object.\n\nDiscovering Methods\nYou can use the dir() function to list all the attributes and methods available for an object. This function returns a list of the object’s attributes and methods, including special methods (also known as “dunder” methods) that begin and end with double underscores.\n\n\nCode\n# List all methods and attributes of a string object\nstring_methods = dir(text)\nprint(string_methods)\n\n\n['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'removeprefix', 'removesuffix', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n\n\n\n\nFiltering User-Facing Methods\nWhile dir() lists all methods, you often only need user-facing methods. You can filter out special methods by ignoring those with double underscores.\n\n\nCode\n# Filter out special methods\nuser_methods = [method for method in dir(text) if not method.startswith('__')]\nprint(user_methods)\n\n\n['capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'removeprefix', 'removesuffix', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']"
  },
  {
    "objectID": "course-materials/interactive-sessions/1c_variables_strings.html#using-methods",
    "href": "course-materials/interactive-sessions/1c_variables_strings.html#using-methods",
    "title": "Interactive Session 1C",
    "section": "Using Methods",
    "text": "Using Methods\nNow that you’ve discovered the methods available for an object, let’s see how to use them. Here are some examples with strings and lists.\n\nString Methods\nStrings in Python have various methods for text manipulation. Let’s look at a few commonly used methods.\n\nExample: upper(), lower(), and replace(), strip()\n\n\nCode\ntext = \" Hello, World!  \"\n\n# Convert to uppercase\nprint(text.upper())\n\n# Convert to lowercase\nprint(text.lower())\n\n# Replace a substring\nprint(text.replace(\"World\", \"Python\"))\n\n# Remove leading and trailing whitespace, including tabs and newlines\nprint(text.strip())\n\n\n HELLO, WORLD!  \n hello, world!  \n Hello, Python!  \nHello, World!\n\n\n\n\nAdvanced Example: String Normalization\nIn this section, we’ll explore how to normalize strings using built-in Python string methods. String normalization is a common task in data processing and can involve operations like converting text to lowercase, removing extra whitespace, and replacing spaces with other characters. For example, in R, the janitor package is often used to normalize dataframe columns names.\n\nBasic String Normalization\nLet’s start with a simple example of string normalization:\n\n\nCode\n# A string with mixed case and extra spaces\noriginal = \"  Hello, World!  \"\n\n# Normalize the string using the .strip() method to remove whitespace \n# and the .lower() method to convert to lowercase\nnormalized = original.strip().lower()\n\nprint(f\"Original: '{original}'\")\nprint(f\"Normalized: '{normalized}'\")\n\n\nOriginal: '  Hello, World!  '\nNormalized: 'hello, world!'\n\n\nIn this example, we used two string methods: - strip(): Removes leading and trailing whitespace - lower(): Converts the string to lowercase\n\n\n\nAdvanced String Normalization\nNow, let’s look at a more comprehensive approach to string normalization:\n\n\nCode\n# Examples of strings to normalize\nexamples = [\n    \"Hello World\",\n    \"  Python Programming  \",\n    \"STRING NORMALIZATION\\n\",\n    \"  Trim  Spaces  \",\n    \"\\tTabs and Newlines\\n\"\n]\n\nprint(\"String Normalization Examples:\")\nprint(\"==============================\")\n\nfor original in examples:\n    # Normalize the string:\n    # 1. Remove leading/trailing whitespace and newlines\n    # 2. Convert to lowercase\n    # 3. Replace internal whitespace with underscores\n    normalized = original.strip().lower().replace(\" \", \"_\")\n    \n    print(f\"Original: '{original}'\")\n    print(f\"Normalized: '{normalized}'\")\n    print()\n\n\nString Normalization Examples:\n==============================\nOriginal: 'Hello World'\nNormalized: 'hello_world'\n\nOriginal: '  Python Programming  '\nNormalized: 'python_programming'\n\nOriginal: 'STRING NORMALIZATION\n'\nNormalized: 'string_normalization'\n\nOriginal: '  Trim  Spaces  '\nNormalized: 'trim__spaces'\n\nOriginal: ' Tabs and Newlines\n'\nNormalized: 'tabs_and_newlines'\n\n\n\nIn this advanced example, we:\n\nStart with strip() to remove all leading and trailing whitespace, including spaces, tabs, and newlines.\nUse lower() to convert the string to lowercase.\nFinally, apply replace(\" \", \"_\") to substitute internal spaces with underscores.\n\n\n\nExercise\nNow it’s your turn to practice! Modify the code below to normalize the given strings according to these rules:\n\nRemove leading and trailing whitespace\nConvert to uppercase (instead of lowercase)\nReplace spaces with hyphens\n\n\n\nCode\nstrings_to_normalize = [\n    \"data science\",\n    \"  MACHINE learning \",\n    \"Artificial Intelligence\\t\",\n    \" Natural\\nLanguage Processing \"\n]\n\nprint(\"Your Normalized Strings:\")\nprint(\"========================\")\n\nfor string in strings_to_normalize:\n    # Your normalization code here\n    normalized = string  # Replace this line with your normalization steps!\n    \n    print(f\"Original: '{string}'\")\n    print(f\"Normalized: '{normalized}'\")\n    print()\n\n\nYour Normalized Strings:\n========================\nOriginal: 'data science'\nNormalized: 'data science'\n\nOriginal: '  MACHINE learning '\nNormalized: '  MACHINE learning '\n\nOriginal: 'Artificial Intelligence  '\nNormalized: 'Artificial Intelligence    '\n\nOriginal: ' Natural\nLanguage Processing '\nNormalized: ' Natural\nLanguage Processing '\n\n\n\nTry experimenting with different normalization techniques or adding your own strings to the list!\n\n\n\nList Methods\nLists also have several useful methods for data manipulation. Here are some examples.\n\nExample: append(), remove(), and sort()\n\n\nCode\nnumbers = [3, 1, 4, 1, 5, 9]\n\n# Append an item\nnumbers.append(2)\nprint(numbers)\n\n# Remove an item\nnumbers.remove(1)  # Removes the first occurrence of 1\nprint(numbers)\n\n# Sort the list\nnumbers.sort()\nprint(numbers)\n\n\n[3, 1, 4, 1, 5, 9, 2]\n[3, 4, 1, 5, 9, 2]\n[1, 2, 3, 4, 5, 9]"
  },
  {
    "objectID": "course-materials/interactive-sessions/1c_variables_strings.html#string-formatting-in-python-what-youll-encounter-in-the-wild",
    "href": "course-materials/interactive-sessions/1c_variables_strings.html#string-formatting-in-python-what-youll-encounter-in-the-wild",
    "title": "Interactive Session 1C",
    "section": "String Formatting in Python: What You’ll Encounter in the Wild",
    "text": "String Formatting in Python: What You’ll Encounter in the Wild\nAn essential skill in data science is creating clear, readable output. Python offers several ways to format strings, and you’ll encounter all of them in real-world code. Let’s explore these approaches, focusing on the modern best practice.\n\nThe Different Approaches\nHere’s a practical example showing all the string formatting methods you might see:\n\n\nCode\n# Example data - like what you'd use in data science\nname = \"Alice\"\nage = 25\nscore = 87.543\n\nprint(\"=== Different String Formatting Approaches ===\")\nprint()\n\n# 1. String concatenation (you'll see this, but avoid it)\nprint(\"Method 1 - String Concatenation (avoid this):\")\nprint(\"Student: \" + name + \" is \" + str(age) + \" years old\")\nprint(\"Problems: messy, requires str() conversion, hard to read\")\nprint()\n\n# 2. % formatting (old style, still in legacy code)\nprint(\"Method 2 - % Formatting (legacy code):\")\nprint(\"Student: %s is %d years old with %.1f%% score\" % (name, age, score))\nprint(\"You'll see this in older code and some libraries\")\nprint()\n\n# 3. .format() method (better than %, still common)\nprint(\"Method 3 - .format() Method (common in existing code):\")\nprint(\"Student: {} is {} years old with {:.1f}% score\".format(name, age, score))\nprint(\"Student: {name} is {age} years old with {score:.1f}% score\".format(name=name, age=age, score=score))\nprint(\"Better than %, but still verbose\")\nprint()\n\n# 4. F-strings (MODERN APPROACH - use this!)\nprint(\"Method 4 - F-strings (MODERN - use this!):\")\nprint(f\"Student: {name} is {age} years old with {score:.1f}% score\")\nprint(f\"Temperature reading: {score:.2f}°C\")\nprint(f\"Data points processed: {age * 1000:,}\")\nprint(\"✅ Most readable, fastest, and Pythonic!\")\n\n\n=== Different String Formatting Approaches ===\n\nMethod 1 - String Concatenation (avoid this):\nStudent: Alice is 25 years old\nProblems: messy, requires str() conversion, hard to read\n\nMethod 2 - % Formatting (legacy code):\nStudent: Alice is 25 years old with 87.5% score\nYou'll see this in older code and some libraries\n\nMethod 3 - .format() Method (common in existing code):\nStudent: Alice is 25 years old with 87.5% score\nStudent: Alice is 25 years old with 87.5% score\nBetter than %, but still verbose\n\nMethod 4 - F-strings (MODERN - use this!):\nStudent: Alice is 25 years old with 87.5% score\nTemperature reading: 87.54°C\nData points processed: 25,000\n✅ Most readable, fastest, and Pythonic!\n\n\n\n\nWhy F-strings are the Modern Standard\nF-strings (formatted string literals) are the preferred approach in modern Python because they are:\n\nMore readable: Variables appear directly in the string\nFaster: Better performance than other methods\n\nMore concise: Less typing and fewer parentheses\nFeature-rich: Easy number formatting, expressions, and method calls\n\n\n\n\n\n\n\nR vs Python: String Formatting\n\n\n\nIf you’re coming from R, here are the equivalent approaches:\nR approaches:\n# R methods\nsprintf(\"Student: %s scored %.1f%%\", name, score)\npaste(\"Hello\", name, \"- you scored\", score)\npaste0(\"Temperature: \", temp, \"°C\")\nPython f-string equivalents:\n# Modern Python f-strings\nf\"Student: {name} scored {score:.1f}%\"\nf\"Hello {name} - you scored {score}\"\nf\"Temperature: {temp}°C\"\nF-strings are similar to R’s sprintf() but more intuitive, and much cleaner than paste()!\n\n\n\n\nF-string Formatting Examples for Data Science\nHere are common formatting patterns you’ll use in data analysis:\n\n\nCode\n# Common data science formatting patterns\ntemperature = 23.456789\ncount = 1234567\npercentage = 0.847\n\nprint(\"=== Data Science F-string Patterns ===\")\nprint()\n\n# Number formatting\nprint(f\"Temperature: {temperature:.2f}°C\")  # 2 decimal places\nprint(f\"Data points: {count:,}\")           # Thousands separator\nprint(f\"Success rate: {percentage:.1%}\")   # Percentage formatting\nprint()\n\n# Dynamic formatting\nprecision = 3\nprint(f\"Precise temperature: {temperature:.{precision}f}°C\")\nprint()\n\n# Expressions in f-strings\nvalues = [10, 20, 30, 40, 50]\nprint(f\"Dataset has {len(values)} values with mean {sum(values)/len(values):.1f}\")\nprint()\n\n# Multiple variables with formatting\nfor i, val in enumerate(values[:3], 1):\n    print(f\"Sample {i:2d}: {val:5.1f} (squared: {val**2:6.1f})\")\n\n\n=== Data Science F-string Patterns ===\n\nTemperature: 23.46°C\nData points: 1,234,567\nSuccess rate: 84.7%\n\nPrecise temperature: 23.457°C\n\nDataset has 5 values with mean 30.0\n\nSample  1:  10.0 (squared:  100.0)\nSample  2:  20.0 (squared:  400.0)\nSample  3:  30.0 (squared:  900.0)\n\n\n\n\nPractice Exercise\nTry updating this older-style code to use modern f-strings:\n\n\nCode\n# Convert these to f-strings\ncity = \"Santa Barbara\"\ntemp_f = 72\ntemp_c = (temp_f - 32) * 5/9\n\n# Old style (update these!):\nprint(\"Weather in \" + city + \":\")\nprint(\"Temperature: %d°F (%.1f°C)\" % (temp_f, temp_c))\n\n# Your f-string versions here:\n# print(f\"???\")\n\n\nWeather in Santa Barbara:\nTemperature: 72°F (22.2°C)"
  },
  {
    "objectID": "course-materials/interactive-sessions/1c_variables_strings.html#special-methods",
    "href": "course-materials/interactive-sessions/1c_variables_strings.html#special-methods",
    "title": "Interactive Session 1C",
    "section": "Special Methods",
    "text": "Special Methods\nSpecial methods, also known as “dunder” (double underscore) methods, allow you to define the behavior of your objects for built-in operations. For example, __init__ is used for initializing objects, and __str__ defines the string representation.\nWhile important, these are generally more advanced and used in object-oriented programming, so we will not focus on them in this introductory course."
  },
  {
    "objectID": "course-materials/interactive-sessions/1c_variables_strings.html#conclusion",
    "href": "course-materials/interactive-sessions/1c_variables_strings.html#conclusion",
    "title": "Interactive Session 1C",
    "section": "Conclusion",
    "text": "Conclusion\nThis guide has introduced you to determining variable types and exploring the methods available for different objects in Python. By understanding how to discover and use methods, you’ll be better equipped to manipulate data and build powerful programs.\nFeel free to experiment with the code examples interactively in your Jupyter notebook to deepen your understanding.\n\nEnd interactive session 1C"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_dataframes.html",
    "href": "course-materials/cheatsheets/pandas_dataframes.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "This cheatsheet provides a quick reference for common operations on Pandas DataFrames. It’s designed for beginning data science students who are just starting to work with Pandas."
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_dataframes.html#introduction",
    "href": "course-materials/cheatsheets/pandas_dataframes.html#introduction",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "This cheatsheet provides a quick reference for common operations on Pandas DataFrames. It’s designed for beginning data science students who are just starting to work with Pandas."
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_dataframes.html#importing-pandas",
    "href": "course-materials/cheatsheets/pandas_dataframes.html#importing-pandas",
    "title": "EDS 217 Cheatsheet",
    "section": "Importing Pandas",
    "text": "Importing Pandas\nAlways start by importing pandas:\n\n\nCode\nimport pandas as pd"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_dataframes.html#creating-a-dataframe",
    "href": "course-materials/cheatsheets/pandas_dataframes.html#creating-a-dataframe",
    "title": "EDS 217 Cheatsheet",
    "section": "Creating a DataFrame",
    "text": "Creating a DataFrame\n\nFrom a dictionary\n\n\nCode\ndata = {'Name': ['Alice', 'Bob', 'Charlie'],\n        'Age': [25, 30, 35],\n        'City': ['New York', 'San Francisco', 'Los Angeles']}\ndf = pd.DataFrame(data)\nprint(df)\n\n\n      Name  Age           City\n0    Alice   25       New York\n1      Bob   30  San Francisco\n2  Charlie   35    Los Angeles\n\n\n\n\nFrom a CSV file\n\n\nCode\n# Here's an example csv file we can use for read_csv:\nfrom io import StringIO\n# Create a CSV string\ncsv_data = \"\"\"\nName,Age,City\nAlice,25,New York\nBob,30,San Francisco\nCharlie,35,Los Angeles\n\"\"\"\n\n# Use StringIO to create a file-like object\ncsv_file = StringIO(csv_data.strip())\n\n# Read the CSV data into a DataFrame\ndf = pd.read_csv(csv_file)\nprint(df)\n\n\n      Name  Age           City\n0    Alice   25       New York\n1      Bob   30  San Francisco\n2  Charlie   35    Los Angeles"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_dataframes.html#basic-dataframe-information",
    "href": "course-materials/cheatsheets/pandas_dataframes.html#basic-dataframe-information",
    "title": "EDS 217 Cheatsheet",
    "section": "Basic DataFrame Information",
    "text": "Basic DataFrame Information\n\n\nCode\n# Display the first few rows\nprint(df.head())\n\n# Get basic information about the DataFrame\nprint(df.info())\n\n# Get summary statistics\nprint(df.describe())\n\n# Get column names\nprint(df.columns)\n\n# Get dimensions (rows, columns)\nprint(df.shape)\n\n\n      Name  Age           City\n0    Alice   25       New York\n1      Bob   30  San Francisco\n2  Charlie   35    Los Angeles\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   Name    3 non-null      object\n 1   Age     3 non-null      int64 \n 2   City    3 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 204.0+ bytes\nNone\n        Age\ncount   3.0\nmean   30.0\nstd     5.0\nmin    25.0\n25%    27.5\n50%    30.0\n75%    32.5\nmax    35.0\nIndex(['Name', 'Age', 'City'], dtype='object')\n(3, 3)"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_dataframes.html#selecting-data",
    "href": "course-materials/cheatsheets/pandas_dataframes.html#selecting-data",
    "title": "EDS 217 Cheatsheet",
    "section": "Selecting Data",
    "text": "Selecting Data\n\nSelecting columns\n\n\nCode\n# Select a single column\nage_column = df['Age']\n\n# Select multiple columns\nsubset = df[['Name', 'City']]\n\n\n\n\nSelecting rows\n\n\nCode\n# Select rows by index\nfirst_row = df.loc[0]\n\n# Select rows by condition\nadults = df[df['Age'] &gt;= 18]"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_dataframes.html#basic-data-manipulation",
    "href": "course-materials/cheatsheets/pandas_dataframes.html#basic-data-manipulation",
    "title": "EDS 217 Cheatsheet",
    "section": "Basic Data Manipulation",
    "text": "Basic Data Manipulation\n\nAdding a new column\n\n\nCode\ndf['Is Adult'] = df['Age'] &gt;= 18\n\n\n\n\nRenaming columns\n\n\nCode\ndf = df.rename(columns={'Name': 'Full Name'})\n\n\n\n\nHandling missing values\n\n\nCode\n# Drop rows with any missing values\ndf_cleaned = df.dropna()\n\n# Fill missing values\ndf_filled = df.fillna(0)  # Fills with 0"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_dataframes.html#basic-calculations",
    "href": "course-materials/cheatsheets/pandas_dataframes.html#basic-calculations",
    "title": "EDS 217 Cheatsheet",
    "section": "Basic Calculations",
    "text": "Basic Calculations\n\n\nCode\n# Calculate mean age\nmean_age = df['Age'].mean()\n\n# Count occurrences\ncity_counts = df['City'].value_counts()"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_dataframes.html#grouping-and-aggregation",
    "href": "course-materials/cheatsheets/pandas_dataframes.html#grouping-and-aggregation",
    "title": "EDS 217 Cheatsheet",
    "section": "Grouping and Aggregation",
    "text": "Grouping and Aggregation\n\n\nCode\n# Group by city and calculate mean age\ncity_age = df.groupby('City')['Age'].mean()"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_dataframes.html#sorting",
    "href": "course-materials/cheatsheets/pandas_dataframes.html#sorting",
    "title": "EDS 217 Cheatsheet",
    "section": "Sorting",
    "text": "Sorting\n\n\nCode\n# Sort by Age in descending order\ndf_sorted = df.sort_values('Age', ascending=False)"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_dataframes.html#saving-a-dataframe",
    "href": "course-materials/cheatsheets/pandas_dataframes.html#saving-a-dataframe",
    "title": "EDS 217 Cheatsheet",
    "section": "Saving a DataFrame",
    "text": "Saving a DataFrame\n\n\nCode\n# Save to CSV\ndf.to_csv('output.csv', index=False)"
  },
  {
    "objectID": "course-materials/cheatsheets/pandas_dataframes.html#further-learning",
    "href": "course-materials/cheatsheets/pandas_dataframes.html#further-learning",
    "title": "EDS 217 Cheatsheet",
    "section": "Further Learning",
    "text": "Further Learning\nFor more advanced operations and in-depth explanations, check out these resources:\n\nPandas Official Documentation\n10 Minutes to Pandas\nPython for Data Analysis by Wes McKinney\nPandas Cheat Sheet\n\nRemember, practice is key! Try these operations with different datasets to become more comfortable with Pandas DataFrames."
  },
  {
    "objectID": "course-materials/cheatsheets/data_selection.html",
    "href": "course-materials/cheatsheets/data_selection.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "While filtering and selection are related concepts in data manipulation, they have distinct differences:\nSelection:\n\nDefinition: Selection refers to choosing specific columns or rows from a DataFrame based on their labels or positions.\nPurpose: It’s used to extract a subset of data you’re interested in, without necessarily applying any conditions.\nMethods: In pandas, selection is typically done using methods like .loc[], .iloc[], or square brackets df[] for column selection.\nExample: Selecting specific columns like df[['name', 'age']] or rows df.loc[0:5].\n\nFiltering:\n\nDefinition: Filtering involves choosing rows that meet specific conditions based on the values in one or more columns.\nPurpose: It’s used to extract data that satisfies certain criteria or conditions.\nMethods: In pandas, filtering is often done using boolean indexing or the .query() method.\nExample: Filtering rows where age is greater than 30: df[df['age'] &gt; 30].\n\nKey differences:\n\nScope:\n\nSelection typically deals with choosing columns or rows based on their labels or positions.\nFiltering typically deals with choosing rows based on conditions applied to the data values.\n\nCondition-based:\n\nSelection doesn’t necessarily involve conditions (though it can with .loc)\nFiltering always involves a condition or criteria.\n\nOutput:\n\nSelection can result in both a subset of columns and/or rows.\nFiltering typically results in a subset of rows (though the number of columns can be affected if combined with selection).\n\nUse cases:\n\nSelection is often used when you know exactly which columns or rows you want.\nFiltering is used when you want to find data that meets certain criteria.\n\n\nIt’s worth noting that in practice, these operations are often combined. For example:\n# This combines filtering (age &gt; 30) and selection (only 'name' and 'profession' columns)\nresult = df.loc[df['age'] &gt; 30, ['name', 'profession']]\nUnderstanding the distinction between filtering and selection helps in choosing the right methods for data manipulation tasks and in communicating clearly about data operations."
  },
  {
    "objectID": "course-materials/cheatsheets/data_selection.html#selection-vs.-filtering",
    "href": "course-materials/cheatsheets/data_selection.html#selection-vs.-filtering",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "While filtering and selection are related concepts in data manipulation, they have distinct differences:\nSelection:\n\nDefinition: Selection refers to choosing specific columns or rows from a DataFrame based on their labels or positions.\nPurpose: It’s used to extract a subset of data you’re interested in, without necessarily applying any conditions.\nMethods: In pandas, selection is typically done using methods like .loc[], .iloc[], or square brackets df[] for column selection.\nExample: Selecting specific columns like df[['name', 'age']] or rows df.loc[0:5].\n\nFiltering:\n\nDefinition: Filtering involves choosing rows that meet specific conditions based on the values in one or more columns.\nPurpose: It’s used to extract data that satisfies certain criteria or conditions.\nMethods: In pandas, filtering is often done using boolean indexing or the .query() method.\nExample: Filtering rows where age is greater than 30: df[df['age'] &gt; 30].\n\nKey differences:\n\nScope:\n\nSelection typically deals with choosing columns or rows based on their labels or positions.\nFiltering typically deals with choosing rows based on conditions applied to the data values.\n\nCondition-based:\n\nSelection doesn’t necessarily involve conditions (though it can with .loc)\nFiltering always involves a condition or criteria.\n\nOutput:\n\nSelection can result in both a subset of columns and/or rows.\nFiltering typically results in a subset of rows (though the number of columns can be affected if combined with selection).\n\nUse cases:\n\nSelection is often used when you know exactly which columns or rows you want.\nFiltering is used when you want to find data that meets certain criteria.\n\n\nIt’s worth noting that in practice, these operations are often combined. For example:\n# This combines filtering (age &gt; 30) and selection (only 'name' and 'profession' columns)\nresult = df.loc[df['age'] &gt; 30, ['name', 'profession']]\nUnderstanding the distinction between filtering and selection helps in choosing the right methods for data manipulation tasks and in communicating clearly about data operations."
  },
  {
    "objectID": "course-materials/cheatsheets/data_selection.html#setup",
    "href": "course-materials/cheatsheets/data_selection.html#setup",
    "title": "EDS 217 Cheatsheet",
    "section": "Setup",
    "text": "Setup\nFirst, let’s import pandas and load our dataset.\n\n\nCode\nimport pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('https://bit.ly/eds217-studentdata')\n\n# Display the first few rows\nprint(df.head())\n\n\n   student_id  age   gpa             major\n0        1000   24  2.18       Mathematics\n1        1001   21  2.39           Physics\n2        1002   22  2.09           Physics\n3        1003   24  2.65  Computer Science\n4        1004   20  2.78         Chemistry"
  },
  {
    "objectID": "course-materials/cheatsheets/data_selection.html#basic-selection",
    "href": "course-materials/cheatsheets/data_selection.html#basic-selection",
    "title": "EDS 217 Cheatsheet",
    "section": "Basic Selection",
    "text": "Basic Selection\n\nSelect a Single Column\n\n\nCode\n# Using square brackets\nages = df['age']\n\n# Using dot notation (only works for valid Python identifiers)\nages = df.age\n\n\n\n\nSelect Multiple Columns\n\n\nCode\n# Select age and gpa columns\nage_gpa = df[['age', 'gpa']]\n\n\n\n\nSelect Rows by Index\n\n\nCode\n# Select first 5 rows\nfirst_five = df.iloc[0:5]\n\n# Select specific rows by index\nspecific_rows = df.iloc[[0, 2, 4]]\n\n\n\n\nSelect Rows and Columns\n\n\nCode\n# Select first 3 rows and 'age', 'gpa' columns\nsubset = df.loc[0:2, ['age', 'gpa']]"
  },
  {
    "objectID": "course-materials/cheatsheets/data_selection.html#filtering",
    "href": "course-materials/cheatsheets/data_selection.html#filtering",
    "title": "EDS 217 Cheatsheet",
    "section": "Filtering",
    "text": "Filtering\n\nFilter by a Single Condition\n\n\nCode\n# Students with age greater than 21\nolder_students = df[df['age'] &gt; 21]\n\n\n\n\nFilter by Multiple Conditions\n\n\nCode\n# Students with age &gt; 21 and gpa &gt; 3.5\nhigh_performing_older = df[(df['age'] &gt; 21) & (df['gpa'] &gt; 3.5)]\n\n\n\n\nFilter Using .isin()\n\n\nCode\n# Students majoring in Computer Science or Biology\ncs_bio_students = df[df['major'].isin(['Computer Science', 'Biology'])]\n\n\n\n\nFilter Using String Methods\n\n\nCode\n# Majors starting with 'E'\ne_majors = df[df['major'].str.startswith('E')]"
  },
  {
    "objectID": "course-materials/cheatsheets/data_selection.html#combining-selection-and-filtering",
    "href": "course-materials/cheatsheets/data_selection.html#combining-selection-and-filtering",
    "title": "EDS 217 Cheatsheet",
    "section": "Combining Selection and Filtering",
    "text": "Combining Selection and Filtering\n\n\nCode\n# Select 'age' and 'gpa' for students with gpa &gt; 3.5\nhigh_gpa_age = df.loc[df['gpa'] &gt; 3.5, ['age', 'gpa']]"
  },
  {
    "objectID": "course-materials/cheatsheets/data_selection.html#useful-methods",
    "href": "course-materials/cheatsheets/data_selection.html#useful-methods",
    "title": "EDS 217 Cheatsheet",
    "section": "Useful Methods",
    "text": "Useful Methods\n\n.loc[] vs .iloc[]\n\nUse .loc[] for label-based indexing\nUse .iloc[] for integer-based indexing\n\n\n\n.query() Method\n\n\nCode\n# Filter using query method\ncs_students = df.query(\"major == 'Computer Science'\")\n\n\n\n\n.where() Method\n\n\nCode\n# Replace values not meeting the condition with NaN\nhigh_gpa = df.where(df['gpa'] &gt; 3.5)"
  },
  {
    "objectID": "course-materials/cheatsheets/data_selection.html#tips-and-tricks",
    "href": "course-materials/cheatsheets/data_selection.html#tips-and-tricks",
    "title": "EDS 217 Cheatsheet",
    "section": "Tips and Tricks",
    "text": "Tips and Tricks\n\nChain methods for complex operations:\nresult = df[df['age'] &gt; 21].groupby('major')['gpa'].mean()\nUse & for AND, | for OR in multiple conditions:\ndf[(df['age'] &gt; 21) & (df['gpa'] &gt; 3.5) | (df['major'] == 'Computer Science')]\nReset index after filtering if needed:\nfiltered_df = df[df['age'] &gt; 21].reset_index(drop=True)\nUse ~ for negation:\nnot_cs = df[~(df['major'] == 'Computer Science')]\n\nRemember: Always chain indexers [] or use .loc[]/.iloc[] to avoid the SettingWithCopyWarning when modifying DataFrames. Alternatively, you can assign the output of a filtering or selection to the original dataframe if you want to alter the dataframe itself (and not make a copy or view)."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day7.html#introduction",
    "href": "course-materials/eod-practice/eod-day7.html#introduction",
    "title": "Day 7: Tasks & activities",
    "section": "Introduction",
    "text": "Introduction\nToday, we will look at the latest plant hardiness zone map distributed in 2023 by the US Department of Agriculture (USDA), and how it differs from the previous map, which came out in 2012.\nThe map is meant to help gardeners, and people interested in gardening and plants, know which plants will survive and thrive in different parts of the United States. The main data point used by the map is the mean low temperature recorded (in Fahrenheit) for a given location. Locations with similar mean low temperatures, within a 10-degree range, are categorized as being in the same zone. Each zone is then further divided into two sub-zones, with a 5-degree range for minimum temperatures.\nThe latest zone map showed that for many locations, the minimum temperature had risen somewhat. This doesn’t come as a massive surprise, given trends in climate change, but I thought that it would be interesting to explore the data and understand just where things had changed.\nThe data was collected by the PRISM research group at Oregon State University (https://prism.oregonstate.edu/projects/plant_hardiness_zones.php). A new data set was just released on November 15th by the US Department of Agriculture, at https://www.ars.usda.gov/news-events/news/research-news/2023/usda-unveils-updated-plant-hardiness-zone-map/.\nThe data is available in a variety of formats, but I decided that it would probably be easiest to work with it via zip codes, because they are spread out through the entire country. In order to figure out just where the zip codes are located, we will download an additional data set, a map of zip codes along with location information for each one, joining it together with our other data.\n\nDatasets\nYou will work with three CSV datasets:\nFirst, we’ll use the latest (2023) Plant Hardiness Zone report from https://prism.oregonstate.edu/projects/plant_hardiness_zones.php . The data comes in several formats and parts; we’ll use the CSV file that provides us with data per US zip code:\nhttps://prism.oregonstate.edu/projects/phm_data/phzm_us_zipcode_2023.csv\nNext, we’ll download data from the previous survey in 2012, described at https://prism.oregonstate.edu/projects/plant_hardiness_zones_2012.php :\nhttps://prism.oregonstate.edu/projects/public/phm/2012/phm_us_zipcode_2012.csv\nFinally, we’ll download and work with a CSV file containing US zip codes:\nhttp://uszipcodelist.com/zip_code_database.csv"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day7.html#tasks",
    "href": "course-materials/eod-practice/eod-day7.html#tasks",
    "title": "Day 7: Tasks & activities",
    "section": "Tasks",
    "text": "Tasks\n\n1. Setup\nFirst, import pandas, matplotlib, and seaborn and load the three datasets.\nNext, display the first few rows and print out the dataset info to get an idea of the contents of each dataset.\nYou may have noticed that the zipcodes were read in as integers rather than strings, and therefore might not be 5 digits long. Ensure the zipcode or zip column in all datasets is a 5-character string, filling in any zeros that were dropped.\nCombine the 2012 and 2023 data together by adding a year column and then stacking them together.\nIn the combined plant hardiness dataframe, create two new columns, trange_min and trange_max, containing the min and max temperatures of the trange column. Remove the original trange column.\nHint: use str.split() to split the trange strings where they have spaces and retrieve the first and last components (min and max, respectively)"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day7.html#tasks-1",
    "href": "course-materials/eod-practice/eod-day7.html#tasks-1",
    "title": "Day 7: Tasks & activities",
    "section": "Tasks",
    "text": "Tasks\n\n2. Exploration and visualization\nOn average, how much has the minimum temperature in a zip code changed from 2012 to 2023?\nMerge together the combined plant hardiness dataset and the zipcode dataset by zipcode. This will give us more informtaion in the plant hardiness dataset, such as the latitude and longitude for each zipcode.\nCreate two scatter plot where the x axis is the longitude, the y axis is the latitude, the color is based on the minimum temperature in 2012 for one and 2023 for the other. Only look at longitude &lt; -60.\nNow create a single scatter plot where you look at the difference between the minimum temperature in 2012 and 2023. Only look at longitude &lt; -60. Color any zipcodes where you do not have information from both years in grey.\nCreate a bar plot showing the top 10 states where the average minimum temperature increased the most. Label your axes appropriately.\n\nEnd Activity Session (Day 7)"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day1-draft.html#welcome-to-your-data-science-future",
    "href": "course-materials/eod-practice/eod-day1-draft.html#welcome-to-your-data-science-future",
    "title": "Day 1: Tasks & activities (DRAFT)",
    "section": "🎯 Welcome to Your Data Science Future!",
    "text": "🎯 Welcome to Your Data Science Future!\nToday’s Mission: Get a sneak peek at where you’re headed! You’ll copy and run Python code to see what’s possible with data science. Think of this as a movie trailer for the skills you’ll build over the next week.\n\n🎬 “Coming Attractions” Approach\n\nYour job: Copy, paste, and run the code exactly as written\nOur job: Show you what’s happening (not how it works yet!)\nThe goal: Get excited about what you’ll learn and see the big picture\n\n\n\n\n\n\n\nDon’t Panic! 🚀\n\n\n\nYou’re not expected to understand every line of code today. By next Friday, you’ll know exactly how all of this works. For now, just enjoy the ride and see what’s possible!"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day1-draft.html#when-youll-master-these-skills",
    "href": "course-materials/eod-practice/eod-day1-draft.html#when-youll-master-these-skills",
    "title": "Day 1: Tasks & activities (DRAFT)",
    "section": "🗓️ When You’ll Master These Skills",
    "text": "🗓️ When You’ll Master These Skills\n\n\n\nWhat you’ll see today\nWhen you’ll learn it\nWhat we’ll cover\n\n\n\n\nimport pandas as pd\nDay 3-4\nData structures and DataFrames\n\n\npd.read_csv()\nDay 4\nLoading data from files\n\n\ndf.head(), df.info()\nDay 4\nData exploration methods\n\n\ndf.groupby()\nDay 6\nData aggregation and grouping\n\n\nplt.plot(), plt.bar()\nDay 7\nData visualization"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day1-draft.html#background-and-data-source",
    "href": "course-materials/eod-practice/eod-day1-draft.html#background-and-data-source",
    "title": "Day 1: Tasks & activities (DRAFT)",
    "section": "Background and Data Source",
    "text": "Background and Data Source\nOur data comes from the Arctic Long Term Ecological Research station. The Arctic Long Term Ecological Research (ARC LTER) site is part of a network of sites established by the National Science Foundation to support long-term ecological research in the United States. The research site is located in the foothills region of the Brooks Range, North Slope of Alaska (68° 38’N, 149° 36.4’W, elevation 720 m).\nThe Arctic LTER project’s goal is to understand and predict the effects of environmental change on arctic landscapes, both natural and anthropogenic. Researchers at the site use long-term monitoring and surveys of natural variation of ecosystem characteristics, experimental manipulation of ecosystems (years to decades) and modeling at ecosystem and watershed scales to gain an understanding of the controls of ecosystem structure and function. The data and insights gained are provided to federal, Alaska state and North Slope Borough officials who regulate the lands on the North Slope and through this web site.\nWe will be using some basic weather data downloaded from Toolik Station:\n\nToolik Station Meteorological Data: toolik_weather.csv Shaver, G. 2019. A multi-year DAILY weather file for the Toolik Field Station at Toolik Lake, AK starting 1988 to present. ver 4. Environmental Data Initiative. https://doi.org/10.6073/pasta/ce0f300cdf87ec002909012abefd9c5c (Accessed 2021-08-08).\n\nI have already downloaded this data and placed in our course repository, where we can access it easily using its GitHub raw URL.\nLet’s dive into your preview of data science with Python!"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day1-draft.html#instructions",
    "href": "course-materials/eod-practice/eod-day1-draft.html#instructions",
    "title": "Day 1: Tasks & activities (DRAFT)",
    "section": "Instructions",
    "text": "Instructions\n\n🚀 Step 1: Import the Magic Tools\nFirst, we’ll import the libraries that make data science possible in Python.\n🎬 Copy and paste this code:\nimport pandas as pd\nimport matplotlib.pyplot as plt\n🎭 What just happened? We imported two powerful libraries! pandas is like Excel but supercharged for data analysis, and matplotlib creates beautiful(ish) plots. 🎓 Coming up: You’ll learn about Python imports and libraries on Days 2-3.\n\n\n\n🚀 Step 2: Load Real Climate Data\nNow we’ll load 15,000+ rows of Arctic weather data in just one line!\nOur data is located at: https://raw.githubusercontent.com/environmental-data-science/eds217-day0-comp/main/data/raw_data/toolik_weather.csv\n🎬 Copy and paste this code:\nurl = 'https://raw.githubusercontent.com/environmental-data-science/eds217-day0-comp/main/data/raw_data/toolik_weather.csv'\ndf = pd.read_csv(url)\n🎭 What just happened? We loaded over 15,000 rows of climate data from the internet in one line! The data is now stored in a “DataFrame” called df. 🎓 Coming up: Day 4 will teach you all about loading and working with data files.\n\n\n\n\n\n\nR vs Python: Data Loading\n\n\n\nThis is just like df &lt;- read.csv(url) in R! Both pandas DataFrames and R data.frames are tabular data structures. The main syntax difference is Python’s dot notation: pd.read_csv() vs R’s read.csv(). Both can read directly from URLs, which is incredibly convenient for reproducible research!\n\n\n\n\n\n🚀 Step 3: Peek at the Data\nLet’s see what our Arctic climate data looks like!\n🎬 Copy and paste this code:\ndf.head()\n🎭 What just happened? We previewed the first 5 rows of our 15,000+ row dataset! You can see daily weather measurements from Alaska. 🎓 Coming up: Day 4 morning will teach you data exploration methods like this.\n\n\n\n\n\n\nR vs Python: Data Exploration\n\n\n\nThis is exactly like head(df) in R! The key difference is Python’s object-oriented approach: df.head() vs R’s functional approach head(df). Both show you the first few rows, but Python treats the DataFrame as an object that has methods (like .head()) built into it.\n\n\n\n\n\n🚀 Step 4: Check Data Quality\nReal data science involves checking for missing or problematic data.\n🎬 Copy and paste this code:\ndf.isnull().sum()\n🎭 What just happened? We checked every column for missing data! Looks like our temperature data is complete (0 missing values), which is great. 🎓 Coming up: Day 5 will teach you all about data cleaning and handling missing values.\n\n\n\n\n\n\nR vs Python: Missing Data Check\n\n\n\nIn R, you’d use sum(is.na(df)) to count missing values. Python uses df.isnull().sum() - notice the chaining of methods! This reads left-to-right: “take the DataFrame, check for null values, then sum them up.” Both approaches give you the count of missing values per column.\n\n\n\n\n\n🚀 Step 5: Get Data Summary Statistics\nLet’s get a statistical overview of our climate data.\n🎬 Copy and paste this code:\ndf.describe()\ndf.info()\n🎭 What just happened? We got instant statistics and information about our entire dataset! You can see temperature ranges, averages, and data types. 🎓 Coming up: Day 4 will teach you how to explore and understand your datasets.\n\n\n\n\n\n\nR vs Python: Data Summary\n\n\n\nThese are like summary(df) and str(df) in R. Python’s .describe() gives you the statistical summary (like summary()) while .info() shows the structure (like str()). Notice how Python uses dot notation - the DataFrame object has these methods built in, whereas R uses separate functions that take the data frame as input.\n\n\n\n\n\n🚀 Step 6: Calculate Monthly Averages\nNow for some real data analysis - let’s find average temperatures by month!\n🎬 Copy and paste this code:\nmonthly = df.groupby('Month')\nmonthly_means = monthly['Daily_AirTemp_Mean_C'].mean()\n🎭 What just happened? We grouped 15,000+ daily temperature readings by month and calculated averages! This turned years of daily data into 12 monthly summaries. 🎓 Coming up: Day 6 will teach you all about grouping and aggregating data like this.\n\n\n\n\n\n\nR vs Python: Data Grouping\n\n\n\nThis is exactly like using df %&gt;% group_by(Month) %&gt;% summarize(mean_temp = mean(Daily_AirTemp_Mean_C)) in dplyr! Both approaches group data and calculate statistics. Python’s syntax is df.groupby('column')['target_column'].function(), while R uses the pipe operator %&gt;% to chain operations. Both are powerful for data aggregation!\n\n\n\n\n\n🚀 Step 7: Create Your First Visualization\nTime to turn numbers into pictures! Let’s plot the monthly temperature patterns.\n🎬 Copy and paste this code:\nplt.plot(monthly_means)\nNow let’s make it even better with labels:\nmonths = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\nplt.bar(months, monthly_means)\n🎭 What just happened? You created professional data visualizations! The bar chart clearly shows Alaska’s extreme seasonal temperature differences. 🎓 Coming up: Day 7 will teach you how to create amazing visualizations and customize them.\n\n\n\n\n\n\nR vs Python: Data Visualization\n\n\n\nThis is like creating plots with ggplot(df, aes(x=Month, y=temp)) + geom_bar() in R! Python’s matplotlib uses a more direct approach: plt.plot() and plt.bar() create plots immediately. Both are powerful - ggplot2 uses a “grammar of graphics” approach while matplotlib is more imperative. You’ll learn both have their strengths!\n\n\n\n\n\n🚀 Step 8: Analyze Climate Trends Over Time\nLet’s explore how temperatures have changed over the decades!\n🎬 Copy and paste this code:\nyear = df.groupby('Year')\nyearly_means = year['Daily_AirTemp_Mean_C'].mean()\nplt.plot(yearly_means)\nAnd as a bar chart:\nyear_list = df['Year'].unique()\nplt.bar(year_list, yearly_means)\n🎭 What just happened? You analyzed climate trends across multiple decades! You can see how Arctic temperatures have varied over time - real climate science! 🎓 Coming up: This combines Day 6 skills (grouping data) with Day 7 skills (visualization).\n\n\n\n\n\n\nR vs Python: Time Series Analysis\n\n\n\nThis is just like grouping by year in R and plotting the results! Whether you use df %&gt;% group_by(Year) %&gt;% summarize() in R or df.groupby('Year').mean() in Python, you’re doing the same analytical thinking. The syntax differs, but the data science concepts are identical.\n\n\n\n\n\n🚀 Step 9: Save Your Work\nData scientists always save their analyses for future use.\n🎬 Copy and paste this code:\nmonthly_means.to_csv(\"monthly_means.csv\", header=True)\n🎭 What just happened? You saved your analysis results to a file that you (or other scientists) can use later! This is how research becomes reproducible. 🎓 Coming up: Day 4 will teach you all about importing, exporting, and managing data files.\n\n\n\n\n\n\nR vs Python: Data Export\n\n\n\nThis is just like write.csv(monthly_means, \"monthly_means.csv\") in R! Python uses the object-oriented approach where the data (your Series monthly_means) has a method .to_csv() built into it. R uses a function that takes the data as input. Both create the exact same CSV file - just different syntax approaches to the same goal."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day1-draft.html#congratulations-youre-a-data-scientist",
    "href": "course-materials/eod-practice/eod-day1-draft.html#congratulations-youre-a-data-scientist",
    "title": "Day 1: Tasks & activities (DRAFT)",
    "section": "🎉 Congratulations! You’re a Data Scientist!",
    "text": "🎉 Congratulations! You’re a Data Scientist!\nYou just completed a real data science workflow: - ✅ Loaded real climate data from the internet - ✅ Explored and understood your dataset\n- ✅ Calculated meaningful statistics - ✅ Created professional visualizations - ✅ Analyzed climate trends over time - ✅ Saved your results for future use\n\n🧠 What You’ve Seen Today\nThis preview showed you the power and simplicity of Python for data science. In just a few lines of code, you: - Processed 15,000+ rows of climate data - Created multiple visualizations - Performed real climate analysis - Saved reproducible results\n\n\n🚀 What’s Next\nOver the next week, you’ll learn exactly how each line works: - Days 2-3: Python fundamentals (variables, data types, control flows) - the building blocks - Day 4: DataFrames and data loading (like today’s pd.read_csv()) - working with structured data - Day 5: Data cleaning and manipulation - making messy data analysis-ready - Day 6: Grouping and aggregation (like today’s groupby()) - summarizing and analyzing patterns - Day 7: Data visualization (like today’s plots) - creating publication-quality graphics\nBy Friday, you’ll understand every single line of code you ran today - and you’ll be able to write it all yourself!\n\n\n🔄 R Skills Transfer\nYour R experience gives you a huge advantage! You already understand: - Data structures (data.frames ↔︎ DataFrames) - Statistical thinking (same concepts, different syntax) - Data workflows (load → explore → analyze → visualize) - Reproducible research (scripts and documentation)\nPython will feel familiar because the thinking is the same - you’re just learning new syntax for concepts you already know!"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day1-draft.html#reflection-questions",
    "href": "course-materials/eod-practice/eod-day1-draft.html#reflection-questions",
    "title": "Day 1: Tasks & activities (DRAFT)",
    "section": "🤔 Reflection Questions",
    "text": "🤔 Reflection Questions\nTake a moment to think about: 1. What surprised you most about what Python can do with data? 2. Which part of the analysis was most interesting to you? 3. What questions do you have about how this all works? 4. How might you use these skills in your future research or work?\nAdd your thoughts in a new markdown cell below!\n\n\n\n\n\n\n\nRemember\n\n\n\nToday was about inspiration and seeing possibilities. Don’t worry if you don’t understand everything yet - that’s exactly what the rest of the course is for! You’re going to do amazing things with data science. 🚀\n\n\n\nEnd Activity Session (Day 1) - Welcome to Your Data Science Journey!"
  },
  {
    "objectID": "course-materials/coding-colabs/3b_control_flows.html",
    "href": "course-materials/coding-colabs/3b_control_flows.html",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "",
    "text": "Here’s a quick reference for the basic control flow structures we’ll be using:\n# If statement\nif condition:\n    # code to run if condition is True\n\n# For loop\nfor item in sequence:\n    # code to run for each item\n\n# While loop\nwhile condition:\n    # code to run while condition is True\nHere’s our course cheatsheet on control flows:\n\nControl Flows Cheatsheet\n\nFeel free to refer to this cheatsheet throughout the exercise if you need a quick reminder about syntax or functionality."
  },
  {
    "objectID": "course-materials/coding-colabs/3b_control_flows.html#quick-reference",
    "href": "course-materials/coding-colabs/3b_control_flows.html#quick-reference",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "",
    "text": "Here’s a quick reference for the basic control flow structures we’ll be using:\n# If statement\nif condition:\n    # code to run if condition is True\n\n# For loop\nfor item in sequence:\n    # code to run for each item\n\n# While loop\nwhile condition:\n    # code to run while condition is True\nHere’s our course cheatsheet on control flows:\n\nControl Flows Cheatsheet\n\nFeel free to refer to this cheatsheet throughout the exercise if you need a quick reminder about syntax or functionality."
  },
  {
    "objectID": "course-materials/coding-colabs/3b_control_flows.html#exercise-overview",
    "href": "course-materials/coding-colabs/3b_control_flows.html#exercise-overview",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Exercise Overview",
    "text": "Exercise Overview\nIn this Coding Colab, you’ll practice using basic control flow structures in Python. Work through these simple tasks with your partner, discussing your approach as you go."
  },
  {
    "objectID": "course-materials/coding-colabs/3b_control_flows.html#part-1-if-statements-20-minutes",
    "href": "course-materials/coding-colabs/3b_control_flows.html#part-1-if-statements-20-minutes",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Part 1: If Statements (20 minutes)",
    "text": "Part 1: If Statements (20 minutes)\n\nTask 1: Simple Weather Advice\nWrite a program that gives weather advice based on temperature:\n\nSet a variable temperature = 20\nUse an if-else statement to print advice:\n\nIf temperature is above 25, print “It’s a hot day, stay hydrated!”\nOtherwise, print “Enjoy the pleasant weather!”\n\n\n\ntemperature = 20\n\n# Your code here\n# Use an if-else statement to print weather advice\n\n\n\nTask 2: Grade Classifier\nCreate a program that assigns a letter grade based on a numerical score:\n\nSet a variable score = 85\nUse if-elif-else statements to print the letter grade:\n\n90 or above: “A”\n80-89: “B”\n70-79: “C”\n60-69: “D”\nBelow 60: “F”\n\n\n\nscore = 85\n\n# Your code here\n# Use if-elif-else statements to print the letter grade"
  },
  {
    "objectID": "course-materials/coding-colabs/3b_control_flows.html#part-2-for-loops-20-minutes",
    "href": "course-materials/coding-colabs/3b_control_flows.html#part-2-for-loops-20-minutes",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Part 2: For Loops (20 minutes)",
    "text": "Part 2: For Loops (20 minutes)\n\nTask 3: Counting Sheep\nWrite a program that counts from 1 to 5, printing “sheep” after each number:\n\nUse a for loop with the range() function\nPrint each number followed by “sheep”\n\n\n# Your code here\n# Use a for loop to count sheep\n\n\n\nTask 4: Sum of Numbers\nCalculate the sum of numbers from 1 to 10:\n\nCreate a variable total = 0\nUse a for loop with the range() function to add each number to total\nAfter the loop, print the total\n\n\ntotal = 0\n\n# Your code here\n# Use a for loop to calculate the sum"
  },
  {
    "objectID": "course-materials/coding-colabs/3b_control_flows.html#part-3-while-loops-15-minutes",
    "href": "course-materials/coding-colabs/3b_control_flows.html#part-3-while-loops-15-minutes",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Part 3: While Loops (15 minutes)",
    "text": "Part 3: While Loops (15 minutes)\n\nTask 5: Countdown\nCreate a simple countdown program:\n\nSet a variable countdown = 5\nUse a while loop to print the countdown from 5 to 1\nAfter each print, decrease the countdown by 1\nWhen the countdown reaches 0, print “Blast off!”\n\n\ncountdown = 5\n\n# Your code here\n# Use a while loop for the countdown"
  },
  {
    "objectID": "course-materials/coding-colabs/3b_control_flows.html#conclusion-and-discussion-5-minutes",
    "href": "course-materials/coding-colabs/3b_control_flows.html#conclusion-and-discussion-5-minutes",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Conclusion and Discussion (5 minutes)",
    "text": "Conclusion and Discussion (5 minutes)\nWith your partner, briefly discuss:\n\nWhich control structure (if, for, or while) did you find easiest to use?\nCan you think of a real-life situation where you might use each of these control structures?\n\nRemember, it’s okay if you don’t finish all tasks. The goal is to practice and understand these concepts. Good luck and enjoy coding together!"
  },
  {
    "objectID": "course-materials/interactive-sessions/3c_arrays_and_series.html#getting-started",
    "href": "course-materials/interactive-sessions/3c_arrays_and_series.html#getting-started",
    "title": "Interactive Session 3C",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "course-materials/interactive-sessions/3c_arrays_and_series.html#introduction",
    "href": "course-materials/interactive-sessions/3c_arrays_and_series.html#introduction",
    "title": "Interactive Session 3C",
    "section": "Introduction",
    "text": "Introduction\nIn this interactive session, we’ll explore the most essential aspects of NumPy arrays and Pandas Series. These fundamental data structures are crucial for data manipulation and analysis in Python. We’ll focus on the key concepts that are most relevant for beginning data scientists. We’re also going to assume that you will primarily work with Pandas DataFrames and Series, so we won’t spend too much time on the details of NumPy arrays.\nLet’s start by importing the necessary libraries:\n\n\nCode\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "course-materials/interactive-sessions/3c_arrays_and_series.html#numpy-arrays-the-foundation",
    "href": "course-materials/interactive-sessions/3c_arrays_and_series.html#numpy-arrays-the-foundation",
    "title": "Interactive Session 3C",
    "section": "NumPy Arrays: The Foundation",
    "text": "NumPy Arrays: The Foundation\nNumPy arrays are the building blocks for many data structures in Python, including Pandas Series and DataFrames. Let’s explore their basic properties and operations.\n\nCreating NumPy Arrays\n\n\nCode\n# Create a 1D array\narr_1d = np.array([1, 2, 3, 4, 5])\nprint(\"1D array:\", arr_1d)\n\n# Create a 2D array\narr_2d = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"2D array:\\n\", arr_2d)\n\n\n1D array: [1 2 3 4 5]\n2D array:\n [[1 2 3]\n [4 5 6]]\n\n\n\n\nBasic Array Operations\n\n\nCode\n# Array arithmetic - using modern f-string formatting\nprint(f\"Original array: {arr_1d}\")\nprint(f\"Array + 2: {arr_1d + 2}\")\nprint(f\"Array * 2: {arr_1d * 2}\")\n\n# Array statistics with clear, formatted output\nprint(f\"Mean: {np.mean(arr_1d):.2f}\")\nprint(f\"Sum: {np.sum(arr_1d)}\")\n\n\nOriginal array: [1 2 3 4 5]\nArray + 2: [3 4 5 6 7]\nArray * 2: [ 2  4  6  8 10]\nMean: 3.00\nSum: 15\n\n\nNow it’s your turn! Create a NumPy array of your favorite numbers and calculate its standard deviation (np.std())."
  },
  {
    "objectID": "course-materials/interactive-sessions/3c_arrays_and_series.html#pandas-series-building-on-numpy",
    "href": "course-materials/interactive-sessions/3c_arrays_and_series.html#pandas-series-building-on-numpy",
    "title": "Interactive Session 3C",
    "section": "Pandas Series: Building on NumPy",
    "text": "Pandas Series: Building on NumPy\nPandas Series are one-dimensional labeled arrays built on top of NumPy arrays. They’re like a column in a spreadsheet or a single column of a DataFrame.\n\nCreating Pandas Series\n\n\nCode\n# Create a Series from a list\ns1 = pd.Series([1, 2, 3, 4, 5])\nprint(\"Series from list:\\n\", s1)\n\n# Create a Series with custom index\ns2 = pd.Series([10, 20, 30, 40, 50], index=['a', 'b', 'c', 'd', 'e'])\nprint(\"\\nSeries with custom index:\\n\", s2)\n\n\nSeries from list:\n 0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\nSeries with custom index:\n a    10\nb    20\nc    30\nd    40\ne    50\ndtype: int64\n\n\n\n\nBasic Series Operations\n\n\nCode\n# Accessing elements\nprint(\"Element at index 'c':\", s2['c'])\nprint(\"First three elements:\")\nprint(s2[:3])\n\n# Series arithmetic - using modern f-string formatting\nprint(f\"\\nOriginal Series:\\n{s2}\")\nprint(f\"\\nSeries + 5:\\n{s2 + 5}\")\n\n# Series statistics with clear, formatted output\nprint(f\"\\nMean: {s2.mean():.2f}\")\nprint(f\"Median: {s2.median():.2f}\")\n\n\nElement at index 'c': 30\nFirst three elements:\na    10\nb    20\nc    30\ndtype: int64\n\nOriginal Series:\na    10\nb    20\nc    30\nd    40\ne    50\ndtype: int64\n\nSeries + 5:\na    15\nb    25\nc    35\nd    45\ne    55\ndtype: int64\n\nMean: 30.00\nMedian: 30.00\n\n\nYour turn! Create a Pandas Series representing daily temperatures for a week. Use the days of the week as the index. Then, calculate and print the maximum temperature."
  },
  {
    "objectID": "course-materials/interactive-sessions/3c_arrays_and_series.html#comparing-numpy-arrays-and-pandas-series",
    "href": "course-materials/interactive-sessions/3c_arrays_and_series.html#comparing-numpy-arrays-and-pandas-series",
    "title": "Interactive Session 3C",
    "section": "Comparing NumPy Arrays and Pandas Series",
    "text": "Comparing NumPy Arrays and Pandas Series\nLet’s explore some key differences and similarities between NumPy arrays and Pandas Series.\n\n\nCode\n# Create a NumPy array and a Pandas Series with the same data\nnp_arr = np.array([1, 2, 3, 4, 5])\npd_series = pd.Series([1, 2, 3, 4, 5])\n\nprint(\"NumPy array:\", np_arr)\nprint(\"Pandas Series:\\n\", pd_series)\n\n# Demonstrate label-based indexing in Pandas Series\npd_series.index = ['a', 'b', 'c', 'd', 'e']\nprint(\"\\nPandas Series with custom index:\\n\", pd_series)\nprint(\"Value at index 'c':\", pd_series['c'])\n\n# Show that NumPy operations work on Pandas Series\nprint(\"\\nSquare root of Pandas Series:\\n\", np.sqrt(pd_series))\n\n\nNumPy array: [1 2 3 4 5]\nPandas Series:\n 0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\nPandas Series with custom index:\n a    1\nb    2\nc    3\nd    4\ne    5\ndtype: int64\nValue at index 'c': 3\n\nSquare root of Pandas Series:\n a    1.000000\nb    1.414214\nc    1.732051\nd    2.000000\ne    2.236068\ndtype: float64\n\n\nNow it’s your turn! Create a NumPy array and a Pandas Series, both containing the same data (use any numbers you like). Then, calculate the mean of both and compare the results. Are they the same? Why or why not?"
  },
  {
    "objectID": "course-materials/interactive-sessions/3c_arrays_and_series.html#conclusion",
    "href": "course-materials/interactive-sessions/3c_arrays_and_series.html#conclusion",
    "title": "Interactive Session 3C",
    "section": "Conclusion",
    "text": "Conclusion\nIn this session, we’ve covered the essential aspects of NumPy arrays and Pandas Series. Remember:\n\nNumPy arrays are the foundation for numerical computing in Python.\nPandas Series build on NumPy arrays, adding labeled axes and additional functionality.\nBoth support fast operations on entire datasets without explicit loops.\nPandas Series are particularly useful for labeled data and seamlessly integrate with DataFrames.\n\nAs you continue your journey in data science, you’ll find these structures invaluable for efficient data manipulation and analysis.\n\nEnd interactive session 3C"
  },
  {
    "objectID": "course-materials/cheatsheets/control_flows.html",
    "href": "course-materials/cheatsheets/control_flows.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Code\nx = 10\nif x &gt; 0:\n    print(\"Positive\")\nelif x &lt; 0:\n    print(\"Negative\")\nelse:\n    print(\"Zero\")\n\n\nPositive"
  },
  {
    "objectID": "course-materials/cheatsheets/control_flows.html#conditional-statements",
    "href": "course-materials/cheatsheets/control_flows.html#conditional-statements",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Code\nx = 10\nif x &gt; 0:\n    print(\"Positive\")\nelif x &lt; 0:\n    print(\"Negative\")\nelse:\n    print(\"Zero\")\n\n\nPositive"
  },
  {
    "objectID": "course-materials/cheatsheets/control_flows.html#loops",
    "href": "course-materials/cheatsheets/control_flows.html#loops",
    "title": "EDS 217 Cheatsheet",
    "section": "Loops",
    "text": "Loops\n\nfor loop\n\n\nCode\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor fruit in fruits:\n    print(fruit)\n\n\napple\nbanana\ncherry\n\n\n\n\nwhile loop\n\n\nCode\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    count += 1\n\n\n0\n1\n2\n3\n4"
  },
  {
    "objectID": "course-materials/cheatsheets/control_flows.html#loop-control",
    "href": "course-materials/cheatsheets/control_flows.html#loop-control",
    "title": "EDS 217 Cheatsheet",
    "section": "Loop Control",
    "text": "Loop Control\n\nbreak\n\n\nCode\nfor i in range(10):\n    if i == 5:\n        break\n    print(i)\n\n\n0\n1\n2\n3\n4\n\n\n\n\ncontinue\n\n\nCode\nfor i in range(5):\n    if i == 2:\n        continue\n    print(i)\n\n\n0\n1\n3\n4"
  },
  {
    "objectID": "course-materials/cheatsheets/control_flows.html#comprehensions",
    "href": "course-materials/cheatsheets/control_flows.html#comprehensions",
    "title": "EDS 217 Cheatsheet",
    "section": "Comprehensions",
    "text": "Comprehensions\n\nList Comprehension\n\n\nCode\nsquares = [x**2 for x in range(5)]\nprint(squares)\n\n\n[0, 1, 4, 9, 16]\n\n\n\n\nDictionary Comprehension\n\n\nCode\nsquares_dict = {x: x**2 for x in range(5)}\nprint(squares_dict)\n\n\n{0: 0, 1: 1, 2: 4, 3: 9, 4: 16}"
  },
  {
    "objectID": "course-materials/cheatsheets/control_flows.html#exception-handling",
    "href": "course-materials/cheatsheets/control_flows.html#exception-handling",
    "title": "EDS 217 Cheatsheet",
    "section": "Exception Handling",
    "text": "Exception Handling\n\ntry-except\n\n\nCode\ntry:\n    result = 10 / 0\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero\")\n\n\nCannot divide by zero\n\n\n\n\ntry-except-else-finally\n\n\nCode\ntry:\n    x = 5\n    result = 10 / x\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero\")\nelse:\n    print(f\"Result: {result}\")\nfinally:\n    print(\"Execution completed\")\n\n\nResult: 2.0\nExecution completed\n\n\nFor more detailed information on Python control flows, refer to the Python documentation on Control Flow Tools."
  },
  {
    "objectID": "course-materials/cheatsheets/sets.html",
    "href": "course-materials/cheatsheets/sets.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Code\n# Empty set\nempty_set = set()\nprint(f\"Empty set: {empty_set}\")\n\n# Set from a list\nset_from_list = set([1, 2, 3, 4, 5])\nprint(f\"Set from list: {set_from_list}\")\n\n# Set literal\nset_literal = {1, 2, 3, 4, 5}\nprint(f\"Set literal: {set_literal}\")\n\n\nEmpty set: set()\nSet from list: {1, 2, 3, 4, 5}\nSet literal: {1, 2, 3, 4, 5}"
  },
  {
    "objectID": "course-materials/cheatsheets/sets.html#creating-sets",
    "href": "course-materials/cheatsheets/sets.html#creating-sets",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Code\n# Empty set\nempty_set = set()\nprint(f\"Empty set: {empty_set}\")\n\n# Set from a list\nset_from_list = set([1, 2, 3, 4, 5])\nprint(f\"Set from list: {set_from_list}\")\n\n# Set literal\nset_literal = {1, 2, 3, 4, 5}\nprint(f\"Set literal: {set_literal}\")\n\n\nEmpty set: set()\nSet from list: {1, 2, 3, 4, 5}\nSet literal: {1, 2, 3, 4, 5}"
  },
  {
    "objectID": "course-materials/cheatsheets/sets.html#basic-operations",
    "href": "course-materials/cheatsheets/sets.html#basic-operations",
    "title": "EDS 217 Cheatsheet",
    "section": "Basic Operations",
    "text": "Basic Operations\n\n\nCode\ns = {1, 2, 3, 4, 5}\nprint(f\"Initial set: {s}\")\n\n# Add an element\ns.add(6)\nprint(f\"After adding 6: {s}\")\n\n# Remove an element\ns.remove(3)  # Raises KeyError if not found\nprint(f\"After removing 3: {s}\")\n\ns.discard(10)  # Doesn't raise error if not found\nprint(f\"After discarding 10 (not in set): {s}\")\n\n# Pop a random element\npopped = s.pop()\nprint(f\"Popped element: {popped}\")\nprint(f\"Set after pop: {s}\")\n\n# Check membership\nprint(f\"Is 2 in the set? {2 in s}\")\n\n# Clear the set\ns.clear()\nprint(f\"Set after clear: {s}\")\n\n\nInitial set: {1, 2, 3, 4, 5}\nAfter adding 6: {1, 2, 3, 4, 5, 6}\nAfter removing 3: {1, 2, 4, 5, 6}\nAfter discarding 10 (not in set): {1, 2, 4, 5, 6}\nPopped element: 1\nSet after pop: {2, 4, 5, 6}\nIs 2 in the set? True\nSet after clear: set()"
  },
  {
    "objectID": "course-materials/cheatsheets/sets.html#set-methods",
    "href": "course-materials/cheatsheets/sets.html#set-methods",
    "title": "EDS 217 Cheatsheet",
    "section": "Set Methods",
    "text": "Set Methods\n\n\nCode\na = {1, 2, 3}\nb = {3, 4, 5}\nprint(f\"Set a: {a}\")\nprint(f\"Set b: {b}\")\n\n\nSet a: {1, 2, 3}\nSet b: {3, 4, 5}\n\n\n\nUnion\n\n\nCode\nunion_set = a.union(b)\nprint(f\"Union of a and b: {union_set}\")\n\n\nUnion of a and b: {1, 2, 3, 4, 5}\n\n\n\n\nIntersection\n\n\nCode\nintersection_set = a.intersection(b)\nprint(f\"Intersection of a and b: {intersection_set}\")\n\n\nIntersection of a and b: {3}\n\n\n\n\nDifference\n\n\nCode\ndifference_set = a.difference(b)\nprint(f\"Difference of a and b: {difference_set}\")\n\n\nDifference of a and b: {1, 2}\n\n\n\n\nSymmetric difference\n\n\nCode\nsymmetric_difference_set = a.symmetric_difference(b)\nprint(f\"Symmetric difference of a and b: {symmetric_difference_set}\")\n\n\nSymmetric difference of a and b: {1, 2, 4, 5}\n\n\n\n\nSubset and superset\n\n\nCode\nis_subset = a.issubset(b)\nis_superset = a.issuperset(b)\nprint(f\"Is a a subset of b? {is_subset}\")\nprint(f\"Is a a superset of b? {is_superset}\")\n\n\nIs a a subset of b? False\nIs a a superset of b? False"
  },
  {
    "objectID": "course-materials/cheatsheets/comprehensions.html",
    "href": "course-materials/cheatsheets/comprehensions.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "This cheatsheet provides a quick reference for using comprehensions in Python, including list comprehensions, dictionary comprehensions, and how to incorporate conditional logic. Use this as a guide during your master’s program to write more concise and readable code."
  },
  {
    "objectID": "course-materials/cheatsheets/comprehensions.html#list-comprehensions",
    "href": "course-materials/cheatsheets/comprehensions.html#list-comprehensions",
    "title": "EDS 217 Cheatsheet",
    "section": "List Comprehensions",
    "text": "List Comprehensions\n\nBasic Syntax\nA list comprehension provides a concise way to create lists. The basic syntax is:\n\n\nCode\n# [expression for item in iterable]\nsquares = [i ** 2 for i in range(1, 6)]\nprint(squares)\n\n\n[1, 4, 9, 16, 25]\n\n\n\n\nWith Conditional Logic\nYou can add a condition to include only certain items in the new list:\n\n\nCode\n# [expression for item in iterable if condition]\neven_squares = [i ** 2 for i in range(1, 6) if i % 2 == 0]\nprint(even_squares)\n\n\n[4, 16]\n\n\n\n\nNested List Comprehensions\nList comprehensions can be nested to handle more complex data structures:\n\n\nCode\n# [(expression1, expression2) for item1 in iterable1 for item2 in iterable2]\npairs = [(i, j) for i in range(1, 4) for j in range(1, 3)]\nprint(pairs)\n\n\n[(1, 1), (1, 2), (2, 1), (2, 2), (3, 1), (3, 2)]\n\n\n\n\nEvaluating Functions in a List Comprehension\nYou can use list comprehensions to apply a function to each item in an iterable:\n\n\nCode\n# Function to evaluate\ndef square(x):\n    return x ** 2\n\n# List comprehension applying the function\nsquares = [square(i) for i in range(1, 6)]\nprint(squares)\n\n\n[1, 4, 9, 16, 25]"
  },
  {
    "objectID": "course-materials/cheatsheets/comprehensions.html#dictionary-comprehensions",
    "href": "course-materials/cheatsheets/comprehensions.html#dictionary-comprehensions",
    "title": "EDS 217 Cheatsheet",
    "section": "Dictionary Comprehensions",
    "text": "Dictionary Comprehensions\n\nBasic Syntax\nDictionary comprehensions provide a concise way to create dictionaries. The basic syntax is:\n\n\nCode\n# {key_expression: value_expression for item in iterable}\n# Example: Mapping fruit names to their lengths\nfruits = ['apple', 'banana', 'cherry']\nfruit_lengths = {fruit: len(fruit) for fruit in fruits}\nprint(fruit_lengths)\n\n\n{'apple': 5, 'banana': 6, 'cherry': 6}\n\n\n\n\nWithout zip\nYou can create a dictionary without using zip by leveraging the index:\n\n\nCode\n# {key_expression: value_expression for index in range(len(list))}\n# Example: Mapping employee IDs to names\nemployee_ids = [101, 102, 103]\nemployee_names = ['Alice', 'Bob', 'Charlie']\nid_to_name = {employee_ids[i]: employee_names[i] for i in range(len(employee_ids))}\nprint(id_to_name)\n\n\n{101: 'Alice', 102: 'Bob', 103: 'Charlie'}\n\n\n\n\nWith Conditional Logic\nYou can include conditions to filter out key-value pairs:\n\n\nCode\n# {key_expression: value_expression for item in iterable if condition}\n# Example: Filtering students who passed\nstudents = ['Alice', 'Bob', 'Charlie']\nscores = [85, 62, 90]\npassing_students = {students[i]: scores[i] for i in range(len(students)) if scores[i] &gt;= 70}\nprint(passing_students)\n\n\n{'Alice': 85, 'Charlie': 90}\n\n\n\n\nEvaluating Functions in a Dictionary Comprehension\nYou can use dictionary comprehensions to apply a function to values in an iterable:\n\n\nCode\n# Function to evaluate\ndef capitalize_name(name):\n    return name.upper()\n\n# Example: Mapping student names to capitalized names\nstudents = ['alice', 'bob', 'charlie']\ncapitalized_names = {name: capitalize_name(name) for name in students}\nprint(capitalized_names)\n\n\n{'alice': 'ALICE', 'bob': 'BOB', 'charlie': 'CHARLIE'}"
  },
  {
    "objectID": "course-materials/cheatsheets/comprehensions.html#best-practices-for-using-comprehensions",
    "href": "course-materials/cheatsheets/comprehensions.html#best-practices-for-using-comprehensions",
    "title": "EDS 217 Cheatsheet",
    "section": "Best Practices for Using Comprehensions",
    "text": "Best Practices for Using Comprehensions\n\nKeep It Simple: Use comprehensions for simple transformations and filtering. For complex logic, consider using traditional loops for better readability.\nNested Comprehensions: While powerful, nested comprehensions can be hard to read. Use them sparingly and consider breaking down the logic into multiple steps if needed.\nReadability: Always prioritize code readability. If a comprehension is difficult to understand, it might be better to use a loop."
  },
  {
    "objectID": "course-materials/cheatsheets/comprehensions.html#additional-resources",
    "href": "course-materials/cheatsheets/comprehensions.html#additional-resources",
    "title": "EDS 217 Cheatsheet",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nOfficial Python Documentation: List Comprehensions\nPython Dictionary Comprehensions: Dictionary Comprehensions\nPEP 202: PEP 202 - List Comprehensions"
  },
  {
    "objectID": "course-materials/coding-colabs/3d_pandas_series.html",
    "href": "course-materials/coding-colabs/3d_pandas_series.html",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "",
    "text": "In this collaborative coding exercise, we’ll explore Pandas Series, a fundamental data structure in the Pandas library. You’ll work together to create, manipulate, and analyze Series objects."
  },
  {
    "objectID": "course-materials/coding-colabs/3d_pandas_series.html#introduction",
    "href": "course-materials/coding-colabs/3d_pandas_series.html#introduction",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "",
    "text": "In this collaborative coding exercise, we’ll explore Pandas Series, a fundamental data structure in the Pandas library. You’ll work together to create, manipulate, and analyze Series objects."
  },
  {
    "objectID": "course-materials/coding-colabs/3d_pandas_series.html#resources",
    "href": "course-materials/coding-colabs/3d_pandas_series.html#resources",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Resources",
    "text": "Resources\nHere’s our course cheatsheet on pandas Series:\n\nPandas Series Cheatsheet\n\nFeel free to refer to this cheatsheet throughout the exercise if you need a quick reminder about syntax or functionality."
  },
  {
    "objectID": "course-materials/coding-colabs/3d_pandas_series.html#setup",
    "href": "course-materials/coding-colabs/3d_pandas_series.html#setup",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Setup",
    "text": "Setup\nFirst, let’s import the necessary libraries and create a sample Series.\n\nimport pandas as pd\nimport numpy as np\n\n# Create a sample Series\nfruits = pd.Series(['apple', 'banana', 'cherry', 'date', 'elderberry'], name='Fruits')\nprint(fruits)\n\n0         apple\n1        banana\n2        cherry\n3          date\n4    elderberry\nName: Fruits, dtype: object"
  },
  {
    "objectID": "course-materials/coding-colabs/3d_pandas_series.html#exercise-1-creating-a-series",
    "href": "course-materials/coding-colabs/3d_pandas_series.html#exercise-1-creating-a-series",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Exercise 1: Creating a Series",
    "text": "Exercise 1: Creating a Series\nWork together to create a Series representing the prices of the fruits in our fruits Series.\n\n# Your code here\n# Create a Series called 'prices' with the same index as 'fruits'\n# Use these prices: apple: $0.5, banana: $0.3, cherry: $1.0, date: $1.5, elderberry: $2.0"
  },
  {
    "objectID": "course-materials/coding-colabs/3d_pandas_series.html#exercise-2-series-operations",
    "href": "course-materials/coding-colabs/3d_pandas_series.html#exercise-2-series-operations",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Exercise 2: Series Operations",
    "text": "Exercise 2: Series Operations\nCollaborate to perform the following operations:\n\nCalculate the total price of all fruits.\nFind the most expensive fruit.\nApply a 10% discount to all fruits priced over $1.0.\n\n\n# Your code here\n# 1. Calculate the total price of all fruits\n# 2. Find the most expensive fruit\n# 3. Apply a 10% discount to all fruits priced over $1.0"
  },
  {
    "objectID": "course-materials/coding-colabs/3d_pandas_series.html#exercise-3-series-analysis",
    "href": "course-materials/coding-colabs/3d_pandas_series.html#exercise-3-series-analysis",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Exercise 3: Series Analysis",
    "text": "Exercise 3: Series Analysis\nWork as a team to answer the following questions:\n\nWhat is the average price of the fruits?\nHow many fruits cost less than $1.0?\nWhat is the price range (difference between max and min prices)?\n\n\n# Your code here\n# 1. Calculate the average price of the fruits\n# 2. Count how many fruits cost less than $1.0\n# 3. Calculate the price range (difference between max and min prices)"
  },
  {
    "objectID": "course-materials/coding-colabs/3d_pandas_series.html#exercise-4-series-manipulation",
    "href": "course-materials/coding-colabs/3d_pandas_series.html#exercise-4-series-manipulation",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Exercise 4: Series Manipulation",
    "text": "Exercise 4: Series Manipulation\nCollaborate to perform these manipulations on the fruits and prices Series:\n\nAdd a new fruit ‘fig’ with a price of $1.2 to both Series using pd.concat\nRemove ‘banana’ from both Series.\nSort both Series by fruit name (alphabetically).\n\n\n# Your code here\n# 1. Add 'fig' to both Series (price: $1.2)\n# 2. Remove 'banana' from both Series\n# 3. Sort both Series alphabetically by fruit name"
  },
  {
    "objectID": "course-materials/coding-colabs/3d_pandas_series.html#conclusion",
    "href": "course-materials/coding-colabs/3d_pandas_series.html#conclusion",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Conclusion",
    "text": "Conclusion\nIn this collaborative exercise, you’ve practiced creating, manipulating, and analyzing Pandas Series. You’ve learned how to perform basic operations, apply conditions, and modify Series objects. These skills will be valuable as you work with more complex datasets in the future."
  },
  {
    "objectID": "course-materials/coding-colabs/3d_pandas_series.html#discussion-questions",
    "href": "course-materials/coding-colabs/3d_pandas_series.html#discussion-questions",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Discussion Questions",
    "text": "Discussion Questions\n\nWhat advantages does using a Pandas Series offer compared to using a Python list or dictionary?\nCan you think of a real-world scenario where you might use a Pandas Series instead of a DataFrame?\nWhat challenges did you face while working with Series in this exercise, and how did you overcome them?\n\nDiscuss these questions with your team and share your insights.\n\nEnd Coding Colab Session (Day 4)"
  },
  {
    "objectID": "course-materials/lectures/seaborn.html#philosophy-of-seaborn",
    "href": "course-materials/lectures/seaborn.html#philosophy-of-seaborn",
    "title": "Introduction to Seaborn",
    "section": "Philosophy of Seaborn",
    "text": "Philosophy of Seaborn\nSeaborn aims to make visualization a central part of exploring and understanding data.\nIts dataset-oriented plotting functions operate on dataframes and arrays containing whole datasets.\nIt tries to automatically perform semantic mapping and statistical aggregation to produce informative plots."
  },
  {
    "objectID": "course-materials/lectures/seaborn.html#main-ideas-in-seaborn",
    "href": "course-materials/lectures/seaborn.html#main-ideas-in-seaborn",
    "title": "Introduction to Seaborn",
    "section": "Main Ideas in Seaborn",
    "text": "Main Ideas in Seaborn\n\nIntegration with Pandas: Works well with Pandas data structures.\nBuilt-in Themes: Provides built-in themes for styling matplotlib graphics.\nColor Palettes: Offers a variety of color palettes to reveal patterns in the data.\nStatistical Estimation: Seaborn includes functions to fit and visualize linear regression models."
  },
  {
    "objectID": "course-materials/lectures/seaborn.html#major-features-of-seaborn",
    "href": "course-materials/lectures/seaborn.html#major-features-of-seaborn",
    "title": "Introduction to Seaborn",
    "section": "Major Features of Seaborn",
    "text": "Major Features of Seaborn\nSeaborn simplifies many aspects of creating complex visualizations in Python. Some of its major features include:\n\nFacetGrids and PairGrids: For plotting conditional relationships.\nFactorplot: For categorical variables.\nJointplot: For joint distributions.\nTime Series functionality: Through functions like tsplot."
  },
  {
    "objectID": "course-materials/lectures/seaborn.html#using-seaborn",
    "href": "course-materials/lectures/seaborn.html#using-seaborn",
    "title": "Introduction to Seaborn",
    "section": "Using seaborn",
    "text": "Using seaborn\nimport seaborn as sns\nWhy sns?"
  },
  {
    "objectID": "course-materials/lectures/seaborn.html#theme-options",
    "href": "course-materials/lectures/seaborn.html#theme-options",
    "title": "Introduction to Seaborn",
    "section": "Theme Options",
    "text": "Theme Options\n# Set the theme to whitegrid\nsns.set_theme(style=\"whitegrid\")\n\ndarkgrid: The default theme. Background is a dark gray grid (not to be confused with a solid gray).\nwhitegrid: Similar to darkgrid but with a lighter background. This theme is particularly useful for plots with dense data points."
  },
  {
    "objectID": "course-materials/lectures/seaborn.html#themes-continued",
    "href": "course-materials/lectures/seaborn.html#themes-continued",
    "title": "Introduction to Seaborn",
    "section": "Themes (continued)",
    "text": "Themes (continued)\n\ndark: This theme provides a dark background without any grid lines. It’s suitable for presentations or where visuals are prioritized.\nwhite: Offers a clean, white background without grid lines. This is well in situations where the data and annotations need to stand out without any additional distraction.\nticks: This theme is similar to the white theme but adds ticks on the axes, which enhances the precision of interpreting the data."
  },
  {
    "objectID": "course-materials/lectures/seaborn.html#getting-ready-to-seaborn",
    "href": "course-materials/lectures/seaborn.html#getting-ready-to-seaborn",
    "title": "Introduction to Seaborn",
    "section": "Getting ready to Seaborn",
    "text": "Getting ready to Seaborn\nImport the library and set a style\n\nimport seaborn as sns # (but now you know it should have been ssn 🤓)\nsns.set(style=\"darkgrid\") # This is the default, so skip it if wanted"
  },
  {
    "objectID": "course-materials/lectures/seaborn.html#conclusion",
    "href": "course-materials/lectures/seaborn.html#conclusion",
    "title": "Introduction to Seaborn",
    "section": "Conclusion",
    "text": "Conclusion\nSeaborn is a versatile and powerful tool for statistical data visualization in Python. Whether you need to visualize the distribution of a dataset, the relationship between multiple variables, or the dependencies between categorical data, Seaborn has a plot type to make your analysis more intuitive and insightful."
  },
  {
    "objectID": "course-materials/coding-colabs/7c_visualizations.html",
    "href": "course-materials/coding-colabs/7c_visualizations.html",
    "title": "Day 7: 🙌 Coding Colab",
    "section": "",
    "text": "In this collaborative coding exercise, you’ll work with a partner to explore a dataset using the seaborn library. You’ll focus on a workflow that includes:\n\nExploring distributions with histograms\nExamining correlations among variables\nInvestigating relationships more closely with regression plots and joint distribution plots\n\nWe’ll be using the Palmer Penguins dataset, which contains information about different penguin species, their physical characteristics, and the islands they inhabit."
  },
  {
    "objectID": "course-materials/coding-colabs/7c_visualizations.html#introduction",
    "href": "course-materials/coding-colabs/7c_visualizations.html#introduction",
    "title": "Day 7: 🙌 Coding Colab",
    "section": "",
    "text": "In this collaborative coding exercise, you’ll work with a partner to explore a dataset using the seaborn library. You’ll focus on a workflow that includes:\n\nExploring distributions with histograms\nExamining correlations among variables\nInvestigating relationships more closely with regression plots and joint distribution plots\n\nWe’ll be using the Palmer Penguins dataset, which contains information about different penguin species, their physical characteristics, and the islands they inhabit."
  },
  {
    "objectID": "course-materials/coding-colabs/7c_visualizations.html#setup",
    "href": "course-materials/coding-colabs/7c_visualizations.html#setup",
    "title": "Day 7: 🙌 Coding Colab",
    "section": "Setup",
    "text": "Setup\nFirst, let’s import the necessary libraries and load our dataset.\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Set the style for better-looking plots\nsns.set_style(\"whitegrid\")\n\n# Load the Palmer Penguins dataset\npenguins = sns.load_dataset(\"penguins\")\n\n# Display the first few rows and basic information about the dataset\nprint(penguins.head())\nprint(penguins.info())\n\n\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0  Adelie  Torgersen            39.1           18.7              181.0   \n1  Adelie  Torgersen            39.5           17.4              186.0   \n2  Adelie  Torgersen            40.3           18.0              195.0   \n3  Adelie  Torgersen             NaN            NaN                NaN   \n4  Adelie  Torgersen            36.7           19.3              193.0   \n\n   body_mass_g     sex  \n0       3750.0    Male  \n1       3800.0  Female  \n2       3250.0  Female  \n3          NaN     NaN  \n4       3450.0  Female  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \ndtypes: float64(4), object(3)\nmemory usage: 18.9+ KB\nNone"
  },
  {
    "objectID": "course-materials/coding-colabs/7c_visualizations.html#task-1-exploring-distributions-with-histograms",
    "href": "course-materials/coding-colabs/7c_visualizations.html#task-1-exploring-distributions-with-histograms",
    "title": "Day 7: 🙌 Coding Colab",
    "section": "Task 1: Exploring Distributions with Histograms",
    "text": "Task 1: Exploring Distributions with Histograms\nLet’s start by exploring the distributions of various numerical variables in our dataset using histograms.\n\nCreate histograms for ‘bill_length_mm’, ‘bill_depth_mm’, ‘flipper_length_mm’, and ‘body_mass_g’.\nExperiment with different numbers of bins to see how it affects the visualization.\nTry using sns.histplot() with the ‘kde’ parameter set to True to overlay a kernel density estimate."
  },
  {
    "objectID": "course-materials/coding-colabs/7c_visualizations.html#task-2-examining-correlations",
    "href": "course-materials/coding-colabs/7c_visualizations.html#task-2-examining-correlations",
    "title": "Day 7: 🙌 Coding Colab",
    "section": "Task 2: Examining Correlations",
    "text": "Task 2: Examining Correlations\nNow, let’s look at the correlations between the numerical variables in our dataset using Seaborn’s built-in correlation plot.\n\nUse sns.pairplot() to create a grid of scatter plots for all numeric variables.\nModify the pairplot to show the species information using different colors.\nInterpret the pairplot: which variables seem to be most strongly correlated? Do you notice any patterns related to species?"
  },
  {
    "objectID": "course-materials/coding-colabs/7c_visualizations.html#task-3-investigating-relationships-with-regression-plots",
    "href": "course-materials/coding-colabs/7c_visualizations.html#task-3-investigating-relationships-with-regression-plots",
    "title": "Day 7: 🙌 Coding Colab",
    "section": "Task 3: Investigating Relationships with Regression Plots",
    "text": "Task 3: Investigating Relationships with Regression Plots\nLet’s dig deeper into the relationships between variables using regression plots.\n\nCreate a regression plot (sns.regplot) showing the relationship between ‘flipper_length_mm’ and ‘body_mass_g’.\nCreate another regplot showing the relationship between ‘bill_length_mm’ and ‘bill_depth_mm’.\nTry adding the ‘species’ information to one of these plots using different colors. Hint: You might want to use sns.lmplot for this."
  },
  {
    "objectID": "course-materials/coding-colabs/7c_visualizations.html#task-4-joint-distribution-plots",
    "href": "course-materials/coding-colabs/7c_visualizations.html#task-4-joint-distribution-plots",
    "title": "Day 7: 🙌 Coding Colab",
    "section": "Task 4: Joint Distribution Plots",
    "text": "Task 4: Joint Distribution Plots\nFinally, let’s use joint distribution plots to examine both the relationship between two variables and their individual distributions.\n\nCreate a joint plot for ‘flipper_length_mm’ and ‘body_mass_g’.\nExperiment with different kind parameters in the joint plot (e.g., ‘scatter’, ‘kde’, ‘hex’).\nCreate another joint plot, this time for ‘bill_length_mm’ and ‘bill_depth_mm’, colored by species."
  },
  {
    "objectID": "course-materials/coding-colabs/7c_visualizations.html#bonus-challenge",
    "href": "course-materials/coding-colabs/7c_visualizations.html#bonus-challenge",
    "title": "Day 7: 🙌 Coding Colab",
    "section": "Bonus Challenge",
    "text": "Bonus Challenge\nIf you finish early, try this bonus challenge:\nCreate a correlation matrix heatmap using Seaborn’s sns.heatmap() function. This will provide a different view of the correlations between variables compared to the pairplot.\n\nCreate a correlation matrix using the numerical columns in the dataset.\n\n\n\n\n\n\n\nCreating correlation matricies in pandas\n\n\n\nPandas dataframes include two built-in methods that can be combined to quickly create a correlation matrix between all the numerical data in a dataframe.\n\n.select_dtypes() is a method that selects only the columns of a dataframe that match a type of data. Running the .select_dtypes(include=np.number) method on a dataframe will return a new dataframe that contains only the columns that have a numeric datatype.\n.corr() is a method that creates a correlation matrix between every column in a dataframe. For it to work, you need to make sure you only have numeric data in your dataframe, so chaining this method after the .select_dtypes() method will get you a complete correlation matrix in a single line of code!\n\n\n\n\nVisualize this correlation matrix using sns.heatmap().\nCustomize the heatmap by adding annotations and adjusting the colormap.\nCompare the insights from this heatmap with those from the pairplot. What additional information does each visualization provide?"
  },
  {
    "objectID": "course-materials/coding-colabs/7c_visualizations.html#conclusion",
    "href": "course-materials/coding-colabs/7c_visualizations.html#conclusion",
    "title": "Day 7: 🙌 Coding Colab",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve practiced using seaborn to explore a dataset through various visualization techniques. Often these visualizations can be very helpful at the start of a data exploration activity as they are fundamental to exploratory data analysis in Python. As such, they will be valuable as you continue to work with more complex datasets.\n\nEnd Coding Colab Session (Day 7)"
  },
  {
    "objectID": "course-materials/interactive-sessions/2b_dictionaries.html#getting-started",
    "href": "course-materials/interactive-sessions/2b_dictionaries.html#getting-started",
    "title": "Interactive Session 2B",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_2B_Dictionaries.ipynb\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day 2: Session B - Dictionaries\n\n[Link to session webpage](https://eds-217-essential-python.github.io/course-materials/interactive-sessions/2b_dictionaries.html)\n\nDate: 09/04/2024\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "course-materials/interactive-sessions/2b_dictionaries.html#part-1-basic-concepts-with-species-lookup-table",
    "href": "course-materials/interactive-sessions/2b_dictionaries.html#part-1-basic-concepts-with-species-lookup-table",
    "title": "Interactive Session 2B",
    "section": "Part 1: Basic Concepts with Species Lookup Table",
    "text": "Part 1: Basic Concepts with Species Lookup Table\n\nIntroduction to Dictionaries\nDictionaries in Python are collections of key-value pairs that allow for efficient data storage and retrieval. Each key maps to a specific value, making dictionaries ideal for representing real-world data in a structured format.\nProbably the easiest mental model for thinking about structured data is a spreadsheet. You are all familiar with Excel spreadsheets, with their numbered rows and lettered columns. In the spreadsheet, data is often “structured” so that each row is an entry, and each column is perhaps a variable recorded for that entry.\n\n\n\nstructured-data"
  },
  {
    "objectID": "course-materials/interactive-sessions/2b_dictionaries.html#instructions",
    "href": "course-materials/interactive-sessions/2b_dictionaries.html#instructions",
    "title": "Interactive Session 2B",
    "section": "Instructions",
    "text": "Instructions\nWe will work through this material together, writing a new notebook as we go.\n\n\n\n\n\n\nNote\n\n\n\n🐍     This symbol designates an important note about Python structure, syntax, or another quirk.\n\n\n\n✏️   This symbol designates code you should add to your notebook and run."
  },
  {
    "objectID": "course-materials/interactive-sessions/2b_dictionaries.html#dictionaries",
    "href": "course-materials/interactive-sessions/2b_dictionaries.html#dictionaries",
    "title": "Interactive Session 2B",
    "section": "Dictionaries",
    "text": "Dictionaries\n\nTLDR: Dictionaries are a very common collection type that allows data to be organized using a key:value framework. Because of the similarity between key:value pairs and many data structures (e.g. “lookup tables”), you will see Dictionaries quite a bit when working in python\n\nThe first collection we will look at today is the dictionary, or dict. This is one of the most powerful data structures in python. It is a mutable, unordered collection, which means that it can be altered, but elements within the structure cannot be referenced by their position and they cannot be sorted.\nYou can create a dictionary using the {}, providing both a key and a value, which are separated by a :."
  },
  {
    "objectID": "course-materials/interactive-sessions/2b_dictionaries.html#creating-manipulating-dictionaries",
    "href": "course-materials/interactive-sessions/2b_dictionaries.html#creating-manipulating-dictionaries",
    "title": "Interactive Session 2B",
    "section": "Creating & Manipulating Dictionaries",
    "text": "Creating & Manipulating Dictionaries\nWe’ll start by creating a dictionary to store the common name of various species found in California’s coastal tidepools.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Define a dictionary with species data containing latin names and corresponding common names.\nspecies_dict = {\n    \"P ochraceus\": \"Ochre sea star\",\n    \"M californianus\": \"California mussel\",\n    \"H rufescens\": \"Red abalone\"\n}\n\n\n\n\n\n\n\n\nNote\n\n\n\n🐍 &lt;b&gt;Note.&lt;/b&gt; The use of whitespace and indentation is important in python. In the example above, the dictionary entries are indented relative to the brackets &lt;code&gt;{&lt;/code&gt; and &lt;code&gt;}&lt;/code&gt;. In addition, there is no space between the &lt;code&gt;'key'&lt;/code&gt;, the &lt;code&gt;:&lt;/code&gt;, and the &lt;code&gt;'value'&lt;/code&gt; for each entry. Finally, notice that there is a &lt;code&gt;,&lt;/code&gt; following each dictionary entry. This pattern is the same as all of the other &lt;i&gt;collection&lt;/i&gt; data types we've seen so far, including &lt;b&gt;list&lt;/b&gt;, &lt;b&gt;set&lt;/b&gt;, and &lt;b&gt;tuple&lt;/b&gt;.\n\n\n\nAccessing elements in a dictionary\nAccessing an element in a dictionary is easy if you know what you are looking for.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nspecies_dict['M californianus']\n\n\n'California mussel'\n\n\n\n\nAdding a New Species\nBecause dictionaries are mutable, it is easy to add additional entries and doing so is straightforward. You specify the key and the value it maps to.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Adding a new entry for Leather star\nspecies_dict[\"D imbricata\"] = \"Leather star\"\n\n\n\n\nAccessing and Modifying Data\nAccessing data in a dictionary can be done directly by the key, and modifications are just as direct.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Accessing a species by its latin name\nprint(\"Common name for P ochraceus:\", species_dict[\"P ochraceus\"])\n\n\nCommon name for P ochraceus: Ochre sea star\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Updating the common name for Ochre Sea Star abalone\nspecies_dict[\"P ochraceus\"] = \"Purple Starfish\"\nprint(\"Updated data for Pisaster ochraceus:\", species_dict[\"P ochraceus\"])\n\n\nUpdated data for Pisaster ochraceus: Purple Starfish\n\n\n\n\nRemoving a Dictionary Element\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Removing \"P ochraceus\"\ndel species_dict[\"P ochraceus\"]\nprint(f\"Deleted data for Pisaster ochraceus, new dictionary: {species_dict}\")\n\n\nDeleted data for Pisaster ochraceus, new dictionary: {'M californianus': 'California mussel', 'H rufescens': 'Red abalone', 'D imbricata': 'Leather star'}"
  },
  {
    "objectID": "course-materials/interactive-sessions/2b_dictionaries.html#accessing-dictionary-keys-and-values",
    "href": "course-materials/interactive-sessions/2b_dictionaries.html#accessing-dictionary-keys-and-values",
    "title": "Interactive Session 2B",
    "section": "Accessing dictionary keys and values",
    "text": "Accessing dictionary keys and values\nEvery dictionary has builtin methods to retrieve its keys and values. These functions are called, appropriately, keys() and values()\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Accessing the dictionary keys:\nlatin_names = species_dict.keys()\nprint(f\"Dictionary keys (latin names): {latin_names}\")\n\n\nDictionary keys (latin names): dict_keys(['M californianus', 'H rufescens', 'D imbricata'])\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Accessing the dictionary values\ncommon_names = species_dict.values()\nprint(f\"Dictionary values (common names): {common_names}\")\n\n\nDictionary values (common names): dict_values(['California mussel', 'Red abalone', 'Leather star'])\n\n\n\n\n\n\n\n\nNote\n\n\n\n🐍 Note. The keys() and values() functions return a dict_key object and dict_values object, respectively. Each of these objects contains a list of either the keys or values. You can force the result of the keys() or values() function into a list by wrapping either one in a list() command."
  },
  {
    "objectID": "course-materials/interactive-sessions/2b_dictionaries.html#looping-through-dictionaries",
    "href": "course-materials/interactive-sessions/2b_dictionaries.html#looping-through-dictionaries",
    "title": "Interactive Session 2B",
    "section": "Looping through Dictionaries ",
    "text": "Looping through Dictionaries \nPython has an efficient way to loop through all the keys and values of a dictionary at the same time. The items() method returns a tuple containing a (key, value) for each element in a dictionary. In practice this means that we can loop through a dictionary in the following way:\n\n\nCode\nmy_dict = {'name': 'Homer Simpson',\n           'occupation': 'nuclear engineer',\n           'address': '742 Evergreen Terrace',\n           'city': 'Springfield',\n           'state': ' ? '\n          }\n\nfor key, value in my_dict.items():\n    print(f\"{key.capitalize()}: {value}.\")\n\n\nName: Homer Simpson.\nOccupation: nuclear engineer.\nAddress: 742 Evergreen Terrace.\nCity: Springfield.\nState:  ? .\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\nAdd a new code cell and code to loop through the species_dict dictionary and print out a sentence providing the common name of each species (e.g. “The common name of M californianus” is…“)."
  },
  {
    "objectID": "course-materials/interactive-sessions/2b_dictionaries.html#accessing-un-assigned-elements-in-dictionaries",
    "href": "course-materials/interactive-sessions/2b_dictionaries.html#accessing-un-assigned-elements-in-dictionaries",
    "title": "Interactive Session 2B",
    "section": "Accessing un-assigned elements in Dictionaries",
    "text": "Accessing un-assigned elements in Dictionaries\nAttempting to retrieve an element of a dictionary that doesn’t exist is the same as requesting an index of a list that doesn’t exist - Python will raise an Exception. For example, if you attempt to retrieve the definition of a field that hasn’t been defined, then you get an error:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\nspecies_dict[\"E dofleini\"]\nYou should get a KeyError exception:\nKeyError: ‘E dofleini’\nTo avoid getting an error when requesting an element from a dict, you can use the get() function. The get() function will return None if the element doesn’t exist:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nspecies_description = species_dict.get(\"E dofleini\")\nprint(\"Accessing non-existent latin name, E dofleini:\\n\", species_description)\n\n\nAccessing non-existent latin name, E dofleini:\n None\n\n\nYou can also provide an argument to python to return if the item isn’t found:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nspecies_description = species_dict.get(\"E dofleini\", \"Species not found in dictionary\")\nprint(\"Accessing non-existent latin name, E dofleini:\\n\", species_description)\n\n\nAccessing non-existent latin name, E dofleini:\n Species not found in dictionary"
  },
  {
    "objectID": "course-materials/interactive-sessions/2b_dictionaries.html#summary-and-additional-resources",
    "href": "course-materials/interactive-sessions/2b_dictionaries.html#summary-and-additional-resources",
    "title": "Interactive Session 2B",
    "section": "Summary and Additional Resources",
    "text": "Summary and Additional Resources\nWe’ve explored the creation, modification, and application of dictionaries in Python, highlighting their utility in storing structured data. As you progress in Python, you’ll find dictionaries indispensable across various applications, from data analysis to machine learning.\nFor further study, consult the following resources: - Python’s Official Documentation on Dictionaries - Our class Dictionary Cheatsheet\n\n\nEnd interactive session 2B"
  },
  {
    "objectID": "course-materials/coding-colabs/6b_advanced_data_manipulation.html",
    "href": "course-materials/coding-colabs/6b_advanced_data_manipulation.html",
    "title": "Day 6: 🙌 Coding Colab",
    "section": "",
    "text": "In this coding colab, you’ll analyze global temperature anomalies and CO2 concentration data. You’ll practice data manipulation, joining datasets, time series analysis, and visualization techniques."
  },
  {
    "objectID": "course-materials/coding-colabs/6b_advanced_data_manipulation.html#introduction",
    "href": "course-materials/coding-colabs/6b_advanced_data_manipulation.html#introduction",
    "title": "Day 6: 🙌 Coding Colab",
    "section": "",
    "text": "In this coding colab, you’ll analyze global temperature anomalies and CO2 concentration data. You’ll practice data manipulation, joining datasets, time series analysis, and visualization techniques."
  },
  {
    "objectID": "course-materials/coding-colabs/6b_advanced_data_manipulation.html#learning-objectives",
    "href": "course-materials/coding-colabs/6b_advanced_data_manipulation.html#learning-objectives",
    "title": "Day 6: 🙌 Coding Colab",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this colab, you will be able to:\n\nLoad and preprocess time series data\nJoin datasets based on datetime indices\nPerform basic time series analysis and resampling\nApply data manipulation techniques to extract insights from environmental datasets"
  },
  {
    "objectID": "course-materials/coding-colabs/6b_advanced_data_manipulation.html#setup",
    "href": "course-materials/coding-colabs/6b_advanced_data_manipulation.html#setup",
    "title": "Day 6: 🙌 Coding Colab",
    "section": "Setup",
    "text": "Setup\nLet’s start by importing necessary libraries and loading our datasets:\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the temperature anomaly dataset\ntemp_url = \"https://bit.ly/monthly_temp\"\ntemp_df = pd.read_csv(temp_url, parse_dates=['Date'])\n\n# Load the CO2 concentration dataset\nco2_url = \"https://bit.ly/monthly_CO2\"\nco2_df = pd.read_csv(co2_url, parse_dates=['Date'])\n\nprint(\"Temperature data:\")\nprint(temp_df.head())\nprint(\"\\nCO2 data:\")\nprint(co2_df.head())\n\n\nTemperature data:\n        Date  MonthlyAnomaly\n0 1880-01-01           -0.20\n1 1880-02-01           -0.25\n2 1880-03-01           -0.09\n3 1880-04-01           -0.16\n4 1880-05-01           -0.09\n\nCO2 data:\n        Date  CO2Concentration\n0 1958-04-01            317.45\n1 1958-05-01            317.51\n2 1958-06-01            317.27\n3 1958-07-01            315.87\n4 1958-08-01            314.93"
  },
  {
    "objectID": "course-materials/coding-colabs/6b_advanced_data_manipulation.html#task-1-data-preparation",
    "href": "course-materials/coding-colabs/6b_advanced_data_manipulation.html#task-1-data-preparation",
    "title": "Day 6: 🙌 Coding Colab",
    "section": "Task 1: Data Preparation",
    "text": "Task 1: Data Preparation\n\nSet the ‘Date’ column as the index for both dataframes.\nEnsure that there are no missing values in either dataset."
  },
  {
    "objectID": "course-materials/coding-colabs/6b_advanced_data_manipulation.html#task-2-joining-datasets",
    "href": "course-materials/coding-colabs/6b_advanced_data_manipulation.html#task-2-joining-datasets",
    "title": "Day 6: 🙌 Coding Colab",
    "section": "Task 2: Joining Datasets",
    "text": "Task 2: Joining Datasets\n\nMerge the temperature and CO2 datasets based on their date index.\nHandle any missing values that may have been introduced by the merge.\nCreate some plots showing temperature anomalies and CO2 concentrations over time using pandas built-in plotting functions."
  },
  {
    "objectID": "course-materials/coding-colabs/6b_advanced_data_manipulation.html#task-3-time-series-analysis",
    "href": "course-materials/coding-colabs/6b_advanced_data_manipulation.html#task-3-time-series-analysis",
    "title": "Day 6: 🙌 Coding Colab",
    "section": "Task 3: Time Series Analysis",
    "text": "Task 3: Time Series Analysis\n\nResample the data to annual averages.\nCalculate the year-over-year change in temperature anomalies and CO2 concentrations.\nCreate a scatter plot (use the plt.scatter() function) of annual temperature anomalies vs CO2 concentrations."
  },
  {
    "objectID": "course-materials/coding-colabs/6b_advanced_data_manipulation.html#task-4-seasonal-analysis",
    "href": "course-materials/coding-colabs/6b_advanced_data_manipulation.html#task-4-seasonal-analysis",
    "title": "Day 6: 🙌 Coding Colab",
    "section": "Task 4: Seasonal Analysis",
    "text": "Task 4: Seasonal Analysis\n\nCreate a function to extract the season from a given date (hint: use the date.month attribute and if-elif-else to assign the season in your function).\nUse the function to create a new column called Season\nCalculate the average temperature anomaly and CO2 concentration for each season.\nCreate a box plot (use sns.boxplot) showing the distribution of temperature anomalies for each season."
  },
  {
    "objectID": "course-materials/lectures/05_Drop_or_Impute.html#introduction",
    "href": "course-materials/lectures/05_Drop_or_Impute.html#introduction",
    "title": "EDS 217 - Lecture",
    "section": "Introduction",
    "text": "Introduction\n\nData cleaning is crucial in data analysis\nMissing data is a common challenge\nTwo main approaches:\n\nDropping missing data\nImputation\n\nUnderstanding the nature of missingness is key"
  },
  {
    "objectID": "course-materials/lectures/05_Drop_or_Impute.html#types-of-missing-data",
    "href": "course-materials/lectures/05_Drop_or_Impute.html#types-of-missing-data",
    "title": "EDS 217 - Lecture",
    "section": "Types of Missing Data",
    "text": "Types of Missing Data\n\n\nMissing Completely at Random (MCAR)\nMissing at Random (MAR)\nMissing Not at Random (MNAR)"
  },
  {
    "objectID": "course-materials/lectures/05_Drop_or_Impute.html#missing-completely-at-random-mcar",
    "href": "course-materials/lectures/05_Drop_or_Impute.html#missing-completely-at-random-mcar",
    "title": "EDS 217 - Lecture",
    "section": "Missing Completely at Random (MCAR)",
    "text": "Missing Completely at Random (MCAR)\n\nNo relationship between missingness and any values\nExample: Survey responses lost due to a computer glitch\nLeast problematic type of missing data\nDropping MCAR data is generally safe but reduces sample size"
  },
  {
    "objectID": "course-materials/lectures/05_Drop_or_Impute.html#mcar-example-assigning-nan-randomly",
    "href": "course-materials/lectures/05_Drop_or_Impute.html#mcar-example-assigning-nan-randomly",
    "title": "EDS 217 - Lecture",
    "section": "MCAR Example (Assigning nan randomly)",
    "text": "MCAR Example (Assigning nan randomly)\n\nimport pandas as pd\nimport numpy as np\n\n# Create sample data with MCAR\nnp.random.seed(42)\ndf = pd.DataFrame({'A': np.random.rand(100), 'B': np.random.rand(100)})\ndf.loc[np.random.choice(df.index, 10, replace=False), 'A'] = np.nan\nprint(df.isnull().sum())\n\nA    10\nB     0\ndtype: int64"
  },
  {
    "objectID": "course-materials/lectures/05_Drop_or_Impute.html#missing-at-random-mar",
    "href": "course-materials/lectures/05_Drop_or_Impute.html#missing-at-random-mar",
    "title": "EDS 217 - Lecture",
    "section": "Missing at Random (MAR)",
    "text": "Missing at Random (MAR)\n\nMissingness is related to other observed variables\nExample: Older participants more likely to skip income questions\nMore common in real-world datasets\nDropping MAR data can introduce bias"
  },
  {
    "objectID": "course-materials/lectures/05_Drop_or_Impute.html#mar-example-assigning-nan-randomly-filtered-on-column-value",
    "href": "course-materials/lectures/05_Drop_or_Impute.html#mar-example-assigning-nan-randomly-filtered-on-column-value",
    "title": "EDS 217 - Lecture",
    "section": "MAR Example (Assigning nan randomly, filtered on column value)",
    "text": "MAR Example (Assigning nan randomly, filtered on column value)\n\n# Create sample data with MAR\nnp.random.seed(42)\ndf = pd.DataFrame({\n    'Age': np.random.randint(18, 80, 100),\n    'Income': np.random.randint(20000, 100000, 100)\n})\ndf.loc[df['Age'] &gt; 60, 'Income'] = np.where(\n    np.random.rand(len(df[df['Age'] &gt; 60])) &lt; 0.3, \n    np.nan, \n    df.loc[df['Age'] &gt; 60, 'Income']\n)\nprint(df[df['Age'] &gt; 60]['Income'].isnull().sum() / len(df[df['Age'] &gt; 60]))\n\n0.2972972972972973"
  },
  {
    "objectID": "course-materials/lectures/05_Drop_or_Impute.html#missing-not-at-random-mnar",
    "href": "course-materials/lectures/05_Drop_or_Impute.html#missing-not-at-random-mnar",
    "title": "EDS 217 - Lecture",
    "section": "Missing Not at Random (MNAR)",
    "text": "Missing Not at Random (MNAR)\n\nMissingness is related to the missing values themselves\nExample: People with high incomes more likely to skip income questions\nMost problematic type of missing data\nNeither dropping nor simple imputation may be appropriate"
  },
  {
    "objectID": "course-materials/lectures/05_Drop_or_Impute.html#dropping-missing-data",
    "href": "course-materials/lectures/05_Drop_or_Impute.html#dropping-missing-data",
    "title": "EDS 217 - Lecture",
    "section": "Dropping Missing Data",
    "text": "Dropping Missing Data\nPros:\n\n\nSimple and quick\nMaintains the distribution of complete cases\nAppropriate for MCAR data\n\n\nCons:\n\n\nReduces sample size\nCan introduce bias for MAR or MNAR data\nMay lose important information"
  },
  {
    "objectID": "course-materials/lectures/05_Drop_or_Impute.html#drop-example",
    "href": "course-materials/lectures/05_Drop_or_Impute.html#drop-example",
    "title": "EDS 217 - Lecture",
    "section": "Drop Example",
    "text": "Drop Example\n\n# Dropping missing data\ndf_dropped = df.dropna()\nprint(f\"Original shape: {df.shape}, After dropping: {df_dropped.shape}\")\n\nOriginal shape: (100, 2), After dropping: (89, 2)"
  },
  {
    "objectID": "course-materials/lectures/05_Drop_or_Impute.html#imputation",
    "href": "course-materials/lectures/05_Drop_or_Impute.html#imputation",
    "title": "EDS 217 - Lecture",
    "section": "Imputation",
    "text": "Imputation\nPros:\n\n\nPreserves sample size\nCan reduce bias for MAR data\nAllows use of all available information\n\n\nCons:\n\n\nCan introduce bias if done incorrectly\nMay underestimate variability\nCan be computationally intensive for complex methods"
  },
  {
    "objectID": "course-materials/lectures/05_Drop_or_Impute.html#imputation-example",
    "href": "course-materials/lectures/05_Drop_or_Impute.html#imputation-example",
    "title": "EDS 217 - Lecture",
    "section": "Imputation Example",
    "text": "Imputation Example\n\n# Simple mean imputation\ndf_imputed = df.fillna(df.mean())\nprint(f\"Original missing: {df['Income'].isnull().sum()}, After imputation: {df_imputed['Income'].isnull().sum()}\")\n\nOriginal missing: 11, After imputation: 0"
  },
  {
    "objectID": "course-materials/lectures/05_Drop_or_Impute.html#imputation-methods",
    "href": "course-materials/lectures/05_Drop_or_Impute.html#imputation-methods",
    "title": "EDS 217 - Lecture",
    "section": "Imputation Methods",
    "text": "Imputation Methods\n\nSimple imputation:\n\nMean, median, mode\nLast observation carried forward (LOCF)\n\nAdvanced imputation:\n\nMultiple Imputation\nK-Nearest Neighbors (KNN)\nRegression imputation"
  },
  {
    "objectID": "course-materials/lectures/05_Drop_or_Impute.html#best-practices",
    "href": "course-materials/lectures/05_Drop_or_Impute.html#best-practices",
    "title": "EDS 217 - Lecture",
    "section": "Best Practices",
    "text": "Best Practices\n\n\nUnderstand your data and the missingness mechanism\nVisualize patterns of missingness\nConsider the impact on your analysis\nUse appropriate methods based on the type of missingness\nConduct sensitivity analyses\nDocument your approach and assumptions"
  },
  {
    "objectID": "course-materials/lectures/05_Drop_or_Impute.html#conclusion",
    "href": "course-materials/lectures/05_Drop_or_Impute.html#conclusion",
    "title": "EDS 217 - Lecture",
    "section": "Conclusion",
    "text": "Conclusion\n\nUnderstanding the nature of missingness is crucial\nBoth dropping and imputation have pros and cons\nChoose the appropriate method based on:\n\nType of missingness (MCAR, MAR, MNAR)\nSample size\nAnalysis goals\n\nAlways document your approach and conduct sensitivity analyses"
  },
  {
    "objectID": "course-materials/lectures/05_Drop_or_Impute.html#questions",
    "href": "course-materials/lectures/05_Drop_or_Impute.html#questions",
    "title": "EDS 217 - Lecture",
    "section": "Questions?",
    "text": "Questions?\nThank you for your attention!"
  },
  {
    "objectID": "course-materials/coding-colabs/2c_lists_dictionaries_sets.html",
    "href": "course-materials/coding-colabs/2c_lists_dictionaries_sets.html",
    "title": "Day 2: 🙌 Coding Colab",
    "section": "",
    "text": "Before we begin, here are quick links to our course cheatsheets. These may be helpful during the exercise:\n\nPython Basics Cheatsheet\nList Cheatsheet\nDictionaries Cheatsheet\nSets Cheatsheet\n\nFeel free to refer to these cheatsheets throughout the exercise if you need a quick reminder about syntax or functionality."
  },
  {
    "objectID": "course-materials/coding-colabs/2c_lists_dictionaries_sets.html#quick-references",
    "href": "course-materials/coding-colabs/2c_lists_dictionaries_sets.html#quick-references",
    "title": "Day 2: 🙌 Coding Colab",
    "section": "",
    "text": "Before we begin, here are quick links to our course cheatsheets. These may be helpful during the exercise:\n\nPython Basics Cheatsheet\nList Cheatsheet\nDictionaries Cheatsheet\nSets Cheatsheet\n\nFeel free to refer to these cheatsheets throughout the exercise if you need a quick reminder about syntax or functionality."
  },
  {
    "objectID": "course-materials/coding-colabs/2c_lists_dictionaries_sets.html#introduction-to-paired-programming-5-minutes",
    "href": "course-materials/coding-colabs/2c_lists_dictionaries_sets.html#introduction-to-paired-programming-5-minutes",
    "title": "Day 2: 🙌 Coding Colab",
    "section": "Introduction to Paired Programming (5 minutes)",
    "text": "Introduction to Paired Programming (5 minutes)\nWelcome to today’s Coding Colab! In this session, you’ll be working in pairs to explore and reinforce your understanding of lists and dictionaries, while also discovering the unique features of sets.\n\nBenefits of Paired Programming\n\nKnowledge sharing: Learn from each other’s experiences and approaches.\nImproved code quality: Catch errors earlier with two sets of eyes.\nEnhanced problem-solving: Discuss ideas for more creative solutions.\nSkill development: Improve communication and teamwork skills.\n\n\n\nHow to Make the Most of Paired Programming\n\nAssign roles: One person is the “driver” (typing), the other is the “navigator” (reviewing).\nSwitch roles regularly: Swap every 10-15 minutes to stay engaged.\nCommunicate clearly: Explain your thought process and ask questions.\nBe open to ideas: Listen to your partner’s suggestions.\nStay focused: Keep the conversation relevant to the task."
  },
  {
    "objectID": "course-materials/coding-colabs/2c_lists_dictionaries_sets.html#exercise-overview",
    "href": "course-materials/coding-colabs/2c_lists_dictionaries_sets.html#exercise-overview",
    "title": "Day 2: 🙌 Coding Colab",
    "section": "Exercise Overview",
    "text": "Exercise Overview\nThis Coding Colab will reinforce your understanding of lists and dictionaries while introducing you to sets. You’ll work through a series of tasks, discussing and implementing solutions together."
  },
  {
    "objectID": "course-materials/coding-colabs/2c_lists_dictionaries_sets.html#part-1-lists-and-dictionaries-review-15-minutes",
    "href": "course-materials/coding-colabs/2c_lists_dictionaries_sets.html#part-1-lists-and-dictionaries-review-15-minutes",
    "title": "Day 2: 🙌 Coding Colab",
    "section": "Part 1: Lists and Dictionaries Review (15 minutes)",
    "text": "Part 1: Lists and Dictionaries Review (15 minutes)\n\nTask 1: List Operations\nCreate a list of your favorite fruits and perform the following operations:\n\nCreate a list called fruits with at least 3 fruit names.\nAdd a new fruit to the end of the list.\nRemove the second fruit from the list.\nPrint the final list.\n\n\n\nTask 2: Dictionary Operations\nCreate a dictionary representing a simple inventory system:\n\nCreate a dictionary called inventory with at least 3 items and their quantities.\nAdd a new item to the inventory.\nUpdate the quantity of an existing item.\nPrint the final inventory."
  },
  {
    "objectID": "course-materials/coding-colabs/2c_lists_dictionaries_sets.html#part-2-introducing-sets-15-minutes",
    "href": "course-materials/coding-colabs/2c_lists_dictionaries_sets.html#part-2-introducing-sets-15-minutes",
    "title": "Day 2: 🙌 Coding Colab",
    "section": "Part 2: Introducing Sets (15 minutes)",
    "text": "Part 2: Introducing Sets (15 minutes)\nHere’s a Sets Cheatsheet. Sets are a lot like lists, so take a look at the cheatsheet to see how they are created and manipulated!\n\nTask 3: Creating and Manipulating Sets\n\nCreate two sets: evens with even numbers from 2 to 10, and odds with odd numbers from 1 to 9.\nPrint both sets.\nFind and print the union of the two sets.\nFind and print the intersection of the two sets.\nAdd a new element to the evens set.\n\n\n\nTask 4: Combining Set Operations and List Operations\nUsing a set is a great way to remove duplicates in a list.\n\nCreate a list with some duplicates: numbers = [1, 2, 2, 3, 3, 3, 4, 4, 5]\nUse a set to remove duplicates.\nCreate a new list from the set.\nPrint the new list without duplicates"
  },
  {
    "objectID": "course-materials/coding-colabs/2c_lists_dictionaries_sets.html#conclusion-and-discussion-10-minutes",
    "href": "course-materials/coding-colabs/2c_lists_dictionaries_sets.html#conclusion-and-discussion-10-minutes",
    "title": "Day 2: 🙌 Coding Colab",
    "section": "Conclusion and Discussion (10 minutes)",
    "text": "Conclusion and Discussion (10 minutes)\nAs a pair, discuss the following questions:\n\nWhat are the main differences between lists, dictionaries, and sets?\nIn what situations would you prefer to use a set over a list or dictionary?\nHow did working in pairs help you understand these concepts better?"
  },
  {
    "objectID": "course-materials/live-coding/dictionaries.html",
    "href": "course-materials/live-coding/dictionaries.html",
    "title": "[Live Coding] Session 2B",
    "section": "",
    "text": "Introduction to Dictionaries (5 minutes)\nCreating and Accessing Dictionaries (10 minutes)\nManipulating Dictionaries (10 minutes)\nIterating Over Dictionaries (5 minutes)\nStoring Structured Data Using Dictionaries (10 minutes)\nPractical Application in Data Science (5 minutes)\n\n\n\n\n\n\nObjective: Introduce what dictionaries are and their importance in Python.\nKey Points:\n\nDefinition: Dictionaries are collections of key-value pairs.\nUnordered and indexed by keys, making data access fast and efficient.\n\nLive Code Example:\n\nexample_dict = {'name': 'Earth', 'moons': 1}\nprint(\"Example dictionary:\", example_dict)\n\nExample dictionary: {'name': 'Earth', 'moons': 1}\n\n\n\n\n\n\n\nObjective: Show how to create dictionaries using different methods and how to access elements.\nKey Points:\n\nCreating dictionaries using curly braces {} and the dict() constructor.\nAccessing values using keys, demonstrating safe access with .get().\n\nLive Code Example:\n\n# Creating a dictionary using dict()\nanother_dict = dict(name='Mars', moons=2)\nprint(\"Another dictionary (dict()):\", another_dict)\n\nanother_dict2 = {'name': 'Mars',\n                'moons': 2\n              }\n\nprint(\"Another dictionary ({}):\", another_dict2)\nprint(\"Are they the same?\", another_dict==another_dict2)\n\n# Accessing elements\nprint(\"Temperature using get (no default):\", example_dict.get('temp'))\nprint(\"Temperature using get (with default):\", example_dict.get('temp', 'No temperature data'))\n\nAnother dictionary (dict()): {'name': 'Mars', 'moons': 2}\nAnother dictionary ({}): {'name': 'Mars', 'moons': 2}\nAre they the same? True\nTemperature using get (no default): None\nTemperature using get (with default): No temperature data\n\n\n\n\n\n\n\nObjective: Teach how to add, update, delete dictionary items.\nKey Points:\n\nAdding and updating by assigning values to keys.\nRemoving items using del and pop().\n\nLive Code Example:\n\n# Adding a new key-value pair\nanother_dict['atmosphere'] = 'thin'\nprint(\"Updated with atmosphere:\", another_dict)\n\n# Removing an entry using del\ndel another_dict['atmosphere']\nprint(\"After deletion:\", another_dict)\n\n# Removing an entry using pop\nmoons = another_dict.pop('moons', 'No moons key found')\nprint(\"Removed moons:\", moons)\nprint(\"After popping moons:\", another_dict)\n\nUpdated with atmosphere: {'name': 'Mars', 'moons': 2, 'atmosphere': 'thin'}\nAfter deletion: {'name': 'Mars', 'moons': 2}\nRemoved moons: 2\nAfter popping moons: {'name': 'Mars'}\n\n\n\n\n\n\n\nObjective: Explain how to iterate over dictionary keys, values, and key-value pairs.\nKey Points:\n\nUsing .keys(), .values(), and .items() for different iteration needs.\n\nLive Code Example:\n\n# Creating a new dictionary for iteration examples\niteration_dict = {'planet': 'Earth', 'moons': 1, 'orbit': 'Sun'}\n\n# Iterating over keys\nprint(\"Keys:\")\nfor key in iteration_dict.keys():\n    print(f\"Key: {key}\")\n\n# Iterating over values\nprint(\"\\nValues:\")\nfor value in iteration_dict.values():\n    print(f\"Value: {value}\")\n\n# Iterating over items\nprint(\"\\nKey-Value Pairs:\")\nfor key, value in iteration_dict.items():\n    print(f\"{key}: {value}\")\n\nKeys:\nKey: planet\nKey: moons\nKey: orbit\n\nValues:\nValue: Earth\nValue: 1\nValue: Sun\n\nKey-Value Pairs:\nplanet: Earth\nmoons: 1\norbit: Sun\n\n\nAdditional Notes:\n\nThe dict.keys(), dict.values(), and dict.items() methods are used to return view objects that provide a dynamic view on the dictionary’s keys, values, and key-value pairs respectively.\nThese views are iterable and reflect changes to the dictionary, making them highly useful for looping and other operations that involve dictionary elements.\nWhat Each Function Returns\n\ndict.keys():\n\n\nReturns a view object displaying all the keys in the dictionary (default)\nUseful for iterating over keys or checking if certain keys exist within the dictionary.\n\n\ndict.values():\n\n\nReturns a view object that contains all the values in the dictionary.\nThis is helpful for operations that need to access every value, such as aggregations or conditions applied to dictionary values.\n\n\ndict.items():\n\n\nReturns a view object with tuples containing (key, value) pairs.\nExtremely useful for looping through both keys and values simultaneously, allowing operations that depend on both elements.\n\nThese methods are particularly useful in data analysis, data cleaning, or any task where data stored in dictionaries needs systematic processing.\nTo learn more about how these iterables can be utilized in Python, you can visit the official Python documentation on iterables and iterators: Python Iterables and Iterators Documentation\n\n\n\n\n\n\nObjective: Show how dictionaries can handle complex, structured data.\nKey Points:\n\nNested dictionaries and lists to create multi-dimensional data structures.\n\nLive Code Example:\n\n# Nested dictionary for environmental data\nenvironmental_data = {\n  'Location A': {'temperature': 19, 'conditions': ['sunny', 'dry']},\n  'Location B': {'temperature': 22, 'conditions': ['rainy', 'humid']}\n}\nprint(\"Environmental data for Location A:\", environmental_data['Location A']['conditions'])\n\nEnvironmental data for Location A: ['sunny', 'dry']\n\n\n\n\n\n\n\nObjective: Demonstrate the use of dictionaries in data science for data aggregation.\nKey Points:\n\nUsing dictionaries to count occurrences and summarize data.\n\nLive Code Example:\n\nweather_log = ['sunny', 'rainy', 'sunny', 'cloudy', 'sunny', 'rainy']\nweather_count = {}\nfor condition in weather_log:\n    weather_count[condition] = weather_count.get(condition, 0) + 1\nprint(\"Weather condition counts:\", weather_count)\n\nWeather condition counts: {'sunny': 3, 'rainy': 2, 'cloudy': 1}\n\n\n\n\n\n\n\n\nRecap: Highlight the flexibility and power of dictionaries in Python programming, especially for data manipulation and structured data operations."
  },
  {
    "objectID": "course-materials/live-coding/dictionaries.html#session-outline",
    "href": "course-materials/live-coding/dictionaries.html#session-outline",
    "title": "[Live Coding] Session 2B",
    "section": "",
    "text": "Introduction to Dictionaries (5 minutes)\nCreating and Accessing Dictionaries (10 minutes)\nManipulating Dictionaries (10 minutes)\nIterating Over Dictionaries (5 minutes)\nStoring Structured Data Using Dictionaries (10 minutes)\nPractical Application in Data Science (5 minutes)\n\n\n\n\n\n\nObjective: Introduce what dictionaries are and their importance in Python.\nKey Points:\n\nDefinition: Dictionaries are collections of key-value pairs.\nUnordered and indexed by keys, making data access fast and efficient.\n\nLive Code Example:\n\nexample_dict = {'name': 'Earth', 'moons': 1}\nprint(\"Example dictionary:\", example_dict)\n\nExample dictionary: {'name': 'Earth', 'moons': 1}\n\n\n\n\n\n\n\nObjective: Show how to create dictionaries using different methods and how to access elements.\nKey Points:\n\nCreating dictionaries using curly braces {} and the dict() constructor.\nAccessing values using keys, demonstrating safe access with .get().\n\nLive Code Example:\n\n# Creating a dictionary using dict()\nanother_dict = dict(name='Mars', moons=2)\nprint(\"Another dictionary (dict()):\", another_dict)\n\nanother_dict2 = {'name': 'Mars',\n                'moons': 2\n              }\n\nprint(\"Another dictionary ({}):\", another_dict2)\nprint(\"Are they the same?\", another_dict==another_dict2)\n\n# Accessing elements\nprint(\"Temperature using get (no default):\", example_dict.get('temp'))\nprint(\"Temperature using get (with default):\", example_dict.get('temp', 'No temperature data'))\n\nAnother dictionary (dict()): {'name': 'Mars', 'moons': 2}\nAnother dictionary ({}): {'name': 'Mars', 'moons': 2}\nAre they the same? True\nTemperature using get (no default): None\nTemperature using get (with default): No temperature data\n\n\n\n\n\n\n\nObjective: Teach how to add, update, delete dictionary items.\nKey Points:\n\nAdding and updating by assigning values to keys.\nRemoving items using del and pop().\n\nLive Code Example:\n\n# Adding a new key-value pair\nanother_dict['atmosphere'] = 'thin'\nprint(\"Updated with atmosphere:\", another_dict)\n\n# Removing an entry using del\ndel another_dict['atmosphere']\nprint(\"After deletion:\", another_dict)\n\n# Removing an entry using pop\nmoons = another_dict.pop('moons', 'No moons key found')\nprint(\"Removed moons:\", moons)\nprint(\"After popping moons:\", another_dict)\n\nUpdated with atmosphere: {'name': 'Mars', 'moons': 2, 'atmosphere': 'thin'}\nAfter deletion: {'name': 'Mars', 'moons': 2}\nRemoved moons: 2\nAfter popping moons: {'name': 'Mars'}\n\n\n\n\n\n\n\nObjective: Explain how to iterate over dictionary keys, values, and key-value pairs.\nKey Points:\n\nUsing .keys(), .values(), and .items() for different iteration needs.\n\nLive Code Example:\n\n# Creating a new dictionary for iteration examples\niteration_dict = {'planet': 'Earth', 'moons': 1, 'orbit': 'Sun'}\n\n# Iterating over keys\nprint(\"Keys:\")\nfor key in iteration_dict.keys():\n    print(f\"Key: {key}\")\n\n# Iterating over values\nprint(\"\\nValues:\")\nfor value in iteration_dict.values():\n    print(f\"Value: {value}\")\n\n# Iterating over items\nprint(\"\\nKey-Value Pairs:\")\nfor key, value in iteration_dict.items():\n    print(f\"{key}: {value}\")\n\nKeys:\nKey: planet\nKey: moons\nKey: orbit\n\nValues:\nValue: Earth\nValue: 1\nValue: Sun\n\nKey-Value Pairs:\nplanet: Earth\nmoons: 1\norbit: Sun\n\n\nAdditional Notes:\n\nThe dict.keys(), dict.values(), and dict.items() methods are used to return view objects that provide a dynamic view on the dictionary’s keys, values, and key-value pairs respectively.\nThese views are iterable and reflect changes to the dictionary, making them highly useful for looping and other operations that involve dictionary elements.\nWhat Each Function Returns\n\ndict.keys():\n\n\nReturns a view object displaying all the keys in the dictionary (default)\nUseful for iterating over keys or checking if certain keys exist within the dictionary.\n\n\ndict.values():\n\n\nReturns a view object that contains all the values in the dictionary.\nThis is helpful for operations that need to access every value, such as aggregations or conditions applied to dictionary values.\n\n\ndict.items():\n\n\nReturns a view object with tuples containing (key, value) pairs.\nExtremely useful for looping through both keys and values simultaneously, allowing operations that depend on both elements.\n\nThese methods are particularly useful in data analysis, data cleaning, or any task where data stored in dictionaries needs systematic processing.\nTo learn more about how these iterables can be utilized in Python, you can visit the official Python documentation on iterables and iterators: Python Iterables and Iterators Documentation\n\n\n\n\n\n\nObjective: Show how dictionaries can handle complex, structured data.\nKey Points:\n\nNested dictionaries and lists to create multi-dimensional data structures.\n\nLive Code Example:\n\n# Nested dictionary for environmental data\nenvironmental_data = {\n  'Location A': {'temperature': 19, 'conditions': ['sunny', 'dry']},\n  'Location B': {'temperature': 22, 'conditions': ['rainy', 'humid']}\n}\nprint(\"Environmental data for Location A:\", environmental_data['Location A']['conditions'])\n\nEnvironmental data for Location A: ['sunny', 'dry']\n\n\n\n\n\n\n\nObjective: Demonstrate the use of dictionaries in data science for data aggregation.\nKey Points:\n\nUsing dictionaries to count occurrences and summarize data.\n\nLive Code Example:\n\nweather_log = ['sunny', 'rainy', 'sunny', 'cloudy', 'sunny', 'rainy']\nweather_count = {}\nfor condition in weather_log:\n    weather_count[condition] = weather_count.get(condition, 0) + 1\nprint(\"Weather condition counts:\", weather_count)\n\nWeather condition counts: {'sunny': 3, 'rainy': 2, 'cloudy': 1}\n\n\n\n\n\n\n\n\nRecap: Highlight the flexibility and power of dictionaries in Python programming, especially for data manipulation and structured data operations."
  },
  {
    "objectID": "course-materials/cheatsheets/JupyterLab.html",
    "href": "course-materials/cheatsheets/JupyterLab.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Before we can use the Variable Inspector in JupyterLab, we need to install the extension. Follow these steps:\n\nStart a new JupyterLab session in your web browser.\nClick on the “+” button in the top left corner to open the Launcher (it might already be opened).\nUnder “Other”, click on “Terminal” to open a new terminal session.\nIn the terminal, type the following command and press Enter:\npip install lckr-jupyterlab-variableinspector\nWait for the installation to complete. You should see a message indicating successful installation.\nOnce the installation is complete, you need to restart JupyterLab for the changes to take effect. To do this:\n\nSave all your open notebooks and files.\nClose all browser tabs with JupyterLab.\nLogin to https://workbench-1.bren.ucsb.edu again.\nRestart a JupyterLab session\n\n\nAfter restarting JupyterLab, the Variable Inspector extension should be available for use.\n\n\n\nNow that you have installed the Variable Inspector extension, here’s how to use it:\nOpen the Variable Inspector: - Menu: View &gt; Activate Command Palette, then type “variable inspector” - Shortcut: Ctrl + Shift + I (Windows/Linux) or Cmd + Shift + I (Mac) - Right-click in an open notebook and select “Open Variable Inspector” (will be at the bottom of the list)\nThe Variable Inspector shows: - Variable names - Types - Values/shapes - Count (for collections)\n\n\n\n\n\n\nLimits to the Variable Inspector\n\n\n\nThe variable inspector is not suitable for use with large dataframes or large arrays. You should use standard commands like df.head(), df.tail(), df.info(), df.describe() to inspect large dataframes.\n\n\n\n\nCode\n# Example variables\nx = 5\ny = \"Hello\"\nz = [1, 2, 3]\n\n# These will appear in the Variable Inspector"
  },
  {
    "objectID": "course-materials/cheatsheets/JupyterLab.html#variable-inspection-in-jupyterlab",
    "href": "course-materials/cheatsheets/JupyterLab.html#variable-inspection-in-jupyterlab",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Before we can use the Variable Inspector in JupyterLab, we need to install the extension. Follow these steps:\n\nStart a new JupyterLab session in your web browser.\nClick on the “+” button in the top left corner to open the Launcher (it might already be opened).\nUnder “Other”, click on “Terminal” to open a new terminal session.\nIn the terminal, type the following command and press Enter:\npip install lckr-jupyterlab-variableinspector\nWait for the installation to complete. You should see a message indicating successful installation.\nOnce the installation is complete, you need to restart JupyterLab for the changes to take effect. To do this:\n\nSave all your open notebooks and files.\nClose all browser tabs with JupyterLab.\nLogin to https://workbench-1.bren.ucsb.edu again.\nRestart a JupyterLab session\n\n\nAfter restarting JupyterLab, the Variable Inspector extension should be available for use.\n\n\n\nNow that you have installed the Variable Inspector extension, here’s how to use it:\nOpen the Variable Inspector: - Menu: View &gt; Activate Command Palette, then type “variable inspector” - Shortcut: Ctrl + Shift + I (Windows/Linux) or Cmd + Shift + I (Mac) - Right-click in an open notebook and select “Open Variable Inspector” (will be at the bottom of the list)\nThe Variable Inspector shows: - Variable names - Types - Values/shapes - Count (for collections)\n\n\n\n\n\n\nLimits to the Variable Inspector\n\n\n\nThe variable inspector is not suitable for use with large dataframes or large arrays. You should use standard commands like df.head(), df.tail(), df.info(), df.describe() to inspect large dataframes.\n\n\n\n\nCode\n# Example variables\nx = 5\ny = \"Hello\"\nz = [1, 2, 3]\n\n# These will appear in the Variable Inspector"
  },
  {
    "objectID": "course-materials/cheatsheets/JupyterLab.html#essential-magic-commands",
    "href": "course-materials/cheatsheets/JupyterLab.html#essential-magic-commands",
    "title": "EDS 217 Cheatsheet",
    "section": "Essential Magic Commands",
    "text": "Essential Magic Commands\nMagic commands start with % (line magics) or %% (cell magics). Note that available magic commands may vary depending on your Jupyter environment and installed extensions.\n\nViewing Variables\n\n\nCode\n# List all variables\n%whos\n\n# List just variable names\n%who\n\n\nVariable     Type        Data/Info\n----------------------------------\nojs_define   function    &lt;function ojs_define at 0x1af33d6c0&gt;\nx            int         5\ny            str         Hello\nz            list        n=3\nojs_define   x   y   z   \n\n\n\n\nRunning Shell Commands\n\n\nCode\n# Run a shell command\n!echo \"Hello from the shell!\"\n\n# Capture output in a variable\nfiles = !ls\nprint(files)\n\n\nHello from the shell!\n['bar_plots.html', 'bar_plots.qmd', 'chart_customization.qmd', 'comprehensions.qmd', 'control_flows.qmd', 'data_aggregation.html', 'data_aggregation.qmd', 'data_cleaning.html', 'data_cleaning.qmd', 'data_grouping.qmd', 'data_merging.html', 'data_merging.qmd', 'data_selection.qmd', 'dictionaries.html', 'dictionaries.qmd', 'first_steps.qmd', 'functions.html', 'functions.qmd', 'JupyterLab.qmd', 'JupyterLab.quarto_ipynb', 'lists.html', 'lists.qmd', 'matplotlib.html', 'matplotlib.qmd', 'numpy.html', 'numpy.qmd', 'ocean_temperatures.csv', 'output.csv', 'Pandas_Cheat_Sheet.pdf', 'pandas_dataframes.qmd', 'pandas_series.qmd', 'print.html', 'print.qmd', 'random_numbers.qmd', 'read_csv.qmd', 'seaborn.qmd', 'sets.qmd', 'setting_up_python.html', 'setting_up_python.qmd', 'timeseries.html', 'timeseries.qmd', 'workflow_methods.html', 'workflow_methods.qmd']"
  },
  {
    "objectID": "course-materials/cheatsheets/JupyterLab.html#useful-keyboard-shortcuts",
    "href": "course-materials/cheatsheets/JupyterLab.html#useful-keyboard-shortcuts",
    "title": "EDS 217 Cheatsheet",
    "section": "Useful Keyboard Shortcuts",
    "text": "Useful Keyboard Shortcuts\n\n\n\n\n\n\n\n\nAction\nWindows/Linux\nMac\n\n\n\n\nRun cell\nShift + Enter\nShift + Enter\n\n\nRun cell and insert below\nAlt + Enter\nOption + Enter\n\n\nRun cell and select below\nCtrl + Enter\nCmd + Enter\n\n\nEnter command mode\nEsc\nEsc\n\n\nEnter edit mode\nEnter\nEnter\n\n\nSave notebook\nCtrl + S\nCmd + S\n\n\nInsert cell above\nA (in command mode)\nA (in command mode)\n\n\nInsert cell below\nB (in command mode)\nB (in command mode)\n\n\nCut cell\nX (in command mode)\nX (in command mode)\n\n\nCopy cell\nC (in command mode)\nC (in command mode)\n\n\nPaste cell\nV (in command mode)\nV (in command mode)\n\n\nUndo cell action\nZ (in command mode)\nZ (in command mode)\n\n\nChange to code cell\nY (in command mode)\nY (in command mode)\n\n\nChange to markdown cell\nM (in command mode)\nM (in command mode)\n\n\nSplit cell at cursor\nCtrl + Shift + -\nCmd + Shift + -\n\n\nMerge selected cells\nShift + M (in command mode)\nShift + M (in command mode)\n\n\nToggle line numbers\nShift + L (in command mode)\nShift + L (in command mode)\n\n\nToggle output\nO (in command mode)\nO (in command mode)"
  },
  {
    "objectID": "course-materials/cheatsheets/JupyterLab.html#tips-for-beginners",
    "href": "course-materials/cheatsheets/JupyterLab.html#tips-for-beginners",
    "title": "EDS 217 Cheatsheet",
    "section": "Tips for Beginners",
    "text": "Tips for Beginners\n\nUse Tab for code completion\nAdd ? after a function name for more detailed help (e.g., print?)\nUse dir() to see available attributes/methods (e.g., dir(str))\nUse the help() command to get information about functions and objects."
  },
  {
    "objectID": "course-materials/cheatsheets/JupyterLab.html#resources-for-further-learning",
    "href": "course-materials/cheatsheets/JupyterLab.html#resources-for-further-learning",
    "title": "EDS 217 Cheatsheet",
    "section": "Resources for Further Learning",
    "text": "Resources for Further Learning\n\nJupyterLab Documentation\nIPython Documentation (for magic commands)\nJupyter Notebook Cheatsheet\nDataCamp JupyterLab Tutorial"
  },
  {
    "objectID": "course-materials/lectures/05_Session_1A.html#what-is-a-groupby-object",
    "href": "course-materials/lectures/05_Session_1A.html#what-is-a-groupby-object",
    "title": "What the Python (WTP)?: GroupBy(), copy(), and The Triple Dilemma",
    "section": "What is a GroupBy Object?",
    "text": "What is a GroupBy Object?\n\n\nCreated when you use the groupby() function in pandas\nA plan for splitting data into groups, not the result itself\nLazy evaluation: computations occur only when an aggregation method is called\nContains:\n\nReference to the original DataFrame\nColumns to group by\nInternal dictionary mapping group keys to row indices"
  },
  {
    "objectID": "course-materials/lectures/05_Session_1A.html#structure-of-a-groupby-object",
    "href": "course-materials/lectures/05_Session_1A.html#structure-of-a-groupby-object",
    "title": "What the Python (WTP)?: GroupBy(), copy(), and The Triple Dilemma",
    "section": "Structure of a GroupBy Object",
    "text": "Structure of a GroupBy Object\n\n\nInternal dictionary structure:\n{\n  group_key_1: [row_index_1, row_index_3, ...],\n  group_key_2: [row_index_2, row_index_4, ...],\n  ...\n}\nThis structure allows for efficient data access and aggregation\nActual data isn’t copied or split until necessary"
  },
  {
    "objectID": "course-materials/lectures/05_Session_1A.html#groupby-example",
    "href": "course-materials/lectures/05_Session_1A.html#groupby-example",
    "title": "What the Python (WTP)?: GroupBy(), copy(), and The Triple Dilemma",
    "section": "GroupBy Example",
    "text": "GroupBy Example\n\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Category': ['A', 'B', 'A', 'B', 'A', 'B'],\n    'Value': [1, 2, 3, 4, 5, 6]\n})\n\ngrouped = df.groupby('Category')\n# No computation yet!\n\nresult = grouped.sum()  # Now we compute\nprint(result)\n\n          Value\nCategory       \nA             9\nB            12"
  },
  {
    "objectID": "course-materials/lectures/05_Session_1A.html#why-do-we-need-.copy",
    "href": "course-materials/lectures/05_Session_1A.html#why-do-we-need-.copy",
    "title": "What the Python (WTP)?: GroupBy(), copy(), and The Triple Dilemma",
    "section": "Why Do We Need .copy()?",
    "text": "Why Do We Need .copy()?\n\n\nMany pandas operations return views instead of copies\nViews are memory-efficient but can lead to unexpected modifications\n.copy() creates a new, independent object\nUse .copy() when you want to modify data without affecting the original"
  },
  {
    "objectID": "course-materials/lectures/05_Session_1A.html#views-vs.-copies-in-pandas",
    "href": "course-materials/lectures/05_Session_1A.html#views-vs.-copies-in-pandas",
    "title": "What the Python (WTP)?: GroupBy(), copy(), and The Triple Dilemma",
    "section": "Views vs. Copies in Pandas",
    "text": "Views vs. Copies in Pandas\n\n\nFiltering operations usually create views:\n\ndf[df['column'] &gt; value]\ndf.loc[condition]\n\nSome operations create copies by default:\n\ndf.drop(columns=['col'])\ndf.dropna()\ndf.reset_index()\n\nBut it’s not always clear which operations do what!"
  },
  {
    "objectID": "course-materials/lectures/05_Session_1A.html#when-to-use-.copy",
    "href": "course-materials/lectures/05_Session_1A.html#when-to-use-.copy",
    "title": "What the Python (WTP)?: GroupBy(), copy(), and The Triple Dilemma",
    "section": "When to Use .copy()",
    "text": "When to Use .copy()\n\n\nWhen assigning a slice of a DataFrame to a new variable\nBefore making changes to a DataFrame you want to keep separate\nIn functions where you don’t want to modify the input data\nWhen chaining operations and you’re unsure about view vs. copy behavior\nTo ensure you have an independent copy, regardless of the operation"
  },
  {
    "objectID": "course-materials/lectures/05_Session_1A.html#copy-example",
    "href": "course-materials/lectures/05_Session_1A.html#copy-example",
    "title": "What the Python (WTP)?: GroupBy(), copy(), and The Triple Dilemma",
    "section": ".copy() Example",
    "text": ".copy() Example\n\n# Filtering creates a view\ndf_view = df[df['Category'] == 'A']\ndf_view['Value'] += 10  # This modifies the original df!\n\n# Using copy() creates an independent DataFrame\ndf_copy = df[df['Category'] == 'A'].copy()\ndf_copy['Value'] += 10  # This doesn't affect the original df\n\nprint(\"Original df:\")\nprint(df)\nprint(\"\\nModified copy:\")\nprint(df_copy)\n\nOriginal df:\n  Category  Value\n0        A      1\n1        B      2\n2        A      3\n3        B      4\n4        A      5\n5        B      6\n\nModified copy:\n  Category  Value\n0        A     11\n2        A     13\n4        A     15"
  },
  {
    "objectID": "course-materials/lectures/05_Session_1A.html#the-triple-constraint-dilemma",
    "href": "course-materials/lectures/05_Session_1A.html#the-triple-constraint-dilemma",
    "title": "What the Python (WTP)?: GroupBy(), copy(), and The Triple Dilemma",
    "section": "The Triple Constraint Dilemma",
    "text": "The Triple Constraint Dilemma\n\n\nIn software design, often you can only optimize two out of three:\n\nPerformance\nFlexibility\nEase of Use\n\nThis applies to data science tools like R and Python"
  },
  {
    "objectID": "course-materials/lectures/05_Session_1A.html#r-vs-python-trade-offs",
    "href": "course-materials/lectures/05_Session_1A.html#r-vs-python-trade-offs",
    "title": "What the Python (WTP)?: GroupBy(), copy(), and The Triple Dilemma",
    "section": "R vs Python: Trade-offs",
    "text": "R vs Python: Trade-offs\n\n\nR\n\n✓✓ Ease of Use\n✓ General Flexibility\n✗ General Performance\n\n\nPython\n\n✓✓ Performance\n✓ General Flexibility\n✗ Ease of Use (for data tasks)"
  },
  {
    "objectID": "course-materials/lectures/05_Session_1A.html#r-strengths-and-limitations",
    "href": "course-materials/lectures/05_Session_1A.html#r-strengths-and-limitations",
    "title": "What the Python (WTP)?: GroupBy(), copy(), and The Triple Dilemma",
    "section": "R: Strengths and Limitations",
    "text": "R: Strengths and Limitations\n\n\nStrengths:\n\nIntuitive for statistical operations\nConsistent data manipulation with tidyverse\nExcellent for quick analyses and visualizations\n\nLimitations:\n\nCan be slower for very large datasets\nLess efficient memory usage (more frequent copying)\nLimited in general-purpose programming tasks"
  },
  {
    "objectID": "course-materials/lectures/05_Session_1A.html#python-strengths-and-limitations",
    "href": "course-materials/lectures/05_Session_1A.html#python-strengths-and-limitations",
    "title": "What the Python (WTP)?: GroupBy(), copy(), and The Triple Dilemma",
    "section": "Python: Strengths and Limitations",
    "text": "Python: Strengths and Limitations\n\n\nStrengths:\n\nEfficient for large-scale data processing\nVersatile for various programming tasks\nStrong in machine learning and deep learning\n\nLimitations:\n\nLess intuitive API for data manipulation (pandas)\nSteeper learning curve for data science tasks\nRequires more code for some statistical operations"
  },
  {
    "objectID": "course-materials/lectures/05_Session_1A.html#implications-for-data-science",
    "href": "course-materials/lectures/05_Session_1A.html#implications-for-data-science",
    "title": "What the Python (WTP)?: GroupBy(), copy(), and The Triple Dilemma",
    "section": "Implications for Data Science",
    "text": "Implications for Data Science\n\n\nR excels in statistical computing and quick analyses\nPython shines in large-scale data processing and diverse applications\nChoice depends on specific needs:\n\nProject scale\nPerformance requirements\nTeam expertise\nIntegration with other systems"
  },
  {
    "objectID": "course-materials/lectures/05_Session_1A.html#conclusion",
    "href": "course-materials/lectures/05_Session_1A.html#conclusion",
    "title": "What the Python (WTP)?: GroupBy(), copy(), and The Triple Dilemma",
    "section": "Conclusion",
    "text": "Conclusion\n\nUnderstanding these concepts helps in:\n\nChoosing the right tool for the job\nWriting efficient and correct code\nAppreciating the design decisions in data science tools\n\nBoth R and Python have their places in a data scientist’s toolkit\nConsider using both languages to leverage their respective strengths"
  },
  {
    "objectID": "course-materials/lectures/05_Session_1A.html#questions",
    "href": "course-materials/lectures/05_Session_1A.html#questions",
    "title": "What the Python (WTP)?: GroupBy(), copy(), and The Triple Dilemma",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "course-materials/interactive-sessions/1b_Jupyter_Notebooks.html",
    "href": "course-materials/interactive-sessions/1b_Jupyter_Notebooks.html",
    "title": "Interactive Session 1B",
    "section": "",
    "text": "Now that you’ve seen the REPL in iPython, from now on in this class you will code in Jupyter Notebooks. Jupyter is an incredibly awesome and user-friendly integrated development environment (IDE). An IDE provides a place for data scientists to see and work with a bunch of different aspects of their work in a nice, organized interface."
  },
  {
    "objectID": "course-materials/interactive-sessions/1b_Jupyter_Notebooks.html#meet-jupyterlab",
    "href": "course-materials/interactive-sessions/1b_Jupyter_Notebooks.html#meet-jupyterlab",
    "title": "Interactive Session 1B",
    "section": "1. Meet JupyterLab",
    "text": "1. Meet JupyterLab\nJupyterLab provides a nice user interface for data science, development, reporting, and collaboration (all of which you’ll learn about throughout the MEDS program) in one place.\n\nFeatures of JupyterLab as an IDE\n\nInteractive Computing: JupyterLab is designed primarily for interactive computing and data analysis. It supports live code execution, data visualization, and interactive widgets, which are key features of modern IDEs.\nMulti-language Support: While originally developed for Python, JupyterLab supports many other programming languages through the use of kernels, making it versatile for various programming tasks.\nRich Text Editing: It provides a rich text editor for creating and editing Jupyter Notebooks, which can contain both code and narrative text (Markdown), allowing for documentation and code to coexist.\nCode Execution: JupyterLab allows you to execute code cells and see the output immediately, making it suitable for testing and iterating on code quickly.\nFile Management: It includes a file manager for browsing and managing project files, similar to the file explorers found in traditional IDEs.\nExtensions and Customization: JupyterLab supports numerous extensions that can enhance its capabilities, such as version control integration, terminal access, and enhanced visualizations.\nIntegrated Tools: It has an integrated terminal, variable inspector, and other tools that are typically part of an IDE, providing a comprehensive environment for development.\n\n\n\nDifferences from Traditional IDEs\n\nFocus on Notebooks: Unlike many traditional IDEs that focus on scripting and full-scale software development, JupyterLab emphasizes the use of notebooks for exploratory data analysis and research.\nNon-linear Workflow: JupyterLab allows for a non-linear workflow, where users can execute cells out of order and iteratively modify and test code."
  },
  {
    "objectID": "course-materials/interactive-sessions/1b_Jupyter_Notebooks.html#jupyterlab-interface",
    "href": "course-materials/interactive-sessions/1b_Jupyter_Notebooks.html#jupyterlab-interface",
    "title": "Interactive Session 1B",
    "section": "JupyterLab Interface",
    "text": "JupyterLab Interface\n\nPrimary panes include the Main Work Area pane, Sidebar, and Menu Bar.\n\n\n\nAs you work, Jupyer Lab will add additional tabs/panes that contain figures and data inspectors, or even other file types. You can rearrange these panes to organize your workspace however you like.\n\nYou can check out the JupyterLab User Guide for tons of information and helpful tips!\nJupyterLab is a powerful interactive development environment (IDE) that allows you to work with Jupyter Notebooks, text editors, terminals, and other components in a single, integrated environment. It’s widely used in data science, scientific computing, and education."
  },
  {
    "objectID": "course-materials/interactive-sessions/1b_Jupyter_Notebooks.html#getting-started-with-jupyter-notebooks-in-jupyterlab",
    "href": "course-materials/interactive-sessions/1b_Jupyter_Notebooks.html#getting-started-with-jupyter-notebooks-in-jupyterlab",
    "title": "Interactive Session 1B",
    "section": "Getting Started with Jupyter Notebooks in JupyterLab",
    "text": "Getting Started with Jupyter Notebooks in JupyterLab\n\nCreating a New Notebook\n\nOpen JupyterLab: Once JupyterLab is running, you’ll see the JupyterLab interface with the file browser on the left.\nCreate a New Notebook:\n\nClick on the + button in the file browser to open a new Launcher tab.\nUnder the “Notebook” section, click “Python 3” to create a new Python notebook.\n\nRename the Notebook:\n\nClick on the notebook title (usually “Untitled”) at the top of the notebook interface.\nEnter a new name for your notebook and click “Rename”.\n\n\n\n\nUnderstanding the Notebook Interface\nThe Jupyter Notebook interface is divided into cells. There are two main types of cells:\n\nCode Cells: For writing and executing Python code.\nMarkdown Cells: For writing formatted text using Markdown syntax.\n\n\n\nWriting and Running Code\nLet’s start by writing some simple Python code in a code cell.\n\nAdd a Code Cell:\n\nClick inside the cell and start typing your Python code.\n\n\n\n# Simple Python code\nprint(\"Hello, Jupyter!\")\n\nRun the Code Cell:\n\nClick the “Run” button in the toolbar or press Shift + Enter to execute the code.\nThe output will be displayed directly below the cell.\n\n\n\n\nWriting Markdown\nMarkdown cells allow you to write formatted text. You can use Markdown to create headings, lists, links, and more.\n\nAdd a Markdown Cell:\n\nClick on the “+” button in the toolbar to add a new cell.\nChange the cell type to “Markdown” from the dropdown menu in the toolbar.\n\nWrite Markdown Text:\n\n# My First Markdown Cell\n\nThis is a simple example of a Markdown cell in JupyterLab.\n\n## Features of Markdown\n\n- **Bold Text**: Use `**text**` for **bold**.\n- **Italic Text**: Use `*text*` for *italic*.\n- **Lists**: Create bullet points using `-` or `*`.\n- **Links**: [JupyterLab Documentation](https://jupyterlab.readthedocs.io/)\n\nRender the Markdown:\n\nClick the “Run” button or press Shift + Enter to render the Markdown text.\n\n\n\n\nCombining Code and Markdown\nJupyter Notebooks are powerful because they allow you to combine code and markdown in a single document. This is useful for creating interactive tutorials, reports, and data analyses.\n\n\nRendering Images\nJupyter Notebooks can render images directly in the output cells, which is particularly useful for data visualization.\n\nExample: Displaying an Image\n\n\nCode\nfrom IPython.display import Image, display\n\n# Display an image\nimg_path = 'https://jupyterlab.readthedocs.io/en/stable/_images/interface-jupyterlab.png'\ndisplay(Image(url=img_path, width=700))\n\n\n\n\n\n\n\n\nInteractive Features\nJupyter Notebooks support interactive features, such as widgets, which enhance the interactivity of your notebooks.\n\nExample: Using Interactive Widgets\nWidgets allow users to interact with your code and visualize results dynamically.\n\n\nCode\nimport ipywidgets as widgets\n\n# Create a simple slider widget\nslider = widgets.IntSlider(value=50, min=0, max=100, step=1, description='Value:')\ndisplay(slider)\n\n\n\n\n\n\n\n\nSaving and Exporting Notebooks\n\nSave the Notebook:\n\nClick the save icon in the toolbar or press Ctrl + S (Cmd + S on macOS) to save your work.\n\nExport the Notebook:\n\nJupyterLab allows you to export notebooks to various formats, such as PDF or HTML. Go to File &gt; Export Notebook As and choose your desired format."
  },
  {
    "objectID": "course-materials/interactive-sessions/1b_Jupyter_Notebooks.html#tips-for-using-jupyterlab",
    "href": "course-materials/interactive-sessions/1b_Jupyter_Notebooks.html#tips-for-using-jupyterlab",
    "title": "Interactive Session 1B",
    "section": "Tips for Using JupyterLab",
    "text": "Tips for Using JupyterLab\n\nKeyboard Shortcuts: Familiarize yourself with keyboard shortcuts to speed up your workflow. You can view shortcuts by clicking Help &gt; Keyboard Shortcuts. You can also refer to our class Jupyter Keyboard Shortcuts Cheatsheet\nUsing the File Browser: Drag and drop files into the file browser to upload them to your workspace.\nUsing the Variable Inspector: The variable inspector shows variable names, types, values/shapes, and counts (for collections). Open the Variable Inspector using Menu: View &gt; Activate Command Palette, then type “variable inspector.” Or use the keyboard shortcut: Ctrl + Shift + I (Windows/Linux) or Cmd + Shift + I (Mac)"
  },
  {
    "objectID": "course-materials/interactive-sessions/1b_Jupyter_Notebooks.html#conclusion",
    "href": "course-materials/interactive-sessions/1b_Jupyter_Notebooks.html#conclusion",
    "title": "Interactive Session 1B",
    "section": "Conclusion",
    "text": "Conclusion\nJupyterLab is a versatile tool that makes it easy to combine code, text, and visualizations in a single document. By mastering the basic functionality of Jupyter Notebooks, you can create powerful and interactive documents that enhance your data analysis and scientific computing tasks.\nFeel free to experiment with the code and markdown examples provided in this guide to deepen your understanding of JupyterLab. Happy coding!"
  },
  {
    "objectID": "course-materials/interactive-sessions/1b_Jupyter_Notebooks.html#resources",
    "href": "course-materials/interactive-sessions/1b_Jupyter_Notebooks.html#resources",
    "title": "Interactive Session 1B",
    "section": "Resources",
    "text": "Resources\nWe will get to know Jupyter Notebooks very well during the rest of this course, but here are even more resources you can use to learn and revisit:\n\nJupyter Notebook Gallery\nThere are many, many examples of textbooks, academic articles, journalism, analyses, and reports written in Jupyter Notebooks. Here is a link to a curated gallery containing many such examples. It’s worth exploring some of these just to get a sense of the diversity of applications and opportunities available using python and jupyter in data science!\n\n\nTutorials and Shortcourses\n\n1. Jupyter Notebook Documentation\n\nWebsite: Jupyter Documentation\nDescription: The official documentation for Jupyter Notebooks provides a comprehensive guide to installing, using, and customizing notebooks. It includes tutorials, tips, and examples to help you get started.\n\n\n\n2. Project Jupyter: Beginner’s Guide\n\nWebsite: Project Jupyter\nDescription: This page offers an interactive “Try Jupyter” experience, allowing you to run Jupyter Notebooks in the browser without installing anything locally. It is a great way to explore the basics of Jupyter in a hands-on manner.\n\n\n\n3. YouTube Tutorial Series by Corey Schafer\n\nVideo Playlist: Jupyter Notebooks Tutorial - Corey Schafer\nDescription: This YouTube series provides an in-depth introduction to Jupyter Notebooks. Corey Schafer covers installation, basic usage, and advanced features, making it easy to follow along and practice on your own.\n\n\n\n4. Jupyter Notebooks Beginner Guide - DataCamp\n\nWebsite: DataCamp Jupyter Notebook Tutorial\nDescription: This tutorial on DataCamp’s community blog offers a step-by-step guide to using Jupyter Notebooks for data science. It covers the basics and explores more advanced topics such as widgets and extensions.\n\n\n\n5. Real Python: Jupyter Notebook 101\n\nArticle: Jupyter Notebook 101\nDescription: This Real Python article introduces Jupyter Notebooks, covering installation, basic usage, and tips for using notebooks effectively. It is an excellent resource for Python developers who are new to Jupyter.\n\n\n\n6. Google Colab\n\nWebsite: Google Colab\nDescription: Google Colab is a free platform that lets you run Jupyter Notebooks in the cloud. You can find many tutorials and example notebooks on their site. For example, here is a link to a notebook they’ve created that includes many pandas snippets.\n\n\nEnd interactive session 1B"
  },
  {
    "objectID": "course-materials/final_project.html",
    "href": "course-materials/final_project.html",
    "title": "Final Activity",
    "section": "",
    "text": "In this final class activity, you will work in small groups (2-3) to develop a example data science workflow.\n\nImport Data\nExplore Data\nClean Data\nFilter Data\nSort Data\nTransform Data\nGroup Data\nAggregate Data\nVisualize Data"
  },
  {
    "objectID": "course-materials/final_project.html#diy-python-data-scienceworkflow",
    "href": "course-materials/final_project.html#diy-python-data-scienceworkflow",
    "title": "Final Activity",
    "section": "",
    "text": "In this final class activity, you will work in small groups (2-3) to develop a example data science workflow.\n\nImport Data\nExplore Data\nClean Data\nFilter Data\nSort Data\nTransform Data\nGroup Data\nAggregate Data\nVisualize Data"
  },
  {
    "objectID": "course-materials/final_project.html#what-to-do",
    "href": "course-materials/final_project.html#what-to-do",
    "title": "Final Activity",
    "section": "What to do",
    "text": "What to do\nTo conduct this exercise, you should find a suitable dataset; it doesn’t need to be environmental data per se - be creative in your search! You should also focus on making a number of exploratory and analysis visualizations using seaborn. You should avoid planning any analysis that absolutely require mapping and focus on using only pandas, numpy, matplotlib, and seaborn libraries.\nYour final product will be a self-contained notebook that is well-documented with markdown and code comments that you will walk through as a presentation to the rest of the class on the final day.\nYour notebook should include each of the nine steps, even if you don’t need to do much in each of them.\n\n\n\n\n\n\nNote\n\n\n\nYou can include visualizations as part of your data exploration (step 2), or anywhere else it is helpful.\n\n\nAdditional figures and graphics are also welcome - you are encouraged to make your notebooks as engaging and visually interesting as possible."
  },
  {
    "objectID": "course-materials/final_project.html#syncing-your-data-to-github",
    "href": "course-materials/final_project.html#syncing-your-data-to-github",
    "title": "Final Activity",
    "section": "Syncing your data to Github",
    "text": "Syncing your data to Github\nHere are some directions for syncing your classwork with GitHub\n\nGeneral places to find fun data\nHere are some links to potential data resources that you can use to develop your analyses:\n\nKaggle\nData is Plural\nUS Data.gov\nZenodo\nR for Data Science\n\n\n\nOddly specific datasets\n\nCentral Park Squirrel Survey\nHarry Potter Characters Dataset\nSpotify Tracks\nLego Dataset"
  },
  {
    "objectID": "course-materials/final_project.html#using-google-drive-to-store-your-.csv-file.",
    "href": "course-materials/final_project.html#using-google-drive-to-store-your-.csv-file.",
    "title": "Final Activity",
    "section": "Using Google Drive to store your .csv file.",
    "text": "Using Google Drive to store your .csv file.\nOnce you’ve found a .csv file that you want to use, you should:\n\nSave your file to a google drive folder in your UCSB account.\nChange the sharing settings to allow anyone with a link to view your file.\nOpen the sharing dialog and copy the sharing link to your clipboard.\nUse the code below to download your file (you will need to add this code to the top of your notebook in the Import Data section)\n\n\n\n\n\n\n\nWarning\n\n\n\nFor this code to work on the workbench server, you will need to switch your kernel from 3.11.0 to 3.7.13. You can switch kernels by clicking on the kernel name in the upper right of your notebook.\n\n\n\n\nCode\nimport pandas as pd\nimport requests\n\ndef extract_file_id(url):\n    \"\"\"Extract file id from Google Drive Sharing URL.\"\"\"\n    return url.split(\"/\")[-2]\n\ndef df_from_gdrive_csv(url):\n    \"\"\" Get the CSV file from a Google Drive Sharing URL.\"\"\"\n    file_id = extract_file_id(url)\n    URL = \"https://docs.google.com/uc?export=download\"\n    session = requests.Session()\n    response = session.get(URL, params={\"id\": file_id}, stream=True)\n    return pd.read_csv(response.raw)\n\n# Example of how to use:\n# Note: your sharing link will be different, but should look like this:\nsharing_url = \"https://drive.google.com/file/d/1RlilHNG7BtvXT2Pm4OpgNvEjVJJZNaps/view?usp=share_link\"\ndf = df_from_gdrive_csv(sharing_url)\ndf.head()\n\n\n\n\n\n\n\n\n\ndate\nlocation\ntemperature\nsalinity\ndepth\n\n\n\n\n0\n2020-01-01\nPacific\n21.523585\nNaN\n200\n\n\n1\n2020-01-02\nPacific\n14.800079\n34.467264\n100\n\n\n2\n2020-01-03\nPacific\n23.752256\n35.016505\n100\n\n\n3\n2020-01-04\nPacific\n24.702824\n36.416944\n200\n\n\n4\n2020-01-05\nPacific\n10.244824\n35.807487\n1000"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Python for Environmental Data Science",
    "section": "Course Description",
    "text": "Course Description\nProgramming skills are critical when working with, understanding, analyzing, and gleaning insights from environmental data. In the intensive EDS 217 course, students will develop fundamental skills in Python programming, data manipulation, and data visualization, specifically tailored for environmental data science applications.\nThe goal of EDS 217 (Python for Environmental Data Science) is to equip incoming MEDS students with the programming methods, skills, notation, and language commonly used in the python data science stack, which will be essential for their python-based data science courses and projects in the program as well as in their data science careers. By the end of the course, students should be able to:\n\nManipulate and analyze data using libraries like pandas and NumPy\nVisualize data using Matplotlib and Seaborn\nWrite, interpret, and debug Python scripts\nImplement basic algorithms for data processing\nUtilize logical operations, control flow, and functions in programming\nCollaborate with peers to solve group programming tasks, and communicate the process and results to the rest of the class"
  },
  {
    "objectID": "index.html#syncing-your-classwork-to-github",
    "href": "index.html#syncing-your-classwork-to-github",
    "title": "Python for Environmental Data Science",
    "section": "Syncing your classwork to Github",
    "text": "Syncing your classwork to Github\nHere are some directions for syncing your classwork with a GitHub repository"
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Python for Environmental Data Science",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\n\nInstructor\n\n\n\n\n\n\n\nKelly Caylor\nEmail: caylor@ucsb.edu\nLearn more: Bren profile\n\n\n\n\nTA\n\n\n\n\n\n\n\nAnna Boser\nEmail: anaboser@ucsb.edu\nLearn more: Bren profile"
  },
  {
    "objectID": "course-materials/live-coding/4b_exploring_dataframes.html#overview",
    "href": "course-materials/live-coding/4b_exploring_dataframes.html#overview",
    "title": "Live Coding Session 4B",
    "section": "Overview",
    "text": "Overview\nIn this 45-minute session, we will explore the basics of pandas DataFrames - a fundamental data structure for data manipulation and analysis in Python. We’ll focus on essential operations that form the foundation of working with DataFrames."
  },
  {
    "objectID": "course-materials/live-coding/4b_exploring_dataframes.html#objectives",
    "href": "course-materials/live-coding/4b_exploring_dataframes.html#objectives",
    "title": "Live Coding Session 4B",
    "section": "Objectives",
    "text": "Objectives\n\nUnderstand the structure and basic properties of pandas DataFrames.\nLearn how to create and load DataFrames.\nApply methods for data selection and filtering.\nPerform basic data manipulation and analysis using DataFrames."
  },
  {
    "objectID": "course-materials/live-coding/4b_exploring_dataframes.html#getting-started-5-minutes",
    "href": "course-materials/live-coding/4b_exploring_dataframes.html#getting-started-5-minutes",
    "title": "Live Coding Session 4B",
    "section": "Getting Started (5 minutes)",
    "text": "Getting Started (5 minutes)\n\nPrepare Your Environment:\n\nOpen JupyterLab and create a new notebook named “pandas_dataframes_intro”.\nDownload the sample dataset from here.\n\nParticipation:\n\nCode along with me during the session.\nAsk questions as we go - if you’re wondering about something, others probably are too!"
  },
  {
    "objectID": "course-materials/live-coding/4b_exploring_dataframes.html#session-outline",
    "href": "course-materials/live-coding/4b_exploring_dataframes.html#session-outline",
    "title": "Live Coding Session 4B",
    "section": "Session Outline",
    "text": "Session Outline\n\n1. Introduction to pandas DataFrames (5 minutes)\n\nWhat are DataFrames?\nImporting pandas and creating a simple DataFrame\n\n\n\n2. Loading and Exploring Data (10 minutes)\n\nReading a CSV file into a DataFrame\nBasic DataFrame attributes and methods (shape, info, describe, head)\n\n\n\n3. Data Selection and Filtering (10 minutes)\n\nSelecting columns and rows\nBoolean indexing\n\n\n\n4. Basic Data Manipulation (10 minutes)\n\nAdding and removing columns\nHandling missing data\n\n\n\n5. Q&A and Wrap-up (5 minutes)\n\nAddress any questions\nRecap key points"
  },
  {
    "objectID": "course-materials/live-coding/4b_exploring_dataframes.html#code-examples-well-cover",
    "href": "course-materials/live-coding/4b_exploring_dataframes.html#code-examples-well-cover",
    "title": "Live Coding Session 4B",
    "section": "Code Examples We’ll Cover",
    "text": "Code Examples We’ll Cover\nimport pandas as pd\n\n# Creating a DataFrame\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'c']})\n\n# Loading data from CSV\ndf = pd.read_csv('sample_data.csv')\n\n# Basic exploration\nprint(df.shape)\ndf.info()\nprint(df.describe())\n\n# Selection and filtering\nselected_columns = df[['column1', 'column2']]\nfiltered_rows = df[df['column1'] &gt; 5]\n\n# Data manipulation\ndf['new_column'] = df['column1'] * 2\ndf.dropna(inplace=True)"
  },
  {
    "objectID": "course-materials/live-coding/4b_exploring_dataframes.html#after-the-session",
    "href": "course-materials/live-coding/4b_exploring_dataframes.html#after-the-session",
    "title": "Live Coding Session 4B",
    "section": "After the Session",
    "text": "After the Session\n\nReview your notes and try to replicate the exercises on your own.\nExperiment with the code using your own datasets.\nCheck out our class DataFrame cheatsheet for quick reference.\nFor more advanced features, explore the official pandas documentation."
  },
  {
    "objectID": "course-materials/live-coding/2d_list_comprehensions.html",
    "href": "course-materials/live-coding/2d_list_comprehensions.html",
    "title": "Live Coding Session 2D",
    "section": "",
    "text": "In this session, we will be exploring List and Dictionary comprehensions together. Live coding is a great way to learn programming as it allows you to see the process of writing code in real-time, including how to deal with unexpected issues and debug errors."
  },
  {
    "objectID": "course-materials/live-coding/2d_list_comprehensions.html#overview",
    "href": "course-materials/live-coding/2d_list_comprehensions.html#overview",
    "title": "Live Coding Session 2D",
    "section": "",
    "text": "In this session, we will be exploring List and Dictionary comprehensions together. Live coding is a great way to learn programming as it allows you to see the process of writing code in real-time, including how to deal with unexpected issues and debug errors."
  },
  {
    "objectID": "course-materials/live-coding/2d_list_comprehensions.html#objectives",
    "href": "course-materials/live-coding/2d_list_comprehensions.html#objectives",
    "title": "Live Coding Session 2D",
    "section": "Objectives",
    "text": "Objectives\n\nUnderstand the fundamentals of comprehensions in Python.\nApply comprehensions in practical examples.\nDevelop the ability to troubleshoot and debug in a live setting."
  },
  {
    "objectID": "course-materials/live-coding/2d_list_comprehensions.html#overview-1",
    "href": "course-materials/live-coding/2d_list_comprehensions.html#overview-1",
    "title": "Live Coding Session 2D",
    "section": "Overview",
    "text": "Overview\nThis session introduces list and dictionary comprehensions, providing a comparison to traditional control flow methods. The goal is to help students understand the advantages of using comprehensions in Python and to practice writing their own.\nThe session is designed to be completed in 45 minutes, including setting up the notebook."
  },
  {
    "objectID": "course-materials/live-coding/2d_list_comprehensions.html#setting-up-your-notebook-5-minutes",
    "href": "course-materials/live-coding/2d_list_comprehensions.html#setting-up-your-notebook-5-minutes",
    "title": "Live Coding Session 2D",
    "section": "1. Setting Up Your Notebook (5 minutes)",
    "text": "1. Setting Up Your Notebook (5 minutes)\nGoal: Start by having students set up their Jupyter notebook with markdown headers. This helps organize the session into distinct sections, making it easier for them to follow along and refer back to their work later.\n\nInstructions:\n\nCreate a new Jupyter notebook or open an existing one for this session.\nAdd markdown cells with the following headers, using ## for each header.\nPlace code cells between the headers where you’ll write and execute your code.\n\n\n\nHeader Texts:\n\nFirst markdown cell:\n## Review: Traditional Control Flow Approaches\nSecond markdown cell:\n## Introduction to List Comprehensions\nThird markdown cell:\n## Introduction to Dictionary Comprehensions\nFourth markdown cell:\n## Using Conditional Logic in Comprehensions\nFifth markdown cell:\n## Summary and Best Practices\nSixth markdown cell:\n## Reflections"
  },
  {
    "objectID": "course-materials/live-coding/2d_list_comprehensions.html#session-format",
    "href": "course-materials/live-coding/2d_list_comprehensions.html#session-format",
    "title": "Live Coding Session 2D",
    "section": "Session Format",
    "text": "Session Format\n\nIntroduction\n\nBrief discussion about the topic and its importance in data science.\n\n\n\nDemonstration\n\nI will demonstrate code examples live. Follow along and write the code into your own Jupyter notebook.\n\n\n\nPractice\n\nYou will have the opportunity to try exercises on your own to apply what you’ve learned.\n\n\n\nQ&A\n\nWe will have a Q&A session at the end where you can ask specific questions about the code, concepts, or issues encountered during the session."
  },
  {
    "objectID": "course-materials/live-coding/2d_list_comprehensions.html#after-the-session",
    "href": "course-materials/live-coding/2d_list_comprehensions.html#after-the-session",
    "title": "Live Coding Session 2D",
    "section": "After the Session",
    "text": "After the Session\n\nReview your notes and try to replicate the exercises on your own.\nExperiment with the code by modifying parameters or adding new features to deepen your understanding.\nCheck out our class comprehensions cheatsheet."
  },
  {
    "objectID": "course-materials/interactive-sessions/interactive-session-git.html",
    "href": "course-materials/interactive-sessions/interactive-session-git.html",
    "title": "Sidebar",
    "section": "",
    "text": "Welcome to git (xkcd)"
  },
  {
    "objectID": "course-materials/interactive-sessions/interactive-session-git.html#setting-up-git-for-collaborating-with-notebooks",
    "href": "course-materials/interactive-sessions/interactive-session-git.html#setting-up-git-for-collaborating-with-notebooks",
    "title": "Sidebar",
    "section": "1. Setting up git for collaborating with Notebooks",
    "text": "1. Setting up git for collaborating with Notebooks\n\nCleaning Up Jupyter Notebook Files Before Committing to Git\nIn data science workflows, particularly when collaborating using Jupyter Notebooks, it’s important to maintain a clean and efficient Git repository. This guide will help you set up your environment to automatically remove outputs from .ipynb files before committing them, which improves collaboration and reduces repository size.\n\n\nWhy Clean Up .ipynb Files?\n\nReduced Size: Outputs can bloat file sizes, making repositories larger and slower to clone.\nFewer Conflicts: Output cells can cause merge conflicts when multiple people edit the same file.\nEncouraged Reproducibility: Keeping notebooks free of outputs encourages others to run the notebooks themselves.\n\n\n\nStep-by-Step Setup\n\nStep 1: Check if jq is Installed\n\nOpen Terminal: Access your terminal application.\nCheck for jq: Run the following command to see if jq is installed and check its version:\njq --version\nVerify Version: Ensure the output is jq-1.5 or higher. If jq is installed and the version is at least 1.5, you can proceed to the next steps. If not, see the installation note below.\n\n\n\nStep 2: Configure Git to Use a Global Attributes File\n\nOpen ~/.gitconfig: Use nano to edit this file:\nnano ~/.gitconfig\nAdd the Configuration: Copy and paste the following lines:\n[core]\nattributesfile = ~/.gitattributes_global\n\n[filter \"nbstrip_full\"]\nclean = \"jq --indent 1 \\\n        '(.cells[] | select(has(\\\"outputs\\\")) | .outputs) = []  \\\n        | (.cells[] | select(has(\\\"execution_count\\\")) | .execution_count) = null  \\\n        | .metadata = {\\\"language_info\\\": {\\\"name\\\": \\\"python\\\", \\\"pygments_lexer\\\": \\\"ipython3\\\"}} \\\n        | .cells[].metadata = {} \\\n        '\"\nsmudge = cat\nrequired = true\nSave and Exit: Press CTRL + X, then Y, and Enter to save the file.\n\n\n\nStep 3: Create a Global Git Attributes File\n\nOpen ~/.gitattributes_global: Use nano to edit this file:\nnano ~/.gitattributes_global\nAdd the Following Line:\n*.ipynb filter=nbstrip_full\nSave and Exit: Press CTRL + X, then Y, and Enter.\n\n\n\n\nHow This Works\n\nfilter \"nbstrip_full\": This filter uses the jq command to strip outputs and reset execution counts in .ipynb files.\nclean: Removes outputs when files are staged for commit.\nsmudge: Ensures the original content is restored upon checkout.\nrequired: Enforces the use of the filter for the specified files.\n\n\n\nBenefits for Python Environmental Data Science Workflows\n\nEfficiency: Smaller files mean faster repository operations.\nCollaboration: Fewer conflicts facilitate teamwork.\nReproducibility: Encourages consistent execution across environments."
  },
  {
    "objectID": "course-materials/interactive-sessions/interactive-session-git.html#optional-installing-jq",
    "href": "course-materials/interactive-sessions/interactive-session-git.html#optional-installing-jq",
    "title": "Sidebar",
    "section": "Optional: Installing jq",
    "text": "Optional: Installing jq\nIf jq is not installed or needs to be updated, follow these instructions for your operating system.\n\nWindows\n\nDownload jq:\n\nVisit the jq downloads page and download the Windows executable (jq-win64.exe).\n\nAdd to PATH:\n\nMove the jq-win64.exe to a directory included in your system’s PATH or rename it to jq.exe and place it in C:\\Windows\\System32.\n\nVerify Installation:\n\nOpen Command Prompt and run jq --version to ensure it’s correctly installed.\n\n\n\n\nmacOS\n\nUsing Homebrew:\n\nHomebrew is a package manager for macOS that simplifies the installation of software. It’s widely used for installing command-line tools and other utilities. If you don’t have Homebrew installed, you can follow the instructions on the Homebrew website.\nOnce Homebrew is installed, open Terminal and run the following command to install jq:\nbrew install jq\n\nVerify Installation:\n\nRun jq --version to confirm it is installed and at least version 1.5.\n\n\nBy following these steps, you ensure that your Jupyter Notebook files remain clean and efficient within your Git repositories, enhancing collaboration and reproducibility in your workflows.\n\n\n\nAdditional Note\nFor Linux users, you can typically install jq using your package manager, such as apt on Debian-based systems or yum on Red Hat-based systems:\n# Debian-based systems\nsudo apt-get install jq\n\n# Red Hat-based systems\nsudo yum install jq\n\n\nEnd interactive session 2A"
  },
  {
    "objectID": "course-materials/day9.html#class-materials",
    "href": "course-materials/day9.html#class-materials",
    "title": "Building a Python Data Science Workflow",
    "section": "Class materials",
    "text": "Class materials\n\n\n\n\n\n\n\n\n Session\n Session 1\n Session 2\n\n\n\n\nday 9 / morning\nWorking on Final Data Science Project  (all morning)\n\n\n\nday 9 / afternoon\nData Science Project Presentations  (all afternoon)"
  },
  {
    "objectID": "course-materials/day9.html#syncing-your-classwork-to-github",
    "href": "course-materials/day9.html#syncing-your-classwork-to-github",
    "title": "Building a Python Data Science Workflow",
    "section": "Syncing your classwork to Github",
    "text": "Syncing your classwork to Github\nHere are some directions for syncing your classwork with a GitHub repository"
  },
  {
    "objectID": "course-materials/day9.html#end-of-day-practice",
    "href": "course-materials/day9.html#end-of-day-practice",
    "title": "Building a Python Data Science Workflow",
    "section": "End-of-day practice",
    "text": "End-of-day practice\nEnd of Class! Congratulations!!"
  },
  {
    "objectID": "course-materials/day9.html#additional-resources",
    "href": "course-materials/day9.html#additional-resources",
    "title": "Building a Python Data Science Workflow",
    "section": "Additional Resources",
    "text": "Additional Resources"
  },
  {
    "objectID": "course-materials/day7.html#class-materials",
    "href": "course-materials/day7.html#class-materials",
    "title": "Data Visualization",
    "section": "Class materials",
    "text": "Class materials\n\n\n\n\n\n\n\n\n Session\n Session 1\n Session 2\n\n\n\n\nday 7 / morning\n📊 Data visualization (Part I)\n📊 Data visualization (Part II)\n\n\nday 7 / afternoon\n🙌 Coding Colab: Exploring data through visualizations"
  },
  {
    "objectID": "course-materials/day7.html#end-of-day-practice",
    "href": "course-materials/day7.html#end-of-day-practice",
    "title": "Data Visualization",
    "section": "End-of-day practice",
    "text": "End-of-day practice\nComplete the following tasks / activities before heading home for the day!\n\n Day 7 Practice: 🌲 USDA Plant Hardiness Zones 🌴"
  },
  {
    "objectID": "course-materials/day7.html#additional-resources",
    "href": "course-materials/day7.html#additional-resources",
    "title": "Data Visualization",
    "section": "Additional Resources",
    "text": "Additional Resources"
  },
  {
    "objectID": "course-materials/day5.html#class-materials",
    "href": "course-materials/day5.html#class-materials",
    "title": "Selecting, Filtering, and Transforming Data in Pandas",
    "section": "Class materials",
    "text": "Class materials\n\n\n\n\n\n\n\n\n Session\n Session 1\n Session 2\n\n\n\n\nday 5 / morning\n📝 Selecting and Filtering in Pandas\n🐼 Cleaning Data\n\n\nday 5 / afternoon\n🙌 Coding Colab: Cleaning DataFrames\nEnd-of-day practice"
  },
  {
    "objectID": "course-materials/day5.html#end-of-day-practice",
    "href": "course-materials/day5.html#end-of-day-practice",
    "title": "Selecting, Filtering, and Transforming Data in Pandas",
    "section": "End-of-day practice",
    "text": "End-of-day practice\nComplete the following tasks / activities before heading home for the day!\n\n Day 5 Practice: 🍌 Analyzing the “Banana Index” 🍌"
  },
  {
    "objectID": "course-materials/day5.html#additional-resources",
    "href": "course-materials/day5.html#additional-resources",
    "title": "Selecting, Filtering, and Transforming Data in Pandas",
    "section": "Additional Resources",
    "text": "Additional Resources"
  },
  {
    "objectID": "course-materials/day3.html#class-materials",
    "href": "course-materials/day3.html#class-materials",
    "title": "Control and Comprehensions",
    "section": "Class materials",
    "text": "Class materials\n\n\n\n\n\n\n\n\n Session\n Session 1\n Session 2\n\n\n\n\nday 3 / morning\n📝 Control Flows\n🙌 Coding Colab: Control flows and data science\n\n\nday 3 / afternoon\n🐼 Intro to Arrays and Series\n🙌 Coding Colab: Working with Pandas Series"
  },
  {
    "objectID": "course-materials/day3.html#end-of-day-practice",
    "href": "course-materials/day3.html#end-of-day-practice",
    "title": "Control and Comprehensions",
    "section": "End-of-day practice",
    "text": "End-of-day practice\nComplete the following tasks / activities before heading home for the day!\n\n Day 3 Practice: Using Pandas Series for Data Analysis"
  },
  {
    "objectID": "course-materials/day3.html#additional-resources",
    "href": "course-materials/day3.html#additional-resources",
    "title": "Control and Comprehensions",
    "section": "Additional Resources",
    "text": "Additional Resources"
  },
  {
    "objectID": "course-materials/day1.html#class-materials",
    "href": "course-materials/day1.html#class-materials",
    "title": "Intro to Python and JupyterLab",
    "section": "Class materials",
    "text": "Class materials\n\n\n\n\n\n\n\n\n Session\n Session 1\n Session 2\n\n\n\n\nday 1 / morning\nPython and Data Science\n⚒️ Meet JupyterLab  ⚒️ Coding in Jupyter Notebooks\n\n\n\n\n\n\n\nday 1 / afternoon\n🐍 Exploring Data Types and Methods\n🐍 Variables & Operators"
  },
  {
    "objectID": "course-materials/day1.html#end-of-day-practice",
    "href": "course-materials/day1.html#end-of-day-practice",
    "title": "Intro to Python and JupyterLab",
    "section": "End-of-day practice",
    "text": "End-of-day practice\nOur last task today is to work through an example of a Python data science workflow. This exercise “skips ahead” to content we will be learning later in the course, but provides a preview of where we are headed and how easy it is to do data science in python!\n\n Day 1 Practice: Example Python Data Science Workflow"
  },
  {
    "objectID": "course-materials/day1.html#additional-resources",
    "href": "course-materials/day1.html#additional-resources",
    "title": "Intro to Python and JupyterLab",
    "section": "Additional Resources",
    "text": "Additional Resources\nNA"
  },
  {
    "objectID": "course-materials/cheatsheets/timeseries.html",
    "href": "course-materials/cheatsheets/timeseries.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "To be added"
  },
  {
    "objectID": "course-materials/cheatsheets/print.html",
    "href": "course-materials/cheatsheets/print.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "The print() function outputs the specified message to the screen. It is often used for debugging to display the values of variables and program status during code execution.\n\n\nprint(\"Hello, World!\")\n\n\n\nx = 10\ny = 20\nprint(x)\nprint(y)"
  },
  {
    "objectID": "course-materials/cheatsheets/print.html#basic-usage-of-print",
    "href": "course-materials/cheatsheets/print.html#basic-usage-of-print",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "The print() function outputs the specified message to the screen. It is often used for debugging to display the values of variables and program status during code execution.\n\n\nprint(\"Hello, World!\")\n\n\n\nx = 10\ny = 20\nprint(x)\nprint(y)"
  },
  {
    "objectID": "course-materials/cheatsheets/print.html#combining-text-and-variables",
    "href": "course-materials/cheatsheets/print.html#combining-text-and-variables",
    "title": "EDS 217 Cheatsheet",
    "section": "Combining Text and Variables",
    "text": "Combining Text and Variables\nYou can combine text and variables in the print() function to make the output more informative. Here are 4 different ways:\n\nThis is a note.\n\n\nMake sure to check your data types before processing.\n\n\n1. Using Comma Separation\nname = \"Alice\"\nage = 30\nprint(\"Name:\", name, \"Age:\", age)\n\n\n2. Using String Formatting\n\nf-string (Formatted String Literal) - Python 3.6+ [PREFERRED]\nname = \"Bob\"\nage = 25\nprint(f\"Name: {name}, Age: {age}\")\n\n\nformat() Method [CAN BE USEFUL IN COMPLICATED PRINT STATEMENTS]\nname = \"Carol\"\nage = 22\nprint(\"Name: {}, Age: {}\".format(name, age))\n\n\nOld % formatting [NOT RECOMMENDED]\nname = \"Dave\"\nage = 28\nprint(\"Name: %s, Age: %d\" % (name, age))"
  },
  {
    "objectID": "course-materials/cheatsheets/print.html#debugging-with-print",
    "href": "course-materials/cheatsheets/print.html#debugging-with-print",
    "title": "EDS 217 Cheatsheet",
    "section": "Debugging with Print",
    "text": "Debugging with Print\nUse print() to display intermediate values in your code to understand how data changes step by step.\n\nExample: Debugging a Loop\nfor i in range(5):\n    print(f\"Current value of i: {i}\")\n\n\nExample: Checking Function Outputs\ndef add(a, b):\n    result = a + b\n    print(f\"Adding {a} + {b} = {result}\")\n    return result\n\nadd(5, 3)"
  },
  {
    "objectID": "course-materials/cheatsheets/matplotlib.html",
    "href": "course-materials/cheatsheets/matplotlib.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Tip\n\n\n\nFor 🤯 inspiration + 👩‍💻 code, check out the Python Graph Gallery"
  },
  {
    "objectID": "course-materials/cheatsheets/matplotlib.html#basic-setup",
    "href": "course-materials/cheatsheets/matplotlib.html#basic-setup",
    "title": "EDS 217 Cheatsheet",
    "section": "Basic Setup",
    "text": "Basic Setup\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Set style for seaborn (optional)\nsns.set_style(\"whitegrid\")"
  },
  {
    "objectID": "course-materials/cheatsheets/matplotlib.html#creating-a-figure-and-axes",
    "href": "course-materials/cheatsheets/matplotlib.html#creating-a-figure-and-axes",
    "title": "EDS 217 Cheatsheet",
    "section": "Creating a Figure and Axes",
    "text": "Creating a Figure and Axes\n# Create a new figure and axis\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Create multiple subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))"
  },
  {
    "objectID": "course-materials/cheatsheets/matplotlib.html#common-plot-types",
    "href": "course-materials/cheatsheets/matplotlib.html#common-plot-types",
    "title": "EDS 217 Cheatsheet",
    "section": "Common Plot Types",
    "text": "Common Plot Types\n\nLine Plot\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\nax.plot(x, y, label='sin(x)')\n\n\nScatter Plot\nx = np.random.rand(50)\ny = np.random.rand(50)\nplt.scatter(x, y, alpha=0.5)\n\n\nBar Plot\ncategories = ['A', 'B', 'C', 'D']\nvalues = [3, 7, 2, 5]\nplt.bar(categories, values)\n\n\nHistogram\ndata = np.random.randn(1000)\nax.hist(data, bins=30, edgecolor='black')"
  },
  {
    "objectID": "course-materials/cheatsheets/matplotlib.html#customizing-plots",
    "href": "course-materials/cheatsheets/matplotlib.html#customizing-plots",
    "title": "EDS 217 Cheatsheet",
    "section": "Customizing Plots",
    "text": "Customizing Plots\n\nLabels and Title\nax.set_xlabel('X-axis label')\nax.set_ylabel('Y-axis label')\nax.set_title('Plot Title')\n\n\nLegend\nax.legend()\n\n\nAxis Limits\nax.set_xlim(0, 10)\nax.set_ylim(-1, 1)\n\n\nGrid\nax.grid(True, linestyle='--', alpha=0.7)\n\n\nTicks\nax.set_xticks([0, 2, 4, 6, 8, 10])\nax.set_yticks([-1, -0.5, 0, 0.5, 1])"
  },
  {
    "objectID": "course-materials/cheatsheets/matplotlib.html#color-and-style",
    "href": "course-materials/cheatsheets/matplotlib.html#color-and-style",
    "title": "EDS 217 Cheatsheet",
    "section": "Color and Style",
    "text": "Color and Style\n\nChanging Colors\nax.plot(x, y, color='r')  # 'r' for red\nax.scatter(x, y, c='blue')\n\n\nLine Styles\nax.plot(x, y, linestyle='--')  # dashed line\nax.plot(x, y, ls=':')  # dotted line\n\n\nMarker Styles\nax.plot(x, y, marker='o')  # circles\nax.plot(x, y, marker='s')  # squares"
  },
  {
    "objectID": "course-materials/cheatsheets/matplotlib.html#saving-and-displaying",
    "href": "course-materials/cheatsheets/matplotlib.html#saving-and-displaying",
    "title": "EDS 217 Cheatsheet",
    "section": "Saving and Displaying",
    "text": "Saving and Displaying\n\nSaving the Figure\nplt.savefig('my_plot.png', dpi=300, bbox_inches='tight')\n\n\nDisplaying the Plot\nplt.show()"
  },
  {
    "objectID": "course-materials/cheatsheets/matplotlib.html#useful-tips-for-seaborn-users",
    "href": "course-materials/cheatsheets/matplotlib.html#useful-tips-for-seaborn-users",
    "title": "EDS 217 Cheatsheet",
    "section": "Useful Tips for Seaborn Users",
    "text": "Useful Tips for Seaborn Users\n\nUse plt.subplots() to create custom layouts that Seaborn doesn’t provide.\nAccess the underlying Matplotlib Axes object in Seaborn plots:\ng = sns.scatterplot(x='x', y='y', data=df)\ng.set_xlabel('Custom X Label')\nCombine Seaborn and Matplotlib in the same figure:\nfig, ax = plt.subplots()\nsns.scatterplot(x='x', y='y', data=df, ax=ax)\nax.plot(x, y, color='r', linestyle='--')\nUse Matplotlib’s plt.tight_layout() to automatically adjust subplot parameters for better spacing.\n\nRemember, most Seaborn functions return a Matplotlib Axes object, allowing you to further customize your plots using Matplotlib functions."
  },
  {
    "objectID": "course-materials/cheatsheets/dictionaries.html",
    "href": "course-materials/cheatsheets/dictionaries.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "my_dict = {}\n\n\n\nmy_dict = {\"name\": \"Alice\", \"age\": 30}\n\n\n\nmixed_dict = {\"number\": 42, \"text\": \"hello\", \"list\": [1, 2, 3], \"flag\": True}\n\n\n\nnested_dict = {\n    \"person1\": {\"name\": \"Alice\", \"age\": 25},\n    \"person2\": {\"name\": \"Bob\", \"age\": 30}\n}"
  },
  {
    "objectID": "course-materials/cheatsheets/dictionaries.html#creating-dictionaries",
    "href": "course-materials/cheatsheets/dictionaries.html#creating-dictionaries",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "my_dict = {}\n\n\n\nmy_dict = {\"name\": \"Alice\", \"age\": 30}\n\n\n\nmixed_dict = {\"number\": 42, \"text\": \"hello\", \"list\": [1, 2, 3], \"flag\": True}\n\n\n\nnested_dict = {\n    \"person1\": {\"name\": \"Alice\", \"age\": 25},\n    \"person2\": {\"name\": \"Bob\", \"age\": 30}\n}"
  },
  {
    "objectID": "course-materials/cheatsheets/dictionaries.html#accessing-elements",
    "href": "course-materials/cheatsheets/dictionaries.html#accessing-elements",
    "title": "EDS 217 Cheatsheet",
    "section": "2. Accessing Elements",
    "text": "2. Accessing Elements\n\n2.1 Access by Key\nprint(my_dict[\"name\"])  # Output: Alice\n\n\n2.2 Safely Access Using .get()\nprint(my_dict.get(\"name\"))  # Output: Alice\nprint(my_dict.get(\"profession\", \"Unknown\"))  # Output: Unknown if not present"
  },
  {
    "objectID": "course-materials/cheatsheets/dictionaries.html#modifying-dictionaries",
    "href": "course-materials/cheatsheets/dictionaries.html#modifying-dictionaries",
    "title": "EDS 217 Cheatsheet",
    "section": "3. Modifying Dictionaries",
    "text": "3. Modifying Dictionaries\n\n3.1 Add or Update an Element\nmy_dict[\"profession\"] = \"Engineer\"  # Adds a new key or updates if exists\n\n\n3.2 Remove Elements\ndel my_dict[\"age\"]  # Removes the key 'age'\nprofession = my_dict.pop(\"profession\", \"No profession found\")  # Removes and returns\nmy_dict.clear()  # Clears all elements"
  },
  {
    "objectID": "course-materials/cheatsheets/dictionaries.html#dictionary-operations",
    "href": "course-materials/cheatsheets/dictionaries.html#dictionary-operations",
    "title": "EDS 217 Cheatsheet",
    "section": "4. Dictionary Operations",
    "text": "4. Dictionary Operations\n\n4.1 Check if Key Exists\n\"name\" in my_dict  # Returns True if 'name' is a key\n\n\n4.2 Iterate Through Keys, Values, or Items\nfor key in my_dict.keys():\n    print(key)\nfor value in my_dict.values():\n    print(value)\nfor key, value in my_dict.items():\n    print(f\"{key}: {value}\")\n\n\n4.3 Dictionary Comprehensions\nsquared = {x: x**2 for x in range(5)}  # {0: 0, 1: 1, 2: 4, 3: 9, 4: 16}\n\n\n4.4 Merge Dictionaries\ndict1 = {\"name\": \"Alice\", \"age\": 25}\ndict2 = {\"city\": \"New York\", \"age\": 30}\nmerged = {**dict1, **dict2}  # Python 3.5+ method"
  },
  {
    "objectID": "course-materials/cheatsheets/dictionaries.html#common-dictionary-methods",
    "href": "course-materials/cheatsheets/dictionaries.html#common-dictionary-methods",
    "title": "EDS 217 Cheatsheet",
    "section": "5. Common Dictionary Methods",
    "text": "5. Common Dictionary Methods\n\n5.1 Get Dictionary Length\nlen(my_dict)  # Returns the number of key-value pairs\n\n\n5.2 Copy a Dictionary\nnew_dict = my_dict.copy()  # Creates a shallow copy of the dictionary\n\n\n5.3 Get All Keys or Values\nall_keys = list(my_dict.keys())\nall_values = list(my_dict.values())\n\n\n5.4 Update Dictionary\nmy_dict.update({\"age\": 26, \"city\": \"Boston\"})  # Updates and adds multiple keys\n\n\n5.5 Set Default Value for Key\nmy_dict.setdefault(\"age\", 29)  # Sets 'age' to 29 if key is not present"
  },
  {
    "objectID": "course-materials/cheatsheets/dictionaries.html#common-dictionary-pitfalls",
    "href": "course-materials/cheatsheets/dictionaries.html#common-dictionary-pitfalls",
    "title": "EDS 217 Cheatsheet",
    "section": "6. Common Dictionary Pitfalls",
    "text": "6. Common Dictionary Pitfalls\n\n6.1 Avoid Modifying a Dictionary While Iterating\n# Incorrect\nfor key in my_dict:\n    if key.startswith('a'):\n        del my_dict[key]\n\n# Correct (Using a copy of keys)\nfor key in list(my_dict.keys()):\n    if key.startswith('a'):\n        del my_dict[key]"
  },
  {
    "objectID": "course-materials/cheatsheets/data_cleaning.html",
    "href": "course-materials/cheatsheets/data_cleaning.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "import pandas as pd"
  },
  {
    "objectID": "course-materials/cheatsheets/data_cleaning.html#importing-pandas",
    "href": "course-materials/cheatsheets/data_cleaning.html#importing-pandas",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "import pandas as pd"
  },
  {
    "objectID": "course-materials/cheatsheets/data_cleaning.html#reading-data",
    "href": "course-materials/cheatsheets/data_cleaning.html#reading-data",
    "title": "EDS 217 Cheatsheet",
    "section": "Reading Data",
    "text": "Reading Data\n# Read CSV file\ndf = pd.read_csv('filename.csv')\n\n# Read Excel file\ndf = pd.read_excel('filename.xlsx')"
  },
  {
    "objectID": "course-materials/cheatsheets/data_cleaning.html#exploring-the-dataframe",
    "href": "course-materials/cheatsheets/data_cleaning.html#exploring-the-dataframe",
    "title": "EDS 217 Cheatsheet",
    "section": "Exploring the DataFrame",
    "text": "Exploring the DataFrame\n# Display first few rows\ndf.head()\n\n# Display basic information about the DataFrame\ndf.info()\n\n# Get summary statistics\ndf.describe()\n\n# Check for missing values\ndf.isnull().sum()"
  },
  {
    "objectID": "course-materials/cheatsheets/data_cleaning.html#handling-missing-data",
    "href": "course-materials/cheatsheets/data_cleaning.html#handling-missing-data",
    "title": "EDS 217 Cheatsheet",
    "section": "Handling Missing Data",
    "text": "Handling Missing Data\n# Drop rows with any missing values\ndf_cleaned = df.dropna()\n\n# Fill missing values with a specific value\ndf['column_name'].fillna(0, inplace=True)\n\n# Fill missing values with the mean of the column\ndf['column_name'].fillna(df['column_name'].mean(), inplace=True)"
  },
  {
    "objectID": "course-materials/cheatsheets/data_cleaning.html#removing-duplicates",
    "href": "course-materials/cheatsheets/data_cleaning.html#removing-duplicates",
    "title": "EDS 217 Cheatsheet",
    "section": "Removing Duplicates",
    "text": "Removing Duplicates\n# Remove duplicate rows\ndf_unique = df.drop_duplicates()\n\n# Remove duplicates based on specific columns\ndf_unique = df.drop_duplicates(subset=['column1', 'column2'])"
  },
  {
    "objectID": "course-materials/cheatsheets/data_cleaning.html#renaming-columns",
    "href": "course-materials/cheatsheets/data_cleaning.html#renaming-columns",
    "title": "EDS 217 Cheatsheet",
    "section": "Renaming Columns",
    "text": "Renaming Columns\n# Rename a single column\ndf = df.rename(columns={'old_name': 'new_name'})\n\n# Rename multiple columns\ndf = df.rename(columns={'old_name1': 'new_name1', 'old_name2': 'new_name2'})"
  },
  {
    "objectID": "course-materials/cheatsheets/data_cleaning.html#changing-data-types",
    "href": "course-materials/cheatsheets/data_cleaning.html#changing-data-types",
    "title": "EDS 217 Cheatsheet",
    "section": "Changing Data Types",
    "text": "Changing Data Types\n# Convert a column to float\ndf['column_name'] = df['column_name'].astype(float)\n\n# Convert column to numeric type\ndf['column_name'] = pd.to_numeric(df['column_name'], errors='coerce')\n\n# Convert column to datetime\ndf['date_column'] = pd.to_datetime(df['date_column'])\n\n# Convert a column to string\ndf['column_name'] = df['column_name'].astype(`string`)"
  },
  {
    "objectID": "course-materials/cheatsheets/data_cleaning.html#filtering-data",
    "href": "course-materials/cheatsheets/data_cleaning.html#filtering-data",
    "title": "EDS 217 Cheatsheet",
    "section": "Filtering Data",
    "text": "Filtering Data\n# Filter rows based on a condition\ndf_filtered = df[df['column_name'] &gt; 5]\n\n# Filter rows based on multiple conditions\ndf_filtered = df[(df['column1'] &gt; 5) & (df['column2'] &lt; 10)]"
  },
  {
    "objectID": "course-materials/cheatsheets/data_cleaning.html#handling-outliers",
    "href": "course-materials/cheatsheets/data_cleaning.html#handling-outliers",
    "title": "EDS 217 Cheatsheet",
    "section": "Handling Outliers",
    "text": "Handling Outliers\n# Remove outliers using Z-score\nfrom scipy import stats\n# Only keep data with a Z-score &lt; 3\ndf_no_outliers = df[(np.abs(stats.zscore(df['column_name'])) &lt; 3)]\n\n# Cap outliers at a specific percentile\nlower = df['column_name'].quantile(0.05)\nupper = df['column_name'].quantile(0.95)\ndf['column_name'] = df['column_name'].clip(lower, upper)"
  },
  {
    "objectID": "course-materials/cheatsheets/data_cleaning.html#resources-for-more-information",
    "href": "course-materials/cheatsheets/data_cleaning.html#resources-for-more-information",
    "title": "EDS 217 Cheatsheet",
    "section": "Resources for More Information",
    "text": "Resources for More Information\n\nPandas Documentation\n10 Minutes to Pandas\nPandas Cheat Sheet\n\nRemember, these are just some of the most common operations for cleaning DataFrames. As you become more comfortable with pandas, you’ll discover many more powerful functions and methods to help you clean and manipulate your data effectively."
  },
  {
    "objectID": "course-materials/cheatsheets/bar_plots.html",
    "href": "course-materials/cheatsheets/bar_plots.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Step 1: Import Libraries\nFirst, you need to import the necessary libraries. We’ll use matplotlib.pyplot for plotting.\nimport matplotlib.pyplot as plt\n\n\nStep 2: Prepare Your Data\nCreate lists or arrays for the categories (x-axis) and their corresponding values (y-axis). Here’s a simple example:\ncategories = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\nvalues = [-22.89, -20.7, -20.69, -11.76, -0.8, 8.59, 11.22, 7.23, -0.11, -10.54, -18.34, -21.44]\n\n\nStep 3: Create the Bar Plot\nUse the bar() function from pyplot to create the bar plot. Pass the categories and values as arguments.\nplt.bar(categories, values)\n\n\nStep 4: Add Labels and Title\nYou can enhance your plot by adding a title and labels for the x-axis and y-axis.\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Simple Bar Plot')\n\n\nStep 5: Display the Plot\nFinally, use plt.show() to display the plot.\nplt.show()\n\n\nComplete Code Example\nHere’s the complete code to create a simple bar plot:\nimport matplotlib.pyplot as plt\n\n# Data for the plot\ncategories = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\nvalues = [-22.89, -20.7, -20.69, -11.76, -0.8, 8.59, 11.22, 7.23, -0.11, -10.54, -18.34, -21.44]\n\n# Create the bar plot\nplt.bar(categories, values)\n\n# Add labels and title\nplt.xlabel('Months')\nplt.ylabel('Average Temperature, deg-C')\nplt.title('Toolik Lake LTER Average Temperatures, 2008-2019')\n\n# Display the plot\nplt.show()\n\n\nCustomizing the Bar Plot\nYou can further customize your bar plot with additional options:\n\nColor: Set the color of the bars using the color parameter.\nplt.bar(categories, values, color='skyblue')\nWidth: Adjust the width of the bars using the width parameter.\nplt.bar(categories, values, width=0.5)\nAdd Grid: Make the plot easier to read by adding a grid.\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nHorizontal Bar Plot: Use barh() for horizontal bar plots.\nplt.barh(categories, values, color='skyblue')\n\n\n\nExample with Customizations\nimport matplotlib.pyplot as plt\n\n# Data for the plot\ncategories = ['Category A', 'Category B', 'Category C', 'Category D']\nvalues = [23, 17, 35, 29]\n\n# Create the bar plot with customizations\nplt.bar(categories, values, color='skyblue', width=0.5)\n\n# Add labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Simple Bar Plot')\n\n# Add grid\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Display the plot\nplt.show()"
  },
  {
    "objectID": "cheatsheets.html",
    "href": "cheatsheets.html",
    "title": "Cheatsheets",
    "section": "",
    "text": "Setting up Python\nJupyterLab Variable Inspection and Shortcuts"
  },
  {
    "objectID": "cheatsheets.html#setup-tools",
    "href": "cheatsheets.html#setup-tools",
    "title": "Cheatsheets",
    "section": "",
    "text": "Setting up Python\nJupyterLab Variable Inspection and Shortcuts"
  },
  {
    "objectID": "cheatsheets.html#python-basics",
    "href": "cheatsheets.html#python-basics",
    "title": "Cheatsheets",
    "section": "🐍 Python Basics",
    "text": "🐍 Python Basics\n\nPython Basics\nPython Functions\nLists\nDictionaries\nSets\nControl Flows\nList and Dictionary Comprehensions\nprint()"
  },
  {
    "objectID": "cheatsheets.html#numpy",
    "href": "cheatsheets.html#numpy",
    "title": "Cheatsheets",
    "section": "🔢 NumPy",
    "text": "🔢 NumPy\n\nNumpy Basics\nNumpy Random Number Generation"
  },
  {
    "objectID": "cheatsheets.html#pandas",
    "href": "cheatsheets.html#pandas",
    "title": "Cheatsheets",
    "section": "🐼 Pandas",
    "text": "🐼 Pandas\n\nPandas 1-pager PDF\nPandas Series\nPandas DataFrames\nPandas DataFrame Methods and Data Science Workflows\nread_csv()\nData Cleaning\nData Filtering & Selection\nData Grouping & Aggregation\nMerging and Joining Data\nWorking with Timeseries"
  },
  {
    "objectID": "cheatsheets.html#data-visualization",
    "href": "cheatsheets.html#data-visualization",
    "title": "Cheatsheets",
    "section": "📊 Data Visualization",
    "text": "📊 Data Visualization\n\nSeaborn Basics\nMatplotlib Basics\nplt.bar() [Matplotlib]\nChart Customization"
  },
  {
    "objectID": "course-materials/lectures/99_dry_vs_wet.html",
    "href": "course-materials/lectures/99_dry_vs_wet.html",
    "title": "EDS 217, Lecture 4: DRY 🏜 vs. WET 🌊",
    "section": "",
    "text": "dry.jpg"
  },
  {
    "objectID": "course-materials/lectures/99_dry_vs_wet.html#dry-vs.-wet",
    "href": "course-materials/lectures/99_dry_vs_wet.html#dry-vs.-wet",
    "title": "EDS 217, Lecture 4: DRY 🏜 vs. WET 🌊",
    "section": "DRY vs. WET",
    "text": "DRY vs. WET\nIf DRY means “Don’t Repeat Yourself”… then WET means “Write Every Time”, or “We Enjoy Typing”\nDon’t write WET code!\n\nHow to DRY out your code\nWe write DRY code - or we DRY out WET code - through a combination of abstraction and normalization."
  },
  {
    "objectID": "course-materials/lectures/99_dry_vs_wet.html#abstraction",
    "href": "course-materials/lectures/99_dry_vs_wet.html#abstraction",
    "title": "EDS 217, Lecture 4: DRY 🏜 vs. WET 🌊",
    "section": "Abstraction",
    "text": "Abstraction\nThe “principle of abstraction” aims to reduce duplication of information (usually code) in a program whenever it is practical to do so:\n“Each significant piece of functionality in a program should be implemented in just one place in the source code. Where similar functions are carried out by distinct pieces of code, it is generally beneficial to combine them into one by abstracting out the varying parts.”\nBenjamin C. Pierce - Types and Programming Languages\n\nAbstraction Example\nThe easiest way to understand abstraction is to see it in action. Here’s an example that you are already familiar with; determining the energy emitted by an object as a function of its temperature:\n\\(Q = \\epsilon  \\sigma  T^4\\)\nwhere \\(\\epsilon\\) is an object’s emmissivity, \\(\\sigma\\) is the Stefan-Boltzmann constant, and \\(T\\) is temperature in degrees Kelvin.\n\n\nAbstraction Example\nWe might write the following code to determine \\(Q\\):\n\n# How much energy is emitted by an object at a certain temperature?\nε = 1      # emissivity [-]\nσ = 5.67e-8  # stefan-boltzmann constant [W/T^4]\nT_C = 40         # temperature [deg-C]\n\nQ = ε * σ * (T_C+273.15)**4\nprint(Q)\n\n\n\nAbstraction Example\nBut this code is going to get very WET very fast.\n\n# How much energy is emitted by an object at a certain temperature?\nε = 1      # emissivity [-]\nσ = 5.67e-8  # stefan-boltzmann constant [W/m^2/K^4]\nT_C = 40         # temperature [deg-C]\n\nQ = ε * σ * (T_C+273.15)**4\n\n# New T value? Different epsilon? What about a bunch of T values?\nT_2 = 30\n\nQ2 = ε * σ * (T_2+273.15)**4\n\n\n\n\nAbstraction Example\nHere’s a DRY version obtained using abstraction:\n\n# energy.py contains a function to calculate Q from T \nfrom energy import Q \n\nT = 40 # deg-C\nE = Q(T, unit='C')\n\n\n\nAbstraction Summary, Part 1\n\nWe keep our code DRY by using abstraction. In addition to functions, python also provides Classes as another important way to create abstractions.\nFunctions and Classes are the subject of this tomorrow’s exercise.\n\n\n\nAbstraction Summary, Part 2\n\nIn general, the process of keeping code DRY through successive layers of abstraction is known as re-factoring.\nThe “Rule of Three” states that you should probably consider refactoring (i.e. adding abstraction) whenever you find your code doing the same thing three times or more."
  },
  {
    "objectID": "course-materials/lectures/99_dry_vs_wet.html#normalization",
    "href": "course-materials/lectures/99_dry_vs_wet.html#normalization",
    "title": "EDS 217, Lecture 4: DRY 🏜 vs. WET 🌊",
    "section": "Normalization",
    "text": "Normalization\nNormalization is the process of structuring data in order to reduce redundancy and improve integrity."
  },
  {
    "objectID": "course-materials/lectures/99_dry_vs_wet.html#normalization-1",
    "href": "course-materials/lectures/99_dry_vs_wet.html#normalization-1",
    "title": "EDS 217, Lecture 4: DRY 🏜 vs. WET 🌊",
    "section": "Normalization",
    "text": "Normalization\nSome of the key principles of Normalization include:\n\nAll data have a Primary Key, which uniquely identifies a record. Usually, in python, this key is called an Index.\nAtomic columns, meaning entries contain a single value. This means no collections should appear as elements within a data table. (i.e. “cells” in structured data should not contain lists!)\nNo transitive dependencies. This means that there should not be implicit associations between columns within data tables.\n\n\nPrimary Keys\nThis form of normalization is easy to obtain, as the idea of an Index is embedded in almost any Python data structure, and a core component of data structures witin pandas, which is the most popular data science library in python (coming next week!).\n\n\nPrimary Keys\n\n# All DataFrames in pandas are created with an index (i.e unique primary key)\nimport pandas as pd\naverage_high_temps = [18.3, 18.3, 18.9, 20.6, 21.1, 21.7,\n                      23.9, 24.4, 23.9, 22.8, 20.6, 18.3]\nsb_high_temp = pd.DataFrame(\n    average_high_temps, # This list will become a single column of values\n    columns=['Average_High_Temperature'] # This is the name of the column\n) # NOTE: use sb_high_temp.head() py-&gt;month_list\n#sb_high_temp.index = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\nsb_high_temp.head()\n\n\n\nAtomic Columns\nThe idea of atomic columns is that each element in a data structure should contain a unique value. This requirement is harder to obtain and you will sometimes violate it.\n\n# import pandas as pd\naverage_high_temps = [18.3, 18.3, 18.9, 20.6, 21.1, 21.7, 23.9, 24.4, 23.9, 22.8, 20.6, 18.3]\naverage_rainfall = [110.7, 119.1, 74.2, 31.5, 8.4, 2.3, 0.5, 1.3, 3.6, 22.9, 45.5, 77.2]\n\n# THIS DATAFRAME IS NOT ATOMIC. EACH ELEMENT IN THE COLUMN IS A LIST.\nsb_climate = pd.DataFrame([\n    [average_high_temps, # The first column will contain a list.\n     average_rainfall]], # The second column will also contain a list.\n    columns=['Monthly Average Temp', 'Monthly Average Rainfall'] # Column names\n)\nsb_climate.head()\n\n\n\nAtomic Columns\nThe idea of atomic columns is that each element in a data structure should contain a unique value. This requirement is harder to obtain and you will sometimes violate it.\n\nimport pandas as pd\naverage_high_temps = [18.3, 18.3, 18.9, 20.6, 21.1, 21.7, 23.9, 24.4, 23.9, 22.8, 20.6, 18.3]\naverage_rainfall = [110.7, 119.1, 74.2, 31.5, 8.4, 2.3, 0.5, 1.3, 3.6, 22.9, 45.5, 77.2]\n\n# THIS DATAFRAME IS ATOMIC. EACH ELEMENT IN THE COLUMN IS A SINGLE VALUE.\nsb_climate = pd.DataFrame({ # Using a dict to create the data frame.\n    'Average_High_Temperature':average_high_temps, # This is the first column\n    'Average_Rainfall':average_rainfall # This is the second column\n})\nsb_climate.index = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\nsb_climate.head()\n\n\n\nTransitive Dependencies\nThe idea of transitive dependencies is the inclusion of multiple associated attributes within the same data structure.\n\nTransitive dependencies make updating data very difficult, but they can be helpful in analyzying data.\nSo we should only introduce them in data that we will not be editing.\n\nUsually environmental data, and especially timeseries, are rarely modified after creation. So we don’t need to worry as much about these dependencies.\nFor example, contrast a data record of “temperatures through time” to a data record of “user contacts in a social network”.\n\n\nTransitive Dependencies\nThe idea of transitive dependencies is the inclusion of multiple associated attributes within the same data structure.\n\nimport pandas as pd\naverage_high_temps = [18.3, 18.3, 18.9, 20.6, 21.1, 21.7, 23.9, 24.4, 23.9, 22.8, 20.6, 18.3]\naverage_rainfall = [110.7, 119.1, 74.2, 31.5, 8.4, 2.3, 0.5, 1.3, 3.6, 22.9, 45.5, 77.2]\n\n# TRANSITIVE ASSOCIATIONS EXIST BETWEEN MONTHS AND SEASONS IN THIS DATAFRAME:\nmonth = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\nseason = ['Winter', 'Winter', 'Spring', 'Spring', 'Spring', 'Summer', 'Summer', 'Summer', 'Fall', 'Fall', 'Fall', 'Winter']\nsb_climate = pd.DataFrame({ # Using a dict to create the data frame.\n    'Month': month,         # Adding month as the first column of the data frame\n    'Season': season,       # Adding the season for each month (this is a transitive dependency)\n    'Avg_High_Temp':average_high_temps, # This is the third column\n    'Avg_Rain':average_rainfall         # This is the fourth column\n})\nsb_climate.head()\n\n\n\nNormalization Summary\nIn general, for data analysis, basic normalization is handled for you.\n\nFor read only data with fixed associations, a lack of normalization is manageable.\nHowever, many analyses are easier if you structure your data in ways that are as normalized as possible.\nIf you are collecting data then it is important to develop an organization structure that is normalized."
  },
  {
    "objectID": "course-materials/lectures/99_dry_vs_wet.html#the-end",
    "href": "course-materials/lectures/99_dry_vs_wet.html#the-end",
    "title": "EDS 217, Lecture 4: DRY 🏜 vs. WET 🌊",
    "section": "The End",
    "text": "The End"
  },
  {
    "objectID": "course-materials/lectures/03-debugging.html",
    "href": "course-materials/lectures/03-debugging.html",
    "title": "Debugging with the VS code debugger",
    "section": "",
    "text": "debug.png\nWhen your code doesn’t work as expected, you might: 1. Use print statements 1. Ask ChatGPT what’s wrong with your code\nPrint statements can be annoying to put all over the place and sometimes ChatGPT doesn’t know. So what if you could step into your code and run it line by line, interactively, to figure out what was wrong? This is what VS code’s debugger does for you."
  },
  {
    "objectID": "course-materials/lectures/03-debugging.html#setup",
    "href": "course-materials/lectures/03-debugging.html#setup",
    "title": "Debugging with the VS code debugger",
    "section": "Setup",
    "text": "Setup\nGet the VS code python extension. 1. Go to the left hand bar in VS code and click “Extensions” 1. Search “Python” 1. Install the extension called “Python” by Microsoft\nNote: VS code has a known bug where sometimes the Debug this cell option disappears. If this happens, you unfortunately need to restart VS code. This is not an issue if debugging .py files though, which is likely where you’ll end up using the debugger the most anyways."
  },
  {
    "objectID": "course-materials/lectures/03-debugging.html#using-the-debugger",
    "href": "course-materials/lectures/03-debugging.html#using-the-debugger",
    "title": "Debugging with the VS code debugger",
    "section": "Using the debugger",
    "text": "Using the debugger\nWe will practice using the debugger with practice 2-1, problem 4.\nFirst, let’s check out how the debugger works by placing a breakpoint on the first line of the cell where we define variables and stepping through it.\nSee how the variables appear under your Run and Debug tab, and try using the Debug Console to print or manipulate the varibles as you step throught the code.\n\n# Import numpy for mean calculations\nimport numpy as np\n\ntempF_2015 = [61.628, 61.7, 61.808, 61.448, 61.52, 61.538, 61.394, 61.52, 61.61, 62.042, 61.988, 62.168]\ntempF_2016 = [62.186, 62.546, 62.528, 62.06, 61.79, 61.52, 61.61, 61.916, 61.718, 61.682, 61.736, 61.628]\ntempF_2017 = [61.916, 62.132, 62.168, 61.772, 61.718, 61.376, 61.556, 61.646, 61.466, 61.7, 61.664, 61.754]\ntempF_2018 = [61.556, 61.61, 61.664, 61.682, 61.556, 61.466, 61.556, 61.448, 61.52, 61.916, 61.556, 61.718]\ntempF_2019 = [61.754, 61.79, 62.186, 61.898, 61.61, 61.7, 61.772, 61.79, 61.754, 61.898, 61.862, 62.042]\ntempF_2020 = [62.186, 62.312, 62.186, 62.114, 61.898, 61.736, 61.7, 61.646, 61.862, 61.664, 62.06, 61.538]\ntempF_2021 = [61.538, 61.232, 61.664, 61.43, 61.484, 61.592, 61.736, 61.556, 61.736, 61.88, 61.772, 61.61]\n\n# List of yearly lists (may or may not be useful)\ntempF_list = [tempF_2015, tempF_2016, tempF_2017, tempF_2018, tempF_2019, tempF_2020, tempF_2021]\n\n# List of years (probably useful)\nyears = [2015, 2016, 2017, 2018, 2019, 2020, 2021]\n\nBeing able to walk through code line by line is especially helpful when you can step into loops or functions that you would otherwise need print statements to see what is happening inside of.\nTry placing a breakpoint on the first line and seeing how the variables change when you step through this for loop.\n\nfor tempF_year,year in zip(tempF_list,years):\n    print(f\"{year} temperatures: {tempF_year}\")\n\n[61.628, 61.7, 61.808, 61.448, 61.52, 61.538, 61.394, 61.52, 61.61, 62.042, 61.988, 62.168]\n2015 temperatures: [61.628, 61.7, 61.808, 61.448, 61.52, 61.538, 61.394, 61.52, 61.61, 62.042, 61.988, 62.168]\n2016 temperatures: [62.186, 62.546, 62.528, 62.06, 61.79, 61.52, 61.61, 61.916, 61.718, 61.682, 61.736, 61.628]\n2017 temperatures: [61.916, 62.132, 62.168, 61.772, 61.718, 61.376, 61.556, 61.646, 61.466, 61.7, 61.664, 61.754]\n2018 temperatures: [61.556, 61.61, 61.664, 61.682, 61.556, 61.466, 61.556, 61.448, 61.52, 61.916, 61.556, 61.718]\n2019 temperatures: [61.754, 61.79, 62.186, 61.898, 61.61, 61.7, 61.772, 61.79, 61.754, 61.898, 61.862, 62.042]\n2020 temperatures: [62.186, 62.312, 62.186, 62.114, 61.898, 61.736, 61.7, 61.646, 61.862, 61.664, 62.06, 61.538]\n2021 temperatures: [61.538, 61.232, 61.664, 61.43, 61.484, 61.592, 61.736, 61.556, 61.736, 61.88, 61.772, 61.61]\n\n\nNow let’s try debugging some code. Suppose your friend has written some code to solve problem 4a but is running into an error. Let’s try using the debugger to fix it:\nPractice 2-1 problem 4a: Calculate the monthly global temperature anomalies (deviation from the mean) in °C for 2015-2021. The mean global land-ocean surface temperature calculated over the 20th century was 15.6°C.\n\n# 4a. Monthly temperature anomalies\n\n# Innitialize a list of yearly lists of anomalies in °C\nanomC_list = [] \n\n# Iterate through yearly lists\nfor tempsF in tempF_list:\n    # Empty list for a single year\n    anomC = []\n    # Iterate through a single year\n    for tempF in tempsF:\n        # Convert temp from F to C\n        tempC = tempF - 32.0 * (5/9)\n        # Calculate temp anomaly in °C\n        anomC = tempC - 15.6\n        # Append to anomC\n        anomC.append(round(anomC,2))\n    # Add list of anomalies in °C to anomC_list (list of lists)\n    anomC_list.append(anomC)\n\nfor anom_list,year in zip(anomC_list,years):\n    print(f\"{year} anomalies: {anom_list}\")\n\n28.25022222222222\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[2], line 17\n     15     anomC = tempC - 15.6\n     16     # Append to anomC\n---&gt; 17     anomC.append(round(anomC,2))\n     18 # Add list of anomalies in °C to anomC_list (list of lists)\n     19 anomC_list.append(anomC)\n\nAttributeError: 'float' object has no attribute 'append'\n\n\n\nNow you try it on your own. Try using the debugger to debug the following answer to question 4b:\nCreate a new list with the mean monthly global surface temperature anomalies in °C for 2015-2023 (i.e. calculate the mean temperature anomaly for each month and put these values in a list).\n\n# 4b. Mean monthly temperature anomaly, 2015-2021\n\n# Empty list for monthly mean temperature anomalies\nmonthly_means = []\n# Set up generic counter loop with 12 iterations\nfor i in range(12 + 1):\n    # Generate list of temperature anomalies for each month by extracting the ith value from each sublist.\n    monthly = []\n    # generate a list of anomalies for month i with anomalies from all years in it\n    for anom in anomC_list:\n        # get the ith anomaly (ith month) from each year list\n        monthly.append(anoms[i + 1])\n    # Calculate mean for month i\n    monthly_mean = np.sum(monthly)\n    # Add mean for month i to list of means\n    monthly_means.append(round(monthly_means,4))\n\n# Print list of mean monthly temperature anomalies, 2015-2018.\nprint(f\"monthly_means: {monthly_means}\")"
  },
  {
    "objectID": "course-materials/lectures/01_the_zen_of_python.html",
    "href": "course-materials/lectures/01_the_zen_of_python.html",
    "title": "The Zen of Python",
    "section": "",
    "text": "# What is the Zen of Python??\nimport this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!"
  },
  {
    "objectID": "course-materials/lectures/01_the_zen_of_python.html#python-errors",
    "href": "course-materials/lectures/01_the_zen_of_python.html#python-errors",
    "title": "The Zen of Python",
    "section": "Python Errors",
    "text": "Python Errors\nThere are two types of errors in Python: SyntaxErrors and Exceptions.\n\nSyntaxErrors\nA SyntaxError happens when the Python language interpreter (the parser) detects an incorrectly formatted statement.\nThis code is trying to divide two numbers, but there are mismatched parentheses. What happens when we run it?\n&gt;&gt;&gt; print( 5 / 4 ))\n\nprint( 5 / 4 ))\n\n\n  Cell In[1], line 1\n    print( 5 / 4 ))\n                  ^\nSyntaxError: unmatched ')'\n\n\n\n\nWhen python says SyntaxError, you should read this as I don't know what you want me to do!?\nOften the error includes some indication of where the problem is, although this indication can sometimes be misleading if the detection occurs far away from the syntax problem that created the error. Often the interpreter will attempt to explain what the problem is!\n\n\nExceptions\nAn Exception happens the code you have written violates the Python language specification.\nThis code is trying to divide zero by 0. Its syntax is correct. But what happens when we run it?\n&gt;&gt;&gt; print( 0 / 0 )\n\ntry:\n    print( 0 / 0 ) \nexcept ZeroDivisionError:\n    print(f\"It didn't work because you tried to divide by zero\")\n\nIt didn't work because you tried to divide by zero\n\n\nWhen python says anything other than SyntaxError, you should read this as You are asking to do something I can't do\nIn this case, the ZeroDivisionError is raised because the Python language specification does not allow for division by zero.\n\n\nTypes of Exceptions\nPython has a lot of builtin Errors that correspond to the definition of the Python language.\nA few common Exceptions you will see include TypeError, IndexError, and KeyError.\n\n\nTypeError\nA TypeError is raised when you try to perform a valid method on an inappropriate data type.\n\n# TypeError Examples:\n'a' + 3\n\n\n\nIndexError\nAn IndexError is raised when you try to access an undefined element of a sequence. Sequences are structured data types whose elements are stored in a specific order. A list is an example of a sequence.\n\n# IndexError Example:\nmy_list = ['a', 'b', 'c', 'd']\nmy_list[4]\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[14], line 3\n      1 # IndexError Example:\n      2 my_list = ['a', 'b', 'c', 'd']\n----&gt; 3 my_list[4]\n\nIndexError: list index out of range\n\n\n\n\n\nKeyError\nA KeyError is raised when you try to perform a valid method on an inappropriate data type.\n\n# KeyError Examples:\n\nmy_dict = {'column_1': 'definition 1', 'another_word': 'a second definition'}\nmy_dict['column1']\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[16], line 4\n      1 # KeyError Examples:\n      3 my_dict = {'column_1': 'definition 1', 'another_word': 'a second definition'}\n----&gt; 4 my_dict['column1']\n\nKeyError: 'column1'\n\n\n\n\n\nDeciphering Tracebacks\nWhen an exception is raised in python the interpreter generates a “Traceback” that shows where and why the error occurred. Generally, the REPL has most detailed Traceback information, although Jupyter Notebooks and iPython interactive shells also provide necessary information to debug any exception.\n\n# defining a function\ndef multiply(num1, num2):\n    result = num1 * num2\n    print(results)\n \n# calling the function\nmultiply(10, 2)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 7\n      4     print(results)\n      6 # calling the function\n----&gt; 7 multiply(10, 2)\n\nCell In[17], line 4, in multiply(num1, num2)\n      2 def multiply(num1, num2):\n      3     result = num1 * num2\n----&gt; 4     print(results)\n\nNameError: name 'results' is not defined\n\n\n\n\n## The End"
  },
  {
    "objectID": "course-materials/coding-colabs/6b_preprocess.html",
    "href": "course-materials/coding-colabs/6b_preprocess.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\n\n# Load the CO2 dataset\nco2_url = \"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv\"\nco2_df = pd.read_csv(co2_url, comment='#', header=1, \n                     names=['Year', 'Month', 'DecimalDate', 'MonthlyAverage', \n                            'Deseasonalized', 'DaysInMonth', 'StdDev', 'Uncertainty'])\n\n\nco2_df.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nDecimalDate\nMonthlyAverage\nDeseasonalized\nDaysInMonth\nStdDev\nUncertainty\n\n\n\n\n0\n1958\n4\n1958.2877\n317.45\n315.16\n-1\n-9.99\n-0.99\n\n\n1\n1958\n5\n1958.3699\n317.51\n314.69\n-1\n-9.99\n-0.99\n\n\n2\n1958\n6\n1958.4548\n317.27\n315.15\n-1\n-9.99\n-0.99\n\n\n3\n1958\n7\n1958.5370\n315.87\n315.20\n-1\n-9.99\n-0.99\n\n\n4\n1958\n8\n1958.6219\n314.93\n316.21\n-1\n-9.99\n-0.99\n\n\n\n\n\n\n\n\n\n# Convert Year and Month to datetime\nco2_df['Date'] = pd.to_datetime(co2_df['Year'].astype(str) + '-' + co2_df['Month'].astype(str) + '-01')\n\n# Select only the Date and MonthlyAverage columns\nco2_df = co2_df[['Date', 'MonthlyAverage']].rename(columns={'MonthlyAverage': 'CO2Concentration'})\n\n# Sort by date and reset index\nco2_df = co2_df.sort_values('Date').reset_index(drop=True)\n\n# Save to CSV\nco2_df.to_csv('monthly_co2_concentration.csv', index=False)\n\nprint(co2_df.head())\n\n        Date  CO2Concentration\n0 1958-04-01            317.45\n1 1958-05-01            317.51\n2 1958-06-01            317.27\n3 1958-07-01            315.87\n4 1958-08-01            314.93"
  },
  {
    "objectID": "course-materials/lectures/00_intro_to_python.html",
    "href": "course-materials/lectures/00_intro_to_python.html",
    "title": "Lecture 1 - Intro to Python and Environmental Data Science",
    "section": "",
    "text": "Course Webpage: https://eds-217-essential-python.github.io\n\n\n\n\ndata_science.jpg\n\n\n\n\n\nenvironmental_data_science.jpg\n\n\n\n\n\n🐍 What Python?\n❓ Why Python?\n💻 How Python?\n\n\n“Python is powerful… and fast; plays well with others; runs everywhere; is friendly & easy to learn; is Open.”\n\n\n\n\nPython is a general-purpose, object-oriented programming language that emphasizes code readability through its generous use of white space. Released in 1989, Python is easy to learn and a favorite of programmers and developers.\n\n\n(Python, C, C++, Java, Javascript, R, Pascal) - Take less time to write - Shorter and easier to read - Portable, meaning that they can run on different kinds of computers with few or no modifications.\nThe engine that translates and runs Python is called the Python Interpreter\n\n\"\"\" \nEntering code into this notebook cell \nand pressing [SHIFT-ENTER] will cause the \npython interpreter to execute the code\n\"\"\"\n\n  \nprint(\"Hello world!\")\nprint(\"[from this notebook cell]\")\n\nHello world!\n[from this notebook cell]\n\n\n\n\"\"\"\nAlternatively, you can run a \nany python script file (.py file)\nso long as it contains valid\npython code.\n\"\"\"\n!python hello_world.py\n\nHello world!\n[from hello_world.py]\n\n\n\n \n\n\n\n\nNatural languages are the languages that people speak. They are not designed (although they are subjected to various degrees of “order”) and evolve naturally.\nFormal languages are languages that are designed by people for specific applications. - Mathematical Notation \\(E=mc^2\\) - Chemical Notation: \\(\\text{H}_2\\text{O}\\)\nProgramming languages are formal languages that have been designed to express computations.\nParsing: The process of figuring out what the structure of a sentence or statement is (in a natural language you do this subconsciously).\nFormal Languages have strict syntax for tokens and structure:\n\nMathematical syntax error: \\(E=\\$m🦆_2\\) (bad tokens & bad structure)\nChemical syntax error: \\(\\text{G}_3\\text{Z}\\) (bad tokens, but structure is okay)\n\n\n\n\n\nAmbiguity: Natural languages are full of ambiguity, which people parse using contextual clues. Formal languages are nearly or completely unambiguous; any statement has exactly one meaning, regardless of context.\nRedundancy: In order to make up for ambiguity, natural languages employ lots of redundancy. Formal languages are less redundant and more concise.\nLiteralness: Formal languages mean exactly what they say. Natural languages employ idioms and metaphors.\n\nThe inherent differences between familiar natural languages and unfamiliar formal languages creates one of the greatest challenges in learning to code.\n\n\n\n\npoetry: Words are used for sound and meaning. Ambiguity is common and often deliberate.\nprose: The literal meaning of words is important, and the structure contributes meaning. Amenable to analysis but still often ambiguous.\nprogram: Meaning is unambiguous and literal, and can be understood entirely by analysis of the tokens and structure.\n\n\n\n\nFormal languages are very dense, so it takes longer to read them.\nStructure is very important, so it is usually not a good idea to read from top to bottom, left to right. Instead, learn to parse the program in your head, identifying the tokens and interpreting the structure.\nDetails matter. Little things like spelling errors and bad punctuation, which you can get away with in natural languages, will make a big difference in a formal language.\n\n\n\n\n\n\nIBM: R vs. Python\nPython is a multi-purpose language with a readable syntax that’s easy to learn. Programmers use Python to delve into data analysis or use machine learning in scalable production environments.\nR is built by statisticians and leans heavily into statistical models and specialized analytics. Data scientists use R for deep statistical analysis, supported by just a few lines of code and beautiful data visualizations.\nIn general, R is better for initial exploratory analyses, statistical analyses, and data visualization.\nIn general, Python is better for working with APIs, writing maintainable, production-ready code, working with a diverse array of data, and building machine learning or AI workflows.\nBoth languages can do anything. Most data science teams use both languages. (and others too.. Matlab, Javascript, Go, Fortran, etc…)\n\nfrom IPython.lib.display import YouTubeVideo\nYouTubeVideo('GVvfNgszdU0')\n\n\n        \n        \n\n\n\n\nAnaconda State of Data Science\nData from 2021: \n\n\n\n\nThe data are available here…\nBut, unfortunately, they changed the format of the responses concerning language use between 2022 and 2023. But we can take look at the 2022 data…\nLet’s do some python data science!\n\n# First, we need to gather our tools\nimport pandas as pd  # This is the most common data science package used in python!\nimport matplotlib.pyplot as plt # This is the most widely-used plotting package.\n\nimport requests # This package helps us make https requests \nimport io # This package is good at handling input/output streams\n\n\n# Here's the url for the 2022 data. It has a similar structure to the 2021 data, so we can compare them.\nurl = \"https://static.anaconda.cloud/content/Anaconda_2022_State_of_Data_Science_+Raw_Data.csv\"\nresponse = requests.get(url)\n\n\n# Read the response into a dataframe, using the io.StringIO function to feed the response.txt.\n# Also, skip the first three rows\ndf = pd.read_csv(io.StringIO(response.text), skiprows=3)\n\n# Our very first dataframe!\ndf.head()\n\n# Jupyter notebook cells only output the last value requested...\n\n\n\n\n\n\n\n\nIn which country is your primary residence?\nWhich of the following age groups best describes you?\nWhat is the highest level of education you've achieved?\nGender: How do you identify? - Selected Choice\nThe organization I work for is best classified as a:\nWhat is your primary role? - Selected Choice\nFor how many years have you been in your current role?\nWhat position did you hold prior to this? - Selected Choice\nHow would you rate your job satisfaction in your current role?\nWhat would cause you to leave your current employer for a new job? Please select the top option besides pay/benefits. - Selected Choice\n...\nWhat should an AutoML tool do for data scientists? Please drag answers to rank from most important to least important. (1=most important) - Help choose the best model types to solve specific problems\nWhat should an AutoML tool do for data scientists? Please drag answers to rank from most important to least important. (1=most important) - Speed up the ML pipeline by automating certain workflows (data cleaning, etc.)\nWhat should an AutoML tool do for data scientists? Please drag answers to rank from most important to least important. (1=most important) - Tune the model once performance (such as accuracy, etc.) starts to degrade\nWhat should an AutoML tool do for data scientists? Please drag answers to rank from most important to least important. (1=most important) - Other (please indicate)\nWhat do you think is the biggest problem in the data science/AI/ML space today? - Selected Choice\nWhat tools and resources do you feel are lacking for data scientists who want to learn and develop their skills? (Select all that apply). - Selected Choice\nHow do you typically learn about new tools and topics relevant to your role? (Select all that apply). - Selected Choice\nWhat are you most hoping to see from the data science industry this year? - Selected Choice\nWhat do you believe is the biggest challenge in the open-source community today? - Selected Choice\nHave supply chain disruption problems, such as the ongoing chip shortage, impacted your access to computing resources?\n\n\n\n\n0\nUnited States\n26-41\nDoctoral degree\nMale\nEducational institution\nData Scientist\n1-2 years\nData Scientist\nVery satisfied\nMore flexibility with my work hours\n...\n4.0\n2.0\n5.0\n6.0\nA reduction in job opportunities caused by aut...\nHands-on projects,Mentorship opportunities\nReading technical books, blogs, newsletters, a...\nFurther innovation in the open-source data sci...\nUndermanagement\nNo\n\n\n1\nUnited States\n42-57\nDoctoral degree\nMale\nCommercial (for-profit) entity\nProduct Manager\n5-6 years\nNaN\nVery satisfied\nMore responsibility/opportunity for career adv...\n...\n2.0\n5.0\n4.0\n6.0\nSocial impacts from bias in data and models\nTailored learning paths\nFree video content (e.g. YouTube)\nMore specialized data science hardware\nPublic trust\nYes\n\n\n2\nIndia\n18-25\nBachelor's degree\nFemale\nEducational institution\nData Scientist\nNaN\nNaN\nNaN\nNaN\n...\n1.0\n4.0\n2.0\n6.0\nA reduction in job opportunities caused by aut...\nHands-on projects,Mentorship opportunities\nReading technical books, blogs, newsletters, a...\nFurther innovation in the open-source data sci...\nUndermanagement\nI'm not sure\n\n\n3\nUnited States\n42-57\nBachelor's degree\nMale\nCommercial (for-profit) entity\nProfessor/Instructor/Researcher\n10+ years\nNaN\nModerately satisfied\nMore responsibility/opportunity for career adv...\n...\n1.0\n5.0\n4.0\n6.0\nSocial impacts from bias in data and models\nHands-on projects\nReading technical books, blogs, newsletters, a...\nNew optimized models that allow for more compl...\nTalent shortage\nNo\n\n\n4\nSingapore\n18-25\nHigh School or equivalent\nMale\nNaN\nStudent\nNaN\nNaN\nNaN\nNaN\n...\n4.0\n2.0\n3.0\n6.0\nSocial impacts from bias in data and models\nCommunity engagement and learning platforms,Ta...\nReading technical books, blogs, newsletters, a...\nFurther innovation in the open-source data sci...\nUndermanagement\nYes\n\n\n\n\n5 rows × 120 columns\n\n\n\n\n\n# Jupyter notebook cells only output the last value... unless you use print commands!\nprint(f'Number of survey responses: {len(df)}')\nprint(f'Number of survey questions: {len(df.columns)}') \n\nNumber of survey responses: 3493\nNumber of survey questions: 120\n\n\n\n# 1. Filter the dataframe to only the questions about programming language usage, and \nfiltered_df = df.filter(like='How often do you use the following languages?').copy() # Use copy to force python to make a new copy of the data, not just a reference to a subset.\n\n# 2. Rename the columns to just be the programming languages, without the question preamble\nfiltered_df.rename(columns=lambda x: x.split('-')[-1].strip() if '-' in x else x, inplace=True)\n\nprint(filtered_df.columns)\n\nIndex(['Python', 'R', 'Java', 'JavaScript', 'C/C++', 'C#', 'Julia', 'HTML/CSS',\n       'Bash/Shell', 'SQL', 'Go', 'PHP', 'Rust', 'TypeScript',\n       'Other (please indicate below)'],\n      dtype='object')\n\n\n\n# Show the unique values of the `Python` column\nprint(filtered_df['Python'].unique())\n\n['Frequently' 'Sometimes' 'Always' 'Never' 'Rarely' nan]\n\n\n\n# Calculate the percentage of each response for each language\npercentage_df = filtered_df.apply(lambda x: x.value_counts(normalize=True).fillna(0) * 100).transpose()\n\n# Remove the last row, which is the \"Other\" category\npercentage_df = percentage_df[:-1]\n\n# Sort the DataFrame based on the 'Always' responses\nsorted_percentage_df = percentage_df.sort_values(by='Always', ascending=True)\n\n\n# Let's get ready to plot the 2022 data...\nfrom IPython.display import display\n\n# We are going to use the display command to update our figure over multiple cells. \n# This usually isn't necessary, but it's helpful here to see how each set of commands updates the figure\n\n# Define the custom order for plotting\norder = ['Always', 'Frequently', 'Sometimes', 'Rarely', 'Never']\n\ncolors = {\n    'Always': (8/255, 40/255, 81/255),       # Replace R1, G1, B1 with the RGB values for 'Dark Blue'\n    'Frequently': (12/255, 96/255, 152/255),   # Replace R2, G2, B2 with the RGB values for 'Light Ocean Blue'\n    'Sometimes': (16/255, 146/255, 136/255),    # and so on...\n    'Rarely': (11/255, 88/255, 73/255),\n    'Never': (52/255, 163/255, 32/255)\n}\n\n\n# Make the plot\nfig, ax = plt.subplots(figsize=(10, 7))\nsorted_percentage_df[order].plot(kind='barh', stacked=True, ax=ax, color=[colors[label] for label in order])\nax.set_xlabel('Percentage')\nax.set_title('Frequency of Language Usage, 2022',y=1.05)\n\nplt.show() # This command draws our figure. \n\n\n\n\n\n\n\n\n\n# Add labels across the top, like in the original graph\n\n# Get the patches for the top-most bar\nnum_languages = len(sorted_percentage_df)\n\npatches = ax.patches[num_languages-1::num_languages]\n# Calculate the cumulative width of the patches for the top-most bar\ncumulative_widths = [0] * len(order)\nwidths = [patch.get_width() for patch in patches]\nfor i, width in enumerate(widths):\n    cumulative_widths[i] = width + (cumulative_widths[i-1] if i &gt; 0 else 0)\n\n\n\n# Add text labels above the bars\nfor i, (width, label) in enumerate(zip(cumulative_widths, order)):\n    # Get the color of the current bar segment\n    # Calculate the position for the text label\n    position = width - (patches[i].get_width() / 2)\n    # Add the text label to the plot\n    # Adjust the y-coordinate for the text label\n    y_position = len(sorted_percentage_df) - 0.3  # Adjust the 0.3 value as needed\n    ax.text(position, y_position, label, ha='center', color=colors[label], fontweight='bold')\n\n# Remove the legend\nax.legend().set_visible(False)\n\n#plt.show()\ndisplay(fig) # This command shows our updated figure (we can't re-use \"plt.show()\")\n\n\n\n\n\n\n\n\n\n# Add percentage values inside each patch\nfor patch in ax.patches:\n    # Get the width and height of the patch\n    width, height = patch.get_width(), patch.get_height()\n    \n    # Calculate the position for the text label\n    x = patch.get_x() + width / 2\n    y = patch.get_y() + height / 2\n    \n    # Get the percentage value for the current patch\n    percentage = \"{:.0f}%\".format(width)\n    \n    # Add the text label to the plot\n    ax.text(x, y, percentage, ha='center', va='center', color='white', fontweight='bold')\n\ndisplay(fig) # Let's see those nice text labels!\n\n\n\n\n\n\n\n\n\n# Clean up the figure to remove spines and unecessary labels/ticks, etc..\n\n# Remove x-axis label\nax.set_xlabel('')\n\n# Remove the spines\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(False)\n\n# Remove the y-axis tick marks\nax.tick_params(axis='y', which='both', length=0)\n\n# Remove the x-axis tick marks and labels\nax.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n\ndisplay(fig) # Now 100% less visually cluttered!"
  },
  {
    "objectID": "course-materials/lectures/00_intro_to_python.html#lecture-agenda",
    "href": "course-materials/lectures/00_intro_to_python.html#lecture-agenda",
    "title": "Lecture 1 - Intro to Python and Environmental Data Science",
    "section": "",
    "text": "🐍 What Python?\n❓ Why Python?\n💻 How Python?\n\n\n“Python is powerful… and fast; plays well with others; runs everywhere; is friendly & easy to learn; is Open.”"
  },
  {
    "objectID": "course-materials/lectures/00_intro_to_python.html#what-is-python",
    "href": "course-materials/lectures/00_intro_to_python.html#what-is-python",
    "title": "Lecture 1 - Intro to Python and Environmental Data Science",
    "section": "",
    "text": "Python is a general-purpose, object-oriented programming language that emphasizes code readability through its generous use of white space. Released in 1989, Python is easy to learn and a favorite of programmers and developers.\n\n\n(Python, C, C++, Java, Javascript, R, Pascal) - Take less time to write - Shorter and easier to read - Portable, meaning that they can run on different kinds of computers with few or no modifications.\nThe engine that translates and runs Python is called the Python Interpreter\n\n\"\"\" \nEntering code into this notebook cell \nand pressing [SHIFT-ENTER] will cause the \npython interpreter to execute the code\n\"\"\"\n\n  \nprint(\"Hello world!\")\nprint(\"[from this notebook cell]\")\n\nHello world!\n[from this notebook cell]\n\n\n\n\"\"\"\nAlternatively, you can run a \nany python script file (.py file)\nso long as it contains valid\npython code.\n\"\"\"\n!python hello_world.py\n\nHello world!\n[from hello_world.py]\n\n\n\n \n\n\n\n\nNatural languages are the languages that people speak. They are not designed (although they are subjected to various degrees of “order”) and evolve naturally.\nFormal languages are languages that are designed by people for specific applications. - Mathematical Notation \\(E=mc^2\\) - Chemical Notation: \\(\\text{H}_2\\text{O}\\)\nProgramming languages are formal languages that have been designed to express computations.\nParsing: The process of figuring out what the structure of a sentence or statement is (in a natural language you do this subconsciously).\nFormal Languages have strict syntax for tokens and structure:\n\nMathematical syntax error: \\(E=\\$m🦆_2\\) (bad tokens & bad structure)\nChemical syntax error: \\(\\text{G}_3\\text{Z}\\) (bad tokens, but structure is okay)\n\n\n\n\n\nAmbiguity: Natural languages are full of ambiguity, which people parse using contextual clues. Formal languages are nearly or completely unambiguous; any statement has exactly one meaning, regardless of context.\nRedundancy: In order to make up for ambiguity, natural languages employ lots of redundancy. Formal languages are less redundant and more concise.\nLiteralness: Formal languages mean exactly what they say. Natural languages employ idioms and metaphors.\n\nThe inherent differences between familiar natural languages and unfamiliar formal languages creates one of the greatest challenges in learning to code.\n\n\n\n\npoetry: Words are used for sound and meaning. Ambiguity is common and often deliberate.\nprose: The literal meaning of words is important, and the structure contributes meaning. Amenable to analysis but still often ambiguous.\nprogram: Meaning is unambiguous and literal, and can be understood entirely by analysis of the tokens and structure.\n\n\n\n\nFormal languages are very dense, so it takes longer to read them.\nStructure is very important, so it is usually not a good idea to read from top to bottom, left to right. Instead, learn to parse the program in your head, identifying the tokens and interpreting the structure.\nDetails matter. Little things like spelling errors and bad punctuation, which you can get away with in natural languages, will make a big difference in a formal language."
  },
  {
    "objectID": "course-materials/lectures/00_intro_to_python.html#why-python",
    "href": "course-materials/lectures/00_intro_to_python.html#why-python",
    "title": "Lecture 1 - Intro to Python and Environmental Data Science",
    "section": "",
    "text": "IBM: R vs. Python\nPython is a multi-purpose language with a readable syntax that’s easy to learn. Programmers use Python to delve into data analysis or use machine learning in scalable production environments.\nR is built by statisticians and leans heavily into statistical models and specialized analytics. Data scientists use R for deep statistical analysis, supported by just a few lines of code and beautiful data visualizations.\nIn general, R is better for initial exploratory analyses, statistical analyses, and data visualization.\nIn general, Python is better for working with APIs, writing maintainable, production-ready code, working with a diverse array of data, and building machine learning or AI workflows.\nBoth languages can do anything. Most data science teams use both languages. (and others too.. Matlab, Javascript, Go, Fortran, etc…)\n\nfrom IPython.lib.display import YouTubeVideo\nYouTubeVideo('GVvfNgszdU0')\n\n\n        \n        \n\n\n\n\nAnaconda State of Data Science\nData from 2021:"
  },
  {
    "objectID": "course-materials/lectures/00_intro_to_python.html#what-about-2023-data",
    "href": "course-materials/lectures/00_intro_to_python.html#what-about-2023-data",
    "title": "Lecture 1 - Intro to Python and Environmental Data Science",
    "section": "",
    "text": "The data are available here…\nBut, unfortunately, they changed the format of the responses concerning language use between 2022 and 2023. But we can take look at the 2022 data…\nLet’s do some python data science!\n\n# First, we need to gather our tools\nimport pandas as pd  # This is the most common data science package used in python!\nimport matplotlib.pyplot as plt # This is the most widely-used plotting package.\n\nimport requests # This package helps us make https requests \nimport io # This package is good at handling input/output streams\n\n\n# Here's the url for the 2022 data. It has a similar structure to the 2021 data, so we can compare them.\nurl = \"https://static.anaconda.cloud/content/Anaconda_2022_State_of_Data_Science_+Raw_Data.csv\"\nresponse = requests.get(url)\n\n\n# Read the response into a dataframe, using the io.StringIO function to feed the response.txt.\n# Also, skip the first three rows\ndf = pd.read_csv(io.StringIO(response.text), skiprows=3)\n\n# Our very first dataframe!\ndf.head()\n\n# Jupyter notebook cells only output the last value requested...\n\n\n\n\n\n\n\n\nIn which country is your primary residence?\nWhich of the following age groups best describes you?\nWhat is the highest level of education you've achieved?\nGender: How do you identify? - Selected Choice\nThe organization I work for is best classified as a:\nWhat is your primary role? - Selected Choice\nFor how many years have you been in your current role?\nWhat position did you hold prior to this? - Selected Choice\nHow would you rate your job satisfaction in your current role?\nWhat would cause you to leave your current employer for a new job? Please select the top option besides pay/benefits. - Selected Choice\n...\nWhat should an AutoML tool do for data scientists? Please drag answers to rank from most important to least important. (1=most important) - Help choose the best model types to solve specific problems\nWhat should an AutoML tool do for data scientists? Please drag answers to rank from most important to least important. (1=most important) - Speed up the ML pipeline by automating certain workflows (data cleaning, etc.)\nWhat should an AutoML tool do for data scientists? Please drag answers to rank from most important to least important. (1=most important) - Tune the model once performance (such as accuracy, etc.) starts to degrade\nWhat should an AutoML tool do for data scientists? Please drag answers to rank from most important to least important. (1=most important) - Other (please indicate)\nWhat do you think is the biggest problem in the data science/AI/ML space today? - Selected Choice\nWhat tools and resources do you feel are lacking for data scientists who want to learn and develop their skills? (Select all that apply). - Selected Choice\nHow do you typically learn about new tools and topics relevant to your role? (Select all that apply). - Selected Choice\nWhat are you most hoping to see from the data science industry this year? - Selected Choice\nWhat do you believe is the biggest challenge in the open-source community today? - Selected Choice\nHave supply chain disruption problems, such as the ongoing chip shortage, impacted your access to computing resources?\n\n\n\n\n0\nUnited States\n26-41\nDoctoral degree\nMale\nEducational institution\nData Scientist\n1-2 years\nData Scientist\nVery satisfied\nMore flexibility with my work hours\n...\n4.0\n2.0\n5.0\n6.0\nA reduction in job opportunities caused by aut...\nHands-on projects,Mentorship opportunities\nReading technical books, blogs, newsletters, a...\nFurther innovation in the open-source data sci...\nUndermanagement\nNo\n\n\n1\nUnited States\n42-57\nDoctoral degree\nMale\nCommercial (for-profit) entity\nProduct Manager\n5-6 years\nNaN\nVery satisfied\nMore responsibility/opportunity for career adv...\n...\n2.0\n5.0\n4.0\n6.0\nSocial impacts from bias in data and models\nTailored learning paths\nFree video content (e.g. YouTube)\nMore specialized data science hardware\nPublic trust\nYes\n\n\n2\nIndia\n18-25\nBachelor's degree\nFemale\nEducational institution\nData Scientist\nNaN\nNaN\nNaN\nNaN\n...\n1.0\n4.0\n2.0\n6.0\nA reduction in job opportunities caused by aut...\nHands-on projects,Mentorship opportunities\nReading technical books, blogs, newsletters, a...\nFurther innovation in the open-source data sci...\nUndermanagement\nI'm not sure\n\n\n3\nUnited States\n42-57\nBachelor's degree\nMale\nCommercial (for-profit) entity\nProfessor/Instructor/Researcher\n10+ years\nNaN\nModerately satisfied\nMore responsibility/opportunity for career adv...\n...\n1.0\n5.0\n4.0\n6.0\nSocial impacts from bias in data and models\nHands-on projects\nReading technical books, blogs, newsletters, a...\nNew optimized models that allow for more compl...\nTalent shortage\nNo\n\n\n4\nSingapore\n18-25\nHigh School or equivalent\nMale\nNaN\nStudent\nNaN\nNaN\nNaN\nNaN\n...\n4.0\n2.0\n3.0\n6.0\nSocial impacts from bias in data and models\nCommunity engagement and learning platforms,Ta...\nReading technical books, blogs, newsletters, a...\nFurther innovation in the open-source data sci...\nUndermanagement\nYes\n\n\n\n\n5 rows × 120 columns\n\n\n\n\n\n# Jupyter notebook cells only output the last value... unless you use print commands!\nprint(f'Number of survey responses: {len(df)}')\nprint(f'Number of survey questions: {len(df.columns)}') \n\nNumber of survey responses: 3493\nNumber of survey questions: 120\n\n\n\n# 1. Filter the dataframe to only the questions about programming language usage, and \nfiltered_df = df.filter(like='How often do you use the following languages?').copy() # Use copy to force python to make a new copy of the data, not just a reference to a subset.\n\n# 2. Rename the columns to just be the programming languages, without the question preamble\nfiltered_df.rename(columns=lambda x: x.split('-')[-1].strip() if '-' in x else x, inplace=True)\n\nprint(filtered_df.columns)\n\nIndex(['Python', 'R', 'Java', 'JavaScript', 'C/C++', 'C#', 'Julia', 'HTML/CSS',\n       'Bash/Shell', 'SQL', 'Go', 'PHP', 'Rust', 'TypeScript',\n       'Other (please indicate below)'],\n      dtype='object')\n\n\n\n# Show the unique values of the `Python` column\nprint(filtered_df['Python'].unique())\n\n['Frequently' 'Sometimes' 'Always' 'Never' 'Rarely' nan]\n\n\n\n# Calculate the percentage of each response for each language\npercentage_df = filtered_df.apply(lambda x: x.value_counts(normalize=True).fillna(0) * 100).transpose()\n\n# Remove the last row, which is the \"Other\" category\npercentage_df = percentage_df[:-1]\n\n# Sort the DataFrame based on the 'Always' responses\nsorted_percentage_df = percentage_df.sort_values(by='Always', ascending=True)\n\n\n# Let's get ready to plot the 2022 data...\nfrom IPython.display import display\n\n# We are going to use the display command to update our figure over multiple cells. \n# This usually isn't necessary, but it's helpful here to see how each set of commands updates the figure\n\n# Define the custom order for plotting\norder = ['Always', 'Frequently', 'Sometimes', 'Rarely', 'Never']\n\ncolors = {\n    'Always': (8/255, 40/255, 81/255),       # Replace R1, G1, B1 with the RGB values for 'Dark Blue'\n    'Frequently': (12/255, 96/255, 152/255),   # Replace R2, G2, B2 with the RGB values for 'Light Ocean Blue'\n    'Sometimes': (16/255, 146/255, 136/255),    # and so on...\n    'Rarely': (11/255, 88/255, 73/255),\n    'Never': (52/255, 163/255, 32/255)\n}\n\n\n# Make the plot\nfig, ax = plt.subplots(figsize=(10, 7))\nsorted_percentage_df[order].plot(kind='barh', stacked=True, ax=ax, color=[colors[label] for label in order])\nax.set_xlabel('Percentage')\nax.set_title('Frequency of Language Usage, 2022',y=1.05)\n\nplt.show() # This command draws our figure. \n\n\n\n\n\n\n\n\n\n# Add labels across the top, like in the original graph\n\n# Get the patches for the top-most bar\nnum_languages = len(sorted_percentage_df)\n\npatches = ax.patches[num_languages-1::num_languages]\n# Calculate the cumulative width of the patches for the top-most bar\ncumulative_widths = [0] * len(order)\nwidths = [patch.get_width() for patch in patches]\nfor i, width in enumerate(widths):\n    cumulative_widths[i] = width + (cumulative_widths[i-1] if i &gt; 0 else 0)\n\n\n\n# Add text labels above the bars\nfor i, (width, label) in enumerate(zip(cumulative_widths, order)):\n    # Get the color of the current bar segment\n    # Calculate the position for the text label\n    position = width - (patches[i].get_width() / 2)\n    # Add the text label to the plot\n    # Adjust the y-coordinate for the text label\n    y_position = len(sorted_percentage_df) - 0.3  # Adjust the 0.3 value as needed\n    ax.text(position, y_position, label, ha='center', color=colors[label], fontweight='bold')\n\n# Remove the legend\nax.legend().set_visible(False)\n\n#plt.show()\ndisplay(fig) # This command shows our updated figure (we can't re-use \"plt.show()\")\n\n\n\n\n\n\n\n\n\n# Add percentage values inside each patch\nfor patch in ax.patches:\n    # Get the width and height of the patch\n    width, height = patch.get_width(), patch.get_height()\n    \n    # Calculate the position for the text label\n    x = patch.get_x() + width / 2\n    y = patch.get_y() + height / 2\n    \n    # Get the percentage value for the current patch\n    percentage = \"{:.0f}%\".format(width)\n    \n    # Add the text label to the plot\n    ax.text(x, y, percentage, ha='center', va='center', color='white', fontweight='bold')\n\ndisplay(fig) # Let's see those nice text labels!\n\n\n\n\n\n\n\n\n\n# Clean up the figure to remove spines and unecessary labels/ticks, etc..\n\n# Remove x-axis label\nax.set_xlabel('')\n\n# Remove the spines\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(False)\n\n# Remove the y-axis tick marks\nax.tick_params(axis='y', which='both', length=0)\n\n# Remove the x-axis tick marks and labels\nax.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n\ndisplay(fig) # Now 100% less visually cluttered!"
  },
  {
    "objectID": "course-materials/lectures/00_intro_to_python.html#the-end",
    "href": "course-materials/lectures/00_intro_to_python.html#the-end",
    "title": "Lecture 1 - Intro to Python and Environmental Data Science",
    "section": "The End",
    "text": "The End"
  },
  {
    "objectID": "course-materials/lectures/02_helpGPT.html",
    "href": "course-materials/lectures/02_helpGPT.html",
    "title": "Getting Help",
    "section": "",
    "text": "When you get an error, or an unexpected result, or you are not sure what to do…\n\n\n\nFinding help inside Python\nFinding help outside Python\n\n\n\n\nHow do we interrogate the data (and other objects) we encounter while coding?\n\nmy_var = 'some_unknown_thing'\nWhat is it?\n\nmy_var = 'some_unknown_thing'\ntype(my_var)\n\nstr\n\n\nThe type() command tells you what sort of thing an object is.\n\n\n\nHow do we interrogate the data (and other objects) we encounter while coding?\n\nmy_var = 'some_unknown_thing'\nWhat can I do with it?\n\nmy_var = ['my', 'list', 'of', 'things']\nmy_var = my_var + ['a', 'nother', 'list']\ndir(my_var)\n\n['__add__',\n '__class__',\n '__class_getitem__',\n '__contains__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__imul__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__reversed__',\n '__rmul__',\n '__setattr__',\n '__setitem__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'append',\n 'clear',\n 'copy',\n 'count',\n 'extend',\n 'index',\n 'insert',\n 'pop',\n 'remove',\n 'reverse',\n 'sort']\n\n\nThe dir() command tells you what attributes an object has.\n\n\n\n\n# using the dir command\nmy_list = ['a', 'b', 'c']\nlist(reversed(dir(my_list)))\nmy_list.sort?\n\nSignature: my_list.sort(*, key=None, reverse=False)\nDocstring:\nSort the list in ascending order and return None.\n\nThe sort is in-place (i.e. the list itself is modified) and stable (i.e. the\norder of two equal elements is maintained).\n\nIf a key function is given, apply it once to each list item and sort them,\nascending or descending, according to their function values.\n\nThe reverse flag can be set to sort in descending order.\nType:      builtin_function_or_method\n\n\n\n\n\n__attributes__ are internal (or private) attributes associated with all python objects.\nThese are called “magic” or “dunder” methods.\ndunder → “double under” → __\n\n\n\nEverything in Python is an object, and every operation corresponds to a method.\n\n# __add__ and __mul__. __len__. (None). 2 Wrongs.\n\n3 + 4\n\n7\n\n\n\n\n\nGenerally, you will not have to worry about dunder methods.\nHere’s a shortcut function to look at only non-dunder methods\n\n\n\n\n\nYou can use the &lt;tab&gt; key in iPython (or Jupyter environments) to explore object methods. By default, only “public” (non-dunder) methods are returned.\n\n\n\nYou can usually just pause typing and VSCode will provide object introspection:\n\nstring = 'some letters'\n\n\n\n\n\nMost objects - especially packages and libraries - provide help documentation that can be accessed using the python helper function… called… help()\n\n# 3, help, str, soil...\nimport math\nhelp(math)\n\nHelp on module math:\n\nNAME\n    math\n\nMODULE REFERENCE\n    https://docs.python.org/3.10/library/math.html\n    \n    The following documentation is automatically generated from the Python\n    source files.  It may be incomplete, incorrect or include features that\n    are considered implementation detail and may vary between Python\n    implementations.  When in doubt, consult the module reference at the\n    location listed above.\n\nDESCRIPTION\n    This module provides access to the mathematical functions\n    defined by the C standard.\n\nFUNCTIONS\n    acos(x, /)\n        Return the arc cosine (measured in radians) of x.\n        \n        The result is between 0 and pi.\n    \n    acosh(x, /)\n        Return the inverse hyperbolic cosine of x.\n    \n    asin(x, /)\n        Return the arc sine (measured in radians) of x.\n        \n        The result is between -pi/2 and pi/2.\n    \n    asinh(x, /)\n        Return the inverse hyperbolic sine of x.\n    \n    atan(x, /)\n        Return the arc tangent (measured in radians) of x.\n        \n        The result is between -pi/2 and pi/2.\n    \n    atan2(y, x, /)\n        Return the arc tangent (measured in radians) of y/x.\n        \n        Unlike atan(y/x), the signs of both x and y are considered.\n    \n    atanh(x, /)\n        Return the inverse hyperbolic tangent of x.\n    \n    ceil(x, /)\n        Return the ceiling of x as an Integral.\n        \n        This is the smallest integer &gt;= x.\n    \n    comb(n, k, /)\n        Number of ways to choose k items from n items without repetition and without order.\n        \n        Evaluates to n! / (k! * (n - k)!) when k &lt;= n and evaluates\n        to zero when k &gt; n.\n        \n        Also called the binomial coefficient because it is equivalent\n        to the coefficient of k-th term in polynomial expansion of the\n        expression (1 + x)**n.\n        \n        Raises TypeError if either of the arguments are not integers.\n        Raises ValueError if either of the arguments are negative.\n    \n    copysign(x, y, /)\n        Return a float with the magnitude (absolute value) of x but the sign of y.\n        \n        On platforms that support signed zeros, copysign(1.0, -0.0)\n        returns -1.0.\n    \n    cos(x, /)\n        Return the cosine of x (measured in radians).\n    \n    cosh(x, /)\n        Return the hyperbolic cosine of x.\n    \n    degrees(x, /)\n        Convert angle x from radians to degrees.\n    \n    dist(p, q, /)\n        Return the Euclidean distance between two points p and q.\n        \n        The points should be specified as sequences (or iterables) of\n        coordinates.  Both inputs must have the same dimension.\n        \n        Roughly equivalent to:\n            sqrt(sum((px - qx) ** 2.0 for px, qx in zip(p, q)))\n    \n    erf(x, /)\n        Error function at x.\n    \n    erfc(x, /)\n        Complementary error function at x.\n    \n    exp(x, /)\n        Return e raised to the power of x.\n    \n    expm1(x, /)\n        Return exp(x)-1.\n        \n        This function avoids the loss of precision involved in the direct evaluation of exp(x)-1 for small x.\n    \n    fabs(x, /)\n        Return the absolute value of the float x.\n    \n    factorial(x, /)\n        Find x!.\n        \n        Raise a ValueError if x is negative or non-integral.\n    \n    floor(x, /)\n        Return the floor of x as an Integral.\n        \n        This is the largest integer &lt;= x.\n    \n    fmod(x, y, /)\n        Return fmod(x, y), according to platform C.\n        \n        x % y may differ.\n    \n    frexp(x, /)\n        Return the mantissa and exponent of x, as pair (m, e).\n        \n        m is a float and e is an int, such that x = m * 2.**e.\n        If x is 0, m and e are both 0.  Else 0.5 &lt;= abs(m) &lt; 1.0.\n    \n    fsum(seq, /)\n        Return an accurate floating point sum of values in the iterable seq.\n        \n        Assumes IEEE-754 floating point arithmetic.\n    \n    gamma(x, /)\n        Gamma function at x.\n    \n    gcd(*integers)\n        Greatest Common Divisor.\n    \n    hypot(...)\n        hypot(*coordinates) -&gt; value\n        \n        Multidimensional Euclidean distance from the origin to a point.\n        \n        Roughly equivalent to:\n            sqrt(sum(x**2 for x in coordinates))\n        \n        For a two dimensional point (x, y), gives the hypotenuse\n        using the Pythagorean theorem:  sqrt(x*x + y*y).\n        \n        For example, the hypotenuse of a 3/4/5 right triangle is:\n        \n            &gt;&gt;&gt; hypot(3.0, 4.0)\n            5.0\n    \n    isclose(a, b, *, rel_tol=1e-09, abs_tol=0.0)\n        Determine whether two floating point numbers are close in value.\n        \n          rel_tol\n            maximum difference for being considered \"close\", relative to the\n            magnitude of the input values\n          abs_tol\n            maximum difference for being considered \"close\", regardless of the\n            magnitude of the input values\n        \n        Return True if a is close in value to b, and False otherwise.\n        \n        For the values to be considered close, the difference between them\n        must be smaller than at least one of the tolerances.\n        \n        -inf, inf and NaN behave similarly to the IEEE 754 Standard.  That\n        is, NaN is not close to anything, even itself.  inf and -inf are\n        only close to themselves.\n    \n    isfinite(x, /)\n        Return True if x is neither an infinity nor a NaN, and False otherwise.\n    \n    isinf(x, /)\n        Return True if x is a positive or negative infinity, and False otherwise.\n    \n    isnan(x, /)\n        Return True if x is a NaN (not a number), and False otherwise.\n    \n    isqrt(n, /)\n        Return the integer part of the square root of the input.\n    \n    lcm(*integers)\n        Least Common Multiple.\n    \n    ldexp(x, i, /)\n        Return x * (2**i).\n        \n        This is essentially the inverse of frexp().\n    \n    lgamma(x, /)\n        Natural logarithm of absolute value of Gamma function at x.\n    \n    log(...)\n        log(x, [base=math.e])\n        Return the logarithm of x to the given base.\n        \n        If the base not specified, returns the natural logarithm (base e) of x.\n    \n    log10(x, /)\n        Return the base 10 logarithm of x.\n    \n    log1p(x, /)\n        Return the natural logarithm of 1+x (base e).\n        \n        The result is computed in a way which is accurate for x near zero.\n    \n    log2(x, /)\n        Return the base 2 logarithm of x.\n    \n    modf(x, /)\n        Return the fractional and integer parts of x.\n        \n        Both results carry the sign of x and are floats.\n    \n    nextafter(x, y, /)\n        Return the next floating-point value after x towards y.\n    \n    perm(n, k=None, /)\n        Number of ways to choose k items from n items without repetition and with order.\n        \n        Evaluates to n! / (n - k)! when k &lt;= n and evaluates\n        to zero when k &gt; n.\n        \n        If k is not specified or is None, then k defaults to n\n        and the function returns n!.\n        \n        Raises TypeError if either of the arguments are not integers.\n        Raises ValueError if either of the arguments are negative.\n    \n    pow(x, y, /)\n        Return x**y (x to the power of y).\n    \n    prod(iterable, /, *, start=1)\n        Calculate the product of all the elements in the input iterable.\n        \n        The default start value for the product is 1.\n        \n        When the iterable is empty, return the start value.  This function is\n        intended specifically for use with numeric values and may reject\n        non-numeric types.\n    \n    radians(x, /)\n        Convert angle x from degrees to radians.\n    \n    remainder(x, y, /)\n        Difference between x and the closest integer multiple of y.\n        \n        Return x - n*y where n*y is the closest integer multiple of y.\n        In the case where x is exactly halfway between two multiples of\n        y, the nearest even value of n is used. The result is always exact.\n    \n    sin(x, /)\n        Return the sine of x (measured in radians).\n    \n    sinh(x, /)\n        Return the hyperbolic sine of x.\n    \n    sqrt(x, /)\n        Return the square root of x.\n    \n    tan(x, /)\n        Return the tangent of x (measured in radians).\n    \n    tanh(x, /)\n        Return the hyperbolic tangent of x.\n    \n    trunc(x, /)\n        Truncates the Real x to the nearest Integral toward 0.\n        \n        Uses the __trunc__ magic method.\n    \n    ulp(x, /)\n        Return the value of the least significant bit of the float x.\n\nDATA\n    e = 2.718281828459045\n    inf = inf\n    nan = nan\n    pi = 3.141592653589793\n    tau = 6.283185307179586\n\nFILE\n    /Users/kellycaylor/mambaforge/envs/eds217_2023/lib/python3.10/lib-dynload/math.cpython-310-darwin.so\n\n\n\n\n\n\n\nIn the iPython shell (or the Jupyter Notebook/Jupyter Lab environment), you can also access the help() command using ?.\n\nmath\n\n&lt;module 'math' from '/Users/kellycaylor/mambaforge/envs/eds217_2023/lib/python3.10/lib-dynload/math.cpython-310-darwin.so'&gt;\n\n\n\n\n\nIn the iPython shell (or the Jupyter Notebook/Jupyter Lab environment) you can use ?? to see the actual source code of python code\n\n\n\n?? only shows source code for for python functions that aren’t compiled to C code. Otherwise, it will show the same information as ?\n\n\n\n\n\n\n\n\n\n\nThe print command is the most commonly used tool for beginners to understand errors\n# This code generates a `TypeError` that \n# x is not the right kind of variable.\ndo_something(x) \nThe print command is the most commonly used debugging tool for beginners.\n\n\n\nPython has a string format called f-strings. These are strings that are prefixed with an f character and allow in-line variable substitution.\n\n# print using c-style format statements\nx = 3.45\nprint(f\"x = {x}\")\n\nx = 3.45\n\n\n\ndef do_something(x):\n    x = x / 2 \n    return x\n\n# This code generates a `TypeError` that \n# x is not the right kind of variable.\nx = 'f'\n# Check and see what is X?\nprint(\n    f\"calling do_something() with x={x}\" # Python f-string\n)\n\ndo_something(x) \n\ncalling do_something() with x=f\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[13], line 13\n      8 # Check and see what is X?\n      9 print(\n     10     f\"calling do_something() with x={x}\" # Python f-string\n     11 )\n---&gt; 13 do_something(x)\n\nCell In[13], line 2, in do_something(x)\n      1 def do_something(x):\n----&gt; 2     x = x / 2 \n      3     return x\n\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n\n\n\n\n\n\nAs of Fall 2002: - O’Rielly Books (Requires UCSB login) - My O’Rielly pdf library: https://bit.ly/eds-217-books (Requires UCSB login)\nAs of Fall, 2022: - Python Docs - Stack Overflow - Talk Python - Ask Python\nAs of Fall, 2023:\nLLMs.\n\nChatGPT - Need $ for GPT-4, 3.X fine debugger, but not always a great programmer.\nGitHub CoPilot - Should be able to get a free student account. Works great in VSCode; we will set this up together later in the course."
  },
  {
    "objectID": "course-materials/lectures/02_helpGPT.html#finding-help",
    "href": "course-materials/lectures/02_helpGPT.html#finding-help",
    "title": "Getting Help",
    "section": "",
    "text": "When you get an error, or an unexpected result, or you are not sure what to do…\n\n\n\nFinding help inside Python\nFinding help outside Python\n\n\n\n\nHow do we interrogate the data (and other objects) we encounter while coding?\n\nmy_var = 'some_unknown_thing'\nWhat is it?\n\nmy_var = 'some_unknown_thing'\ntype(my_var)\n\nstr\n\n\nThe type() command tells you what sort of thing an object is.\n\n\n\nHow do we interrogate the data (and other objects) we encounter while coding?\n\nmy_var = 'some_unknown_thing'\nWhat can I do with it?\n\nmy_var = ['my', 'list', 'of', 'things']\nmy_var = my_var + ['a', 'nother', 'list']\ndir(my_var)\n\n['__add__',\n '__class__',\n '__class_getitem__',\n '__contains__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__imul__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__reversed__',\n '__rmul__',\n '__setattr__',\n '__setitem__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'append',\n 'clear',\n 'copy',\n 'count',\n 'extend',\n 'index',\n 'insert',\n 'pop',\n 'remove',\n 'reverse',\n 'sort']\n\n\nThe dir() command tells you what attributes an object has.\n\n\n\n\n# using the dir command\nmy_list = ['a', 'b', 'c']\nlist(reversed(dir(my_list)))\nmy_list.sort?\n\nSignature: my_list.sort(*, key=None, reverse=False)\nDocstring:\nSort the list in ascending order and return None.\n\nThe sort is in-place (i.e. the list itself is modified) and stable (i.e. the\norder of two equal elements is maintained).\n\nIf a key function is given, apply it once to each list item and sort them,\nascending or descending, according to their function values.\n\nThe reverse flag can be set to sort in descending order.\nType:      builtin_function_or_method\n\n\n\n\n\n__attributes__ are internal (or private) attributes associated with all python objects.\nThese are called “magic” or “dunder” methods.\ndunder → “double under” → __\n\n\n\nEverything in Python is an object, and every operation corresponds to a method.\n\n# __add__ and __mul__. __len__. (None). 2 Wrongs.\n\n3 + 4\n\n7\n\n\n\n\n\nGenerally, you will not have to worry about dunder methods.\nHere’s a shortcut function to look at only non-dunder methods\n\n\n\n\n\nYou can use the &lt;tab&gt; key in iPython (or Jupyter environments) to explore object methods. By default, only “public” (non-dunder) methods are returned.\n\n\n\nYou can usually just pause typing and VSCode will provide object introspection:\n\nstring = 'some letters'\n\n\n\n\n\nMost objects - especially packages and libraries - provide help documentation that can be accessed using the python helper function… called… help()\n\n# 3, help, str, soil...\nimport math\nhelp(math)\n\nHelp on module math:\n\nNAME\n    math\n\nMODULE REFERENCE\n    https://docs.python.org/3.10/library/math.html\n    \n    The following documentation is automatically generated from the Python\n    source files.  It may be incomplete, incorrect or include features that\n    are considered implementation detail and may vary between Python\n    implementations.  When in doubt, consult the module reference at the\n    location listed above.\n\nDESCRIPTION\n    This module provides access to the mathematical functions\n    defined by the C standard.\n\nFUNCTIONS\n    acos(x, /)\n        Return the arc cosine (measured in radians) of x.\n        \n        The result is between 0 and pi.\n    \n    acosh(x, /)\n        Return the inverse hyperbolic cosine of x.\n    \n    asin(x, /)\n        Return the arc sine (measured in radians) of x.\n        \n        The result is between -pi/2 and pi/2.\n    \n    asinh(x, /)\n        Return the inverse hyperbolic sine of x.\n    \n    atan(x, /)\n        Return the arc tangent (measured in radians) of x.\n        \n        The result is between -pi/2 and pi/2.\n    \n    atan2(y, x, /)\n        Return the arc tangent (measured in radians) of y/x.\n        \n        Unlike atan(y/x), the signs of both x and y are considered.\n    \n    atanh(x, /)\n        Return the inverse hyperbolic tangent of x.\n    \n    ceil(x, /)\n        Return the ceiling of x as an Integral.\n        \n        This is the smallest integer &gt;= x.\n    \n    comb(n, k, /)\n        Number of ways to choose k items from n items without repetition and without order.\n        \n        Evaluates to n! / (k! * (n - k)!) when k &lt;= n and evaluates\n        to zero when k &gt; n.\n        \n        Also called the binomial coefficient because it is equivalent\n        to the coefficient of k-th term in polynomial expansion of the\n        expression (1 + x)**n.\n        \n        Raises TypeError if either of the arguments are not integers.\n        Raises ValueError if either of the arguments are negative.\n    \n    copysign(x, y, /)\n        Return a float with the magnitude (absolute value) of x but the sign of y.\n        \n        On platforms that support signed zeros, copysign(1.0, -0.0)\n        returns -1.0.\n    \n    cos(x, /)\n        Return the cosine of x (measured in radians).\n    \n    cosh(x, /)\n        Return the hyperbolic cosine of x.\n    \n    degrees(x, /)\n        Convert angle x from radians to degrees.\n    \n    dist(p, q, /)\n        Return the Euclidean distance between two points p and q.\n        \n        The points should be specified as sequences (or iterables) of\n        coordinates.  Both inputs must have the same dimension.\n        \n        Roughly equivalent to:\n            sqrt(sum((px - qx) ** 2.0 for px, qx in zip(p, q)))\n    \n    erf(x, /)\n        Error function at x.\n    \n    erfc(x, /)\n        Complementary error function at x.\n    \n    exp(x, /)\n        Return e raised to the power of x.\n    \n    expm1(x, /)\n        Return exp(x)-1.\n        \n        This function avoids the loss of precision involved in the direct evaluation of exp(x)-1 for small x.\n    \n    fabs(x, /)\n        Return the absolute value of the float x.\n    \n    factorial(x, /)\n        Find x!.\n        \n        Raise a ValueError if x is negative or non-integral.\n    \n    floor(x, /)\n        Return the floor of x as an Integral.\n        \n        This is the largest integer &lt;= x.\n    \n    fmod(x, y, /)\n        Return fmod(x, y), according to platform C.\n        \n        x % y may differ.\n    \n    frexp(x, /)\n        Return the mantissa and exponent of x, as pair (m, e).\n        \n        m is a float and e is an int, such that x = m * 2.**e.\n        If x is 0, m and e are both 0.  Else 0.5 &lt;= abs(m) &lt; 1.0.\n    \n    fsum(seq, /)\n        Return an accurate floating point sum of values in the iterable seq.\n        \n        Assumes IEEE-754 floating point arithmetic.\n    \n    gamma(x, /)\n        Gamma function at x.\n    \n    gcd(*integers)\n        Greatest Common Divisor.\n    \n    hypot(...)\n        hypot(*coordinates) -&gt; value\n        \n        Multidimensional Euclidean distance from the origin to a point.\n        \n        Roughly equivalent to:\n            sqrt(sum(x**2 for x in coordinates))\n        \n        For a two dimensional point (x, y), gives the hypotenuse\n        using the Pythagorean theorem:  sqrt(x*x + y*y).\n        \n        For example, the hypotenuse of a 3/4/5 right triangle is:\n        \n            &gt;&gt;&gt; hypot(3.0, 4.0)\n            5.0\n    \n    isclose(a, b, *, rel_tol=1e-09, abs_tol=0.0)\n        Determine whether two floating point numbers are close in value.\n        \n          rel_tol\n            maximum difference for being considered \"close\", relative to the\n            magnitude of the input values\n          abs_tol\n            maximum difference for being considered \"close\", regardless of the\n            magnitude of the input values\n        \n        Return True if a is close in value to b, and False otherwise.\n        \n        For the values to be considered close, the difference between them\n        must be smaller than at least one of the tolerances.\n        \n        -inf, inf and NaN behave similarly to the IEEE 754 Standard.  That\n        is, NaN is not close to anything, even itself.  inf and -inf are\n        only close to themselves.\n    \n    isfinite(x, /)\n        Return True if x is neither an infinity nor a NaN, and False otherwise.\n    \n    isinf(x, /)\n        Return True if x is a positive or negative infinity, and False otherwise.\n    \n    isnan(x, /)\n        Return True if x is a NaN (not a number), and False otherwise.\n    \n    isqrt(n, /)\n        Return the integer part of the square root of the input.\n    \n    lcm(*integers)\n        Least Common Multiple.\n    \n    ldexp(x, i, /)\n        Return x * (2**i).\n        \n        This is essentially the inverse of frexp().\n    \n    lgamma(x, /)\n        Natural logarithm of absolute value of Gamma function at x.\n    \n    log(...)\n        log(x, [base=math.e])\n        Return the logarithm of x to the given base.\n        \n        If the base not specified, returns the natural logarithm (base e) of x.\n    \n    log10(x, /)\n        Return the base 10 logarithm of x.\n    \n    log1p(x, /)\n        Return the natural logarithm of 1+x (base e).\n        \n        The result is computed in a way which is accurate for x near zero.\n    \n    log2(x, /)\n        Return the base 2 logarithm of x.\n    \n    modf(x, /)\n        Return the fractional and integer parts of x.\n        \n        Both results carry the sign of x and are floats.\n    \n    nextafter(x, y, /)\n        Return the next floating-point value after x towards y.\n    \n    perm(n, k=None, /)\n        Number of ways to choose k items from n items without repetition and with order.\n        \n        Evaluates to n! / (n - k)! when k &lt;= n and evaluates\n        to zero when k &gt; n.\n        \n        If k is not specified or is None, then k defaults to n\n        and the function returns n!.\n        \n        Raises TypeError if either of the arguments are not integers.\n        Raises ValueError if either of the arguments are negative.\n    \n    pow(x, y, /)\n        Return x**y (x to the power of y).\n    \n    prod(iterable, /, *, start=1)\n        Calculate the product of all the elements in the input iterable.\n        \n        The default start value for the product is 1.\n        \n        When the iterable is empty, return the start value.  This function is\n        intended specifically for use with numeric values and may reject\n        non-numeric types.\n    \n    radians(x, /)\n        Convert angle x from degrees to radians.\n    \n    remainder(x, y, /)\n        Difference between x and the closest integer multiple of y.\n        \n        Return x - n*y where n*y is the closest integer multiple of y.\n        In the case where x is exactly halfway between two multiples of\n        y, the nearest even value of n is used. The result is always exact.\n    \n    sin(x, /)\n        Return the sine of x (measured in radians).\n    \n    sinh(x, /)\n        Return the hyperbolic sine of x.\n    \n    sqrt(x, /)\n        Return the square root of x.\n    \n    tan(x, /)\n        Return the tangent of x (measured in radians).\n    \n    tanh(x, /)\n        Return the hyperbolic tangent of x.\n    \n    trunc(x, /)\n        Truncates the Real x to the nearest Integral toward 0.\n        \n        Uses the __trunc__ magic method.\n    \n    ulp(x, /)\n        Return the value of the least significant bit of the float x.\n\nDATA\n    e = 2.718281828459045\n    inf = inf\n    nan = nan\n    pi = 3.141592653589793\n    tau = 6.283185307179586\n\nFILE\n    /Users/kellycaylor/mambaforge/envs/eds217_2023/lib/python3.10/lib-dynload/math.cpython-310-darwin.so\n\n\n\n\n\n\n\nIn the iPython shell (or the Jupyter Notebook/Jupyter Lab environment), you can also access the help() command using ?.\n\nmath\n\n&lt;module 'math' from '/Users/kellycaylor/mambaforge/envs/eds217_2023/lib/python3.10/lib-dynload/math.cpython-310-darwin.so'&gt;\n\n\n\n\n\nIn the iPython shell (or the Jupyter Notebook/Jupyter Lab environment) you can use ?? to see the actual source code of python code\n\n\n\n?? only shows source code for for python functions that aren’t compiled to C code. Otherwise, it will show the same information as ?\n\n\n\n\n\n\n\n\n\n\nThe print command is the most commonly used tool for beginners to understand errors\n# This code generates a `TypeError` that \n# x is not the right kind of variable.\ndo_something(x) \nThe print command is the most commonly used debugging tool for beginners.\n\n\n\nPython has a string format called f-strings. These are strings that are prefixed with an f character and allow in-line variable substitution.\n\n# print using c-style format statements\nx = 3.45\nprint(f\"x = {x}\")\n\nx = 3.45\n\n\n\ndef do_something(x):\n    x = x / 2 \n    return x\n\n# This code generates a `TypeError` that \n# x is not the right kind of variable.\nx = 'f'\n# Check and see what is X?\nprint(\n    f\"calling do_something() with x={x}\" # Python f-string\n)\n\ndo_something(x) \n\ncalling do_something() with x=f\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[13], line 13\n      8 # Check and see what is X?\n      9 print(\n     10     f\"calling do_something() with x={x}\" # Python f-string\n     11 )\n---&gt; 13 do_something(x)\n\nCell In[13], line 2, in do_something(x)\n      1 def do_something(x):\n----&gt; 2     x = x / 2 \n      3     return x\n\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n\n\n\n\n\n\nAs of Fall 2002: - O’Rielly Books (Requires UCSB login) - My O’Rielly pdf library: https://bit.ly/eds-217-books (Requires UCSB login)\nAs of Fall, 2022: - Python Docs - Stack Overflow - Talk Python - Ask Python\nAs of Fall, 2023:\nLLMs.\n\nChatGPT - Need $ for GPT-4, 3.X fine debugger, but not always a great programmer.\nGitHub CoPilot - Should be able to get a free student account. Works great in VSCode; we will set this up together later in the course."
  },
  {
    "objectID": "course-materials/lectures/02_helpGPT.html#how-to-move-from-a-beginner-to-a-more-advanced-python-user",
    "href": "course-materials/lectures/02_helpGPT.html#how-to-move-from-a-beginner-to-a-more-advanced-python-user",
    "title": "Getting Help",
    "section": "How to move from a beginner to a more `advanced`` python user",
    "text": "How to move from a beginner to a more `advanced`` python user\nTaken from Talk Python to Me, Episode #427, with some modifications."
  },
  {
    "objectID": "course-materials/lectures/02_helpGPT.html#know-your-goals",
    "href": "course-materials/lectures/02_helpGPT.html#know-your-goals",
    "title": "Getting Help",
    "section": "Know your goals",
    "text": "Know your goals\n\nWhy are you learning python?\nWhy are you learning data science?"
  },
  {
    "objectID": "course-materials/lectures/02_helpGPT.html#have-a-project-in-mind",
    "href": "course-materials/lectures/02_helpGPT.html#have-a-project-in-mind",
    "title": "Getting Help",
    "section": "Have a project in mind",
    "text": "Have a project in mind\n\nWhat do you want to do with it?\nUse python to solve a problem you are interested in solving.\nDon’t be afraid to work on personal projects.\n\n\nSome examples of my personal “problem-solving” projects\nBiobib - Python code to make my CV/Biobib from a google sheets/.csv file.\nTriumph - Python notebooks for a 1959 Triumph TR3A EV conversion project.\nStoplight - A simple python webapp for monitoring EDS217 course pace."
  },
  {
    "objectID": "course-materials/lectures/02_helpGPT.html#dont-limit-your-learning-to-whats-needed-for-your-project",
    "href": "course-materials/lectures/02_helpGPT.html#dont-limit-your-learning-to-whats-needed-for-your-project",
    "title": "Getting Help",
    "section": "Don’t limit your learning to what’s needed for your project",
    "text": "Don’t limit your learning to what’s needed for your project\n\nLearn more than you need to know…\n…but try to use less than you think you need"
  },
  {
    "objectID": "course-materials/lectures/02_helpGPT.html#read-good-code",
    "href": "course-materials/lectures/02_helpGPT.html#read-good-code",
    "title": "Getting Help",
    "section": "Read good code",
    "text": "Read good code\n\nLibraries and packages have great examples of code.\nRead the code (not just docs) of the packages you use.\nGithub is a great place to find code."
  },
  {
    "objectID": "course-materials/lectures/02_helpGPT.html#know-your-tools",
    "href": "course-materials/lectures/02_helpGPT.html#know-your-tools",
    "title": "Getting Help",
    "section": "Know your tools",
    "text": "Know your tools\n\nLearn how to use your IDE (VSCode)\nLearn how to use your package manager (conda)\nLearn how to use your shell (bash)\nLearn how to use your version control system (git)\n\n\nLearn how to test your code\n\nTesting is part of programming.\nTesting is a great way to learn.\nFocus on end-to-end (E2E) tests (rather than unit tests)\n\nUnit tests:\nDoes it work the way you expect it to (operation-centric)?\nEnd-to-end test:\nDoes it do what you want it to do (output-centric)?"
  },
  {
    "objectID": "course-materials/lectures/02_helpGPT.html#know-whats-good-enough-for-any-given-project",
    "href": "course-materials/lectures/02_helpGPT.html#know-whats-good-enough-for-any-given-project",
    "title": "Getting Help",
    "section": "Know what’s good enough for any given project",
    "text": "Know what’s good enough for any given project\n\nYou’re not writing code for a self-driving car or a pacemaker.\n\nDon’t over-engineer your code.\nDon’t over-optimize your code.\nSimple is better than complex."
  },
  {
    "objectID": "course-materials/lectures/02_helpGPT.html#embrace-refactoring",
    "href": "course-materials/lectures/02_helpGPT.html#embrace-refactoring",
    "title": "Getting Help",
    "section": "Embrace refactoring",
    "text": "Embrace refactoring\nRefactoring is the process of changing your code without changing its behavior.\n\nShip of Theseus: If you replace every part of a ship, is it still the same ship?\n\n\nAs you learn more, you will find better ways to do things.\nDon’t be afraid to change your code.\nTests (especially end-to-end tests) help you refactor with confidence.\n“Code smells”… if it smells bad, it probably is bad.\n\nCode Smells\nComments can be a code smell; they can be a sign that your code is not clear enough."
  },
  {
    "objectID": "course-materials/lectures/02_helpGPT.html#write-things-down",
    "href": "course-materials/lectures/02_helpGPT.html#write-things-down",
    "title": "Getting Help",
    "section": "Write things down",
    "text": "Write things down\n\nKeep an ideas notebook\n\nWrite down ideas for projects\nWrite down ideas for code\n\n\n\nWrite comments to yourself and others\n\n\nWrite documentation\n\n\nWrite down questions (use your tools; github issues, etc.)"
  },
  {
    "objectID": "course-materials/lectures/02_helpGPT.html#go-meet-people",
    "href": "course-materials/lectures/02_helpGPT.html#go-meet-people",
    "title": "Getting Help",
    "section": "Go meet people!",
    "text": "Go meet people!\n\nThe Python (and Data Science) community is great!\n\nGo to Python & Data Science meetups.\n\nCentral Coast Python\n\n\n\nGo to python and data science conferences.\n\nPyCon 2024 & 2025 will be in Pittsburgh, PA\nPyData (Conferences all over the world)\n\n\n\nGo to hackathons.\n\nSB Hacks (UCSB)\nMLH (Major League Hacking)\nHackathon.com (Hackathons all over the world)"
  },
  {
    "objectID": "course-materials/lectures/04-next_steps.html",
    "href": "course-materials/lectures/04-next_steps.html",
    "title": "How to move from a beginner to a more advanced python user",
    "section": "",
    "text": "Taken from Talk Python to Me, Episode #427, with some modifications."
  },
  {
    "objectID": "course-materials/lectures/04-next_steps.html#know-your-goals",
    "href": "course-materials/lectures/04-next_steps.html#know-your-goals",
    "title": "How to move from a beginner to a more advanced python user",
    "section": "1. Know your goals",
    "text": "1. Know your goals\n\nWhy are you learning python?\nWhy are you learning data science?"
  },
  {
    "objectID": "course-materials/lectures/04-next_steps.html#have-a-project-in-mind",
    "href": "course-materials/lectures/04-next_steps.html#have-a-project-in-mind",
    "title": "How to move from a beginner to a more advanced python user",
    "section": "2. Have a project in mind",
    "text": "2. Have a project in mind\n\nWhat do you want to do with it?\nUse python to solve a problem you are interested in solving.\nDon’t be afraid to work on personal projects.\n\n\nSome examples of my personal “problem-solving” projects\nBiobib - Python code to make my CV/Biobib from a google sheets/.csv file.\nTriumph - Python notebooks for a 1959 Triumph TR3A EV conversion project.\nStoplight - A simple python webapp for monitoring EDS217 course pace."
  },
  {
    "objectID": "course-materials/lectures/04-next_steps.html#dont-limit-your-learning-to-whats-needed-for-your-project",
    "href": "course-materials/lectures/04-next_steps.html#dont-limit-your-learning-to-whats-needed-for-your-project",
    "title": "How to move from a beginner to a more advanced python user",
    "section": "3. Don’t limit your learning to what’s needed for your project",
    "text": "3. Don’t limit your learning to what’s needed for your project\n\nLearn more than you need to know…\nMath: 3Blue1Brown\nPython Data Science: PyData\nData Visualization: Edward Tufte, Cole Nussbaumer-Knaflic, David McCandless\nBe curious about what’s possible, not just what’s necessary.\n…but try to use less than you think you need"
  },
  {
    "objectID": "course-materials/lectures/04-next_steps.html#read-good-code",
    "href": "course-materials/lectures/04-next_steps.html#read-good-code",
    "title": "How to move from a beginner to a more advanced python user",
    "section": "4. Read good code",
    "text": "4. Read good code\n\nLibraries and packages have great examples of code!\nRead the code (not just docs) of the packages you use.\n\nIt’s okay if you can’t understand it all. Often you can understand intent, but not what the code does. How would you have done it? Why did the author select a different approach?\n\nGithub is a great place to find code."
  },
  {
    "objectID": "course-materials/lectures/04-next_steps.html#know-your-tools",
    "href": "course-materials/lectures/04-next_steps.html#know-your-tools",
    "title": "How to move from a beginner to a more advanced python user",
    "section": "5. Know your tools",
    "text": "5. Know your tools\n\nLearn how to use your IDE (VSCode)\nLearn how to use your package manager (conda, mamba)\nLearn how to use your shell (bash, powershell, WSL)\nLearn how to use your version control system (git, Github Desktop)"
  },
  {
    "objectID": "course-materials/lectures/04-next_steps.html#learn-how-to-test-your-code",
    "href": "course-materials/lectures/04-next_steps.html#learn-how-to-test-your-code",
    "title": "How to move from a beginner to a more advanced python user",
    "section": "6. Learn how to test your code",
    "text": "6. Learn how to test your code\n\nTesting code is part of writing code, and testing is a great way to learn!\nFocus on end-to-end (E2E) tests (rather than unit tests)\n\nUnit tests:\nDoes it work the way you expect it to (operation-centric)?\nEnd-to-end test:\nDoes it do what you want it to do (output-centric)?\n\n\nTesting for data science\nTesting with PyTest for data science"
  },
  {
    "objectID": "course-materials/lectures/04-next_steps.html#know-whats-good-enough-for-any-given-project",
    "href": "course-materials/lectures/04-next_steps.html#know-whats-good-enough-for-any-given-project",
    "title": "How to move from a beginner to a more advanced python user",
    "section": "7. Know what’s good enough for any given project",
    "text": "7. Know what’s good enough for any given project\n\nYou’re not writing code for a self-driving car or a pacemaker.\n\nDon’t over-engineer your code.\nDon’t over-optimize your code.\nSimple is better than complex."
  },
  {
    "objectID": "course-materials/lectures/04-next_steps.html#embrace-refactoring",
    "href": "course-materials/lectures/04-next_steps.html#embrace-refactoring",
    "title": "How to move from a beginner to a more advanced python user",
    "section": "8. Embrace refactoring",
    "text": "8. Embrace refactoring\nRefactoring is the process of changing your code without changing its behavior.\n\nShip of Theseus: If you replace every part of a ship, is it still the same ship?\n\n\nAs you learn more, you will find better ways to do things.\nDon’t be afraid to change your code.\nTests (especially end-to-end tests) help you refactor with confidence.\n“Code smells”… if it smells bad, it probably is bad.\n\nCode Smells\nComments can be a code smell; they can be a sign that your code is not clear enough."
  },
  {
    "objectID": "course-materials/lectures/04-next_steps.html#write-things-down",
    "href": "course-materials/lectures/04-next_steps.html#write-things-down",
    "title": "How to move from a beginner to a more advanced python user",
    "section": "9. Write things down",
    "text": "9. Write things down\n\nKeep an ideas notebook\n\nWrite down ideas for projects\nWrite down ideas for code\n\n\n\nWrite comments to yourself and others\n\n\nWrite documentation\n\nCode Documentation in Python\n\n\n\nWrite down questions (use your tools; github issues, etc…)"
  },
  {
    "objectID": "course-materials/lectures/04-next_steps.html#go-meet-people",
    "href": "course-materials/lectures/04-next_steps.html#go-meet-people",
    "title": "How to move from a beginner to a more advanced python user",
    "section": "10. Go meet people!",
    "text": "10. Go meet people!\n\nThe Python (and Data Science) community is great!\n\nGo to Python & Data Science meetups.\n\nCentral Coast Python\n\n\n\nGo to python and data science conferences.\n\nPyCon 2024 & 2025 will be in Pittsburgh, PA\nPyData (Conferences all over the world)\n\n\n\nGo to hackathons.\n\nSB Hacks (UCSB)\nMLH (Major League Hacking)\nHackathon.com (Hackathons all over the world)"
  },
  {
    "objectID": "course-materials/lectures/lectures.html",
    "href": "course-materials/lectures/lectures.html",
    "title": "EDS 217 Lectures",
    "section": "",
    "text": "This page contains links to lecture materials for EDS 217.\nNOTE: Not all lectures are given during the class.\n\n🐍 Introduction to Python Data Science\n🐍 The Zen of Python\n🐍 Types of Data Types\n🐍 Next Steps in Learning Python\n🐍 What the Python?: Tradeoffs and the Triple Dillema\n🐼 A 9-Step Pandas Data Science Workflow\n🐼 Data Decisions: Drop or Impute?\n🐼 Introduction to Seaborn\n💪 Code Hygiene: Debugging\n💪 Code Hygiene: Dry vs. Wet Code"
  },
  {
    "objectID": "course-materials/cheatsheets/data_aggregation.html",
    "href": "course-materials/cheatsheets/data_aggregation.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "To be added"
  },
  {
    "objectID": "course-materials/cheatsheets/data_merging.html",
    "href": "course-materials/cheatsheets/data_merging.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "To be added"
  },
  {
    "objectID": "course-materials/cheatsheets/lists.html",
    "href": "course-materials/cheatsheets/lists.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "my_list = []\n\n\n\nmy_list = [1, 2, 3, 4, 5]\n\n\n\nmixed_list = [1, \"hello\", 3.14, True]\n\n\n\nnested_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n\n\n\n\n\nmy_list = [10, 20, 30, 40]\nprint(my_list[0])  # Output: 10\nprint(my_list[2])  # Output: 30\n\n\n\nprint(my_list[-1])  # Output: 40\n\n\n\nsublist = my_list[1:3]  # Output: [20, 30]\n\n\n\n\n\n\nmy_list[1] = 25  # my_list becomes [10, 25, 30, 40]\n\n\n\nmy_list.append(50)  # my_list becomes [10, 25, 30, 40, 50]\n\n\n\nmy_list.insert(1, 15)  # my_list becomes [10, 15, 25, 30, 40, 50]\n\n\n\nmy_list.extend([60, 70])  # my_list becomes [10, 15, 25, 30, 40, 50, 60, 70]\n\n\n\n\n\n\nmy_list.remove(25)  # my_list becomes [10, 15, 30, 40, 50, 60, 70]\n\n\n\ndel my_list[0]  # my_list becomes [15, 30, 40, 50, 60, 70]\n\n\n\nlast_element = my_list.pop()  # my_list becomes [15, 30, 40, 50, 60]\n\n\n\nelement = my_list.pop(2)  # my_list becomes [15, 30, 50, 60]\n\n\n\n\n\n\nlength = len(my_list)  # Output: 4\n\n\n\nis_in_list = 30 in my_list  # Output: True\n\n\n\ncombined_list = my_list + [80, 90]  # Output: [15, 30, 50, 60, 80, 90]\n\n\n\nrepeated_list = [1, 2, 3] * 3  # Output: [1, 2, 3, 1, 2, 3, 1, 2, 3]\n\n\n\n\n\n\nfor item in my_list:\n    print(item)\n\n\n\nfor index, value in enumerate(my_list):\n    print(f\"Index {index} has value {value}\")\n\n\n\n\n\n\nsquares = [x**2 for x in range(5)]  # Output: [0, 1, 4, 9, 16]\n\n\n\nevens = [x for x in range(10) if x % 2 == 0]  # Output: [0, 2, 4, 6, 8]\n\n\n\n\n\n\nmy_list.sort()  # Sorts in place\n\n\n\nsorted_list = sorted(my_list)  # Returns a sorted copy\n\n\n\nmy_list.reverse()  # Reverses in place\n\n\n\ncount = my_list.count(30)  # Output: 1\n\n\n\nindex = my_list.index(50)  # Output: 2\n\n\n\n\n\n\n# Incorrect\nfor item in my_list:\n    if item &lt; 20:\n        my_list.remove(item)\n\n# Correct (Using a copy)\nfor item in my_list[:]:\n    if item &lt; 20:\n        my_list.remove(item)"
  },
  {
    "objectID": "course-materials/cheatsheets/lists.html#creating-lists",
    "href": "course-materials/cheatsheets/lists.html#creating-lists",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "my_list = []\n\n\n\nmy_list = [1, 2, 3, 4, 5]\n\n\n\nmixed_list = [1, \"hello\", 3.14, True]\n\n\n\nnested_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"
  },
  {
    "objectID": "course-materials/cheatsheets/lists.html#accessing-elements",
    "href": "course-materials/cheatsheets/lists.html#accessing-elements",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "my_list = [10, 20, 30, 40]\nprint(my_list[0])  # Output: 10\nprint(my_list[2])  # Output: 30\n\n\n\nprint(my_list[-1])  # Output: 40\n\n\n\nsublist = my_list[1:3]  # Output: [20, 30]"
  },
  {
    "objectID": "course-materials/cheatsheets/lists.html#modifying-lists",
    "href": "course-materials/cheatsheets/lists.html#modifying-lists",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "my_list[1] = 25  # my_list becomes [10, 25, 30, 40]\n\n\n\nmy_list.append(50)  # my_list becomes [10, 25, 30, 40, 50]\n\n\n\nmy_list.insert(1, 15)  # my_list becomes [10, 15, 25, 30, 40, 50]\n\n\n\nmy_list.extend([60, 70])  # my_list becomes [10, 15, 25, 30, 40, 50, 60, 70]"
  },
  {
    "objectID": "course-materials/cheatsheets/lists.html#removing-elements",
    "href": "course-materials/cheatsheets/lists.html#removing-elements",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "my_list.remove(25)  # my_list becomes [10, 15, 30, 40, 50, 60, 70]\n\n\n\ndel my_list[0]  # my_list becomes [15, 30, 40, 50, 60, 70]\n\n\n\nlast_element = my_list.pop()  # my_list becomes [15, 30, 40, 50, 60]\n\n\n\nelement = my_list.pop(2)  # my_list becomes [15, 30, 50, 60]"
  },
  {
    "objectID": "course-materials/cheatsheets/lists.html#list-operations",
    "href": "course-materials/cheatsheets/lists.html#list-operations",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "length = len(my_list)  # Output: 4\n\n\n\nis_in_list = 30 in my_list  # Output: True\n\n\n\ncombined_list = my_list + [80, 90]  # Output: [15, 30, 50, 60, 80, 90]\n\n\n\nrepeated_list = [1, 2, 3] * 3  # Output: [1, 2, 3, 1, 2, 3, 1, 2, 3]"
  },
  {
    "objectID": "course-materials/cheatsheets/lists.html#looping-through-lists",
    "href": "course-materials/cheatsheets/lists.html#looping-through-lists",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "for item in my_list:\n    print(item)\n\n\n\nfor index, value in enumerate(my_list):\n    print(f\"Index {index} has value {value}\")"
  },
  {
    "objectID": "course-materials/cheatsheets/lists.html#list-comprehensions",
    "href": "course-materials/cheatsheets/lists.html#list-comprehensions",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "squares = [x**2 for x in range(5)]  # Output: [0, 1, 4, 9, 16]\n\n\n\nevens = [x for x in range(10) if x % 2 == 0]  # Output: [0, 2, 4, 6, 8]"
  },
  {
    "objectID": "course-materials/cheatsheets/lists.html#list-methods",
    "href": "course-materials/cheatsheets/lists.html#list-methods",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "my_list.sort()  # Sorts in place\n\n\n\nsorted_list = sorted(my_list)  # Returns a sorted copy\n\n\n\nmy_list.reverse()  # Reverses in place\n\n\n\ncount = my_list.count(30)  # Output: 1\n\n\n\nindex = my_list.index(50)  # Output: 2"
  },
  {
    "objectID": "course-materials/cheatsheets/lists.html#common-list-pitfalls",
    "href": "course-materials/cheatsheets/lists.html#common-list-pitfalls",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "# Incorrect\nfor item in my_list:\n    if item &lt; 20:\n        my_list.remove(item)\n\n# Correct (Using a copy)\nfor item in my_list[:]:\n    if item &lt; 20:\n        my_list.remove(item)"
  },
  {
    "objectID": "course-materials/cheatsheets/numpy.html",
    "href": "course-materials/cheatsheets/numpy.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "import numpy as np\n\n\n\n\n\narr = np.array([1, 2, 3, 4, 5])\n\n\n\n# From Series\ns = pd.Series([1, 2, 3, 4, 5])\narr = s.to_numpy()\n\n# From DataFrame\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\narr = df.to_numpy()\n\n\n\n\n\n\narr1 = np.array([1, 2, 3])\narr2 = np.array([4, 5, 6])\n\n# Addition\nresult = arr1 + arr2\n\n# Multiplication\nresult = arr1 * arr2\n\n# Division\nresult = arr1 / arr2\n\n\n\n# Square root\nsqrt_arr = np.sqrt(arr)\n\n# Exponential\nexp_arr = np.exp(arr)\n\n# Absolute value\nabs_arr = np.abs(arr)\n\n\n\n\n\n\n# Mean\nmean = np.mean(arr)\n\n# Median\nmedian = np.median(arr)\n\n# Standard deviation\nstd = np.std(arr)\n\n\n\n# Minimum\nmin_val = np.min(arr)\n\n# Maximum\nmax_val = np.max(arr)\n\n# Sum\ntotal = np.sum(arr)\n\n\n\n\n\n\narr = np.array([1, 2, 3, 4, 5, 6])\nreshaped = arr.reshape(2, 3)\n\n\n\ntransposed = arr.T\n\n\n\nflattened = arr.flatten()\n\n\n\n\n\n\n# Generate 5 random numbers between 0 and 1\nrandom_uniform = np.random.rand(5)\n\n# Generate 5 random integers between 1 and 10\nrandom_integers = np.random.randint(1, 11, 5)\n\n\n\nnp.random.seed(42)  # For reproducibility\n\n\n\n\n\n\n# Check for NaN\nnp.isnan(arr)\n\n# Replace NaN with a value\nnp.nan_to_num(arr, nan=0.0)\n\n\n\n\n\n\n# Get unique values\nunique_values = np.unique(arr)\n\n# Get value counts (similar to pandas value_counts())\nvalues, counts = np.unique(arr, return_counts=True)\n\n\n\n# Similar to pandas' where, but returns an array\nresult = np.where(condition, x, y)\n\n\n\n# Concatenate arrays (similar to pd.concat())\nconcatenated = np.concatenate([arr1, arr2, arr3])\n\n\n\n\n\nPerformance: For large datasets, NumPy operations can be faster than pandas.\nMemory efficiency: NumPy arrays use less memory than pandas objects.\nSpecific mathematical operations: Some mathematical operations are more straightforward in NumPy.\nInterfacing with other libraries: Many scientific Python libraries use NumPy arrays.\n\nRemember, while these NumPy operations are useful, many have direct equivalents in pandas that work on Series and DataFrames. Always consider whether you can perform the operation directly in pandas before converting to NumPy arrays."
  },
  {
    "objectID": "course-materials/cheatsheets/numpy.html#importing-numpy",
    "href": "course-materials/cheatsheets/numpy.html#importing-numpy",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "import numpy as np"
  },
  {
    "objectID": "course-materials/cheatsheets/numpy.html#creating-numpy-arrays",
    "href": "course-materials/cheatsheets/numpy.html#creating-numpy-arrays",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "arr = np.array([1, 2, 3, 4, 5])\n\n\n\n# From Series\ns = pd.Series([1, 2, 3, 4, 5])\narr = s.to_numpy()\n\n# From DataFrame\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\narr = df.to_numpy()"
  },
  {
    "objectID": "course-materials/cheatsheets/numpy.html#basic-array-operations",
    "href": "course-materials/cheatsheets/numpy.html#basic-array-operations",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "arr1 = np.array([1, 2, 3])\narr2 = np.array([4, 5, 6])\n\n# Addition\nresult = arr1 + arr2\n\n# Multiplication\nresult = arr1 * arr2\n\n# Division\nresult = arr1 / arr2\n\n\n\n# Square root\nsqrt_arr = np.sqrt(arr)\n\n# Exponential\nexp_arr = np.exp(arr)\n\n# Absolute value\nabs_arr = np.abs(arr)"
  },
  {
    "objectID": "course-materials/cheatsheets/numpy.html#statistical-operations",
    "href": "course-materials/cheatsheets/numpy.html#statistical-operations",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "# Mean\nmean = np.mean(arr)\n\n# Median\nmedian = np.median(arr)\n\n# Standard deviation\nstd = np.std(arr)\n\n\n\n# Minimum\nmin_val = np.min(arr)\n\n# Maximum\nmax_val = np.max(arr)\n\n# Sum\ntotal = np.sum(arr)"
  },
  {
    "objectID": "course-materials/cheatsheets/numpy.html#array-manipulation",
    "href": "course-materials/cheatsheets/numpy.html#array-manipulation",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "arr = np.array([1, 2, 3, 4, 5, 6])\nreshaped = arr.reshape(2, 3)\n\n\n\ntransposed = arr.T\n\n\n\nflattened = arr.flatten()"
  },
  {
    "objectID": "course-materials/cheatsheets/numpy.html#random-number-generation",
    "href": "course-materials/cheatsheets/numpy.html#random-number-generation",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "# Generate 5 random numbers between 0 and 1\nrandom_uniform = np.random.rand(5)\n\n# Generate 5 random integers between 1 and 10\nrandom_integers = np.random.randint(1, 11, 5)\n\n\n\nnp.random.seed(42)  # For reproducibility"
  },
  {
    "objectID": "course-materials/cheatsheets/numpy.html#working-with-missing-data",
    "href": "course-materials/cheatsheets/numpy.html#working-with-missing-data",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "# Check for NaN\nnp.isnan(arr)\n\n# Replace NaN with a value\nnp.nan_to_num(arr, nan=0.0)"
  },
  {
    "objectID": "course-materials/cheatsheets/numpy.html#useful-numpy-functions-for-pandas-users",
    "href": "course-materials/cheatsheets/numpy.html#useful-numpy-functions-for-pandas-users",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "# Get unique values\nunique_values = np.unique(arr)\n\n# Get value counts (similar to pandas value_counts())\nvalues, counts = np.unique(arr, return_counts=True)\n\n\n\n# Similar to pandas' where, but returns an array\nresult = np.where(condition, x, y)\n\n\n\n# Concatenate arrays (similar to pd.concat())\nconcatenated = np.concatenate([arr1, arr2, arr3])"
  },
  {
    "objectID": "course-materials/cheatsheets/numpy.html#when-to-use-numpy-with-pandas",
    "href": "course-materials/cheatsheets/numpy.html#when-to-use-numpy-with-pandas",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Performance: For large datasets, NumPy operations can be faster than pandas.\nMemory efficiency: NumPy arrays use less memory than pandas objects.\nSpecific mathematical operations: Some mathematical operations are more straightforward in NumPy.\nInterfacing with other libraries: Many scientific Python libraries use NumPy arrays.\n\nRemember, while these NumPy operations are useful, many have direct equivalents in pandas that work on Series and DataFrames. Always consider whether you can perform the operation directly in pandas before converting to NumPy arrays."
  },
  {
    "objectID": "course-materials/cheatsheets/setting_up_python.html",
    "href": "course-materials/cheatsheets/setting_up_python.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "This guide will help you set up Python 3.11 and JupyterLab on your local machine using Miniconda. We’ll also install core data science libraries."
  },
  {
    "objectID": "course-materials/cheatsheets/setting_up_python.html#step-0-opening-a-terminal",
    "href": "course-materials/cheatsheets/setting_up_python.html#step-0-opening-a-terminal",
    "title": "EDS 217 Cheatsheet",
    "section": "Step 0: Opening a Terminal",
    "text": "Step 0: Opening a Terminal\nBefore we begin, you’ll need to know how to open a terminal (command-line interface) on your operating system:\n\nFor Windows:\n\nPress the Windows key + R to open the Run dialog.\nType cmd and press Enter. Alternatively, search for “Command Prompt” in the Start menu.\n\n\n\nFor macOS:\n\nPress Command + Space to open Spotlight Search.\nType “Terminal” and press Enter. Alternatively, go to Applications &gt; Utilities &gt; Terminal.\n\n\n\nFor Linux:\n\nMost Linux distributions use Ctrl + Alt + T as a keyboard shortcut to open the terminal.\nYou can also search for “Terminal” in your distribution’s application menu."
  },
  {
    "objectID": "course-materials/cheatsheets/setting_up_python.html#step-1-download-and-install-miniconda",
    "href": "course-materials/cheatsheets/setting_up_python.html#step-1-download-and-install-miniconda",
    "title": "EDS 217 Cheatsheet",
    "section": "Step 1: Download and Install Miniconda",
    "text": "Step 1: Download and Install Miniconda\n\nFor Windows:\n\nDownload the Miniconda installer for Windows from the official website.\nRun the installer and follow the prompts.\nDuring installation, make sure to add Miniconda to your PATH environment variable when prompted.\n\n\n\nFor macOS:\n\nDownload the Miniconda installer for macOS from the official website.\nOpen Terminal and navigate to the directory containing the downloaded file.\nRun the following command:\nbash Miniconda3-latest-MacOSX-x86_64.sh\nFollow the prompts and accept the license agreement.\n\n\n\nFor Linux:\n\nDownload the Miniconda installer for Linux from the official website.\nOpen a terminal and navigate to the directory containing the downloaded file.\nRun the following command:\nbash Miniconda3-latest-Linux-x86_64.sh\nFollow the prompts and accept the license agreement."
  },
  {
    "objectID": "course-materials/cheatsheets/setting_up_python.html#step-2-set-up-python-3.11-and-core-libraries",
    "href": "course-materials/cheatsheets/setting_up_python.html#step-2-set-up-python-3.11-and-core-libraries",
    "title": "EDS 217 Cheatsheet",
    "section": "Step 2: Set up Python 3.11 and Core Libraries",
    "text": "Step 2: Set up Python 3.11 and Core Libraries\nOpen a new terminal or command prompt window to ensure the Miniconda installation is recognized.\nRun the following commands:\nconda install python=3.11\nconda install jupyter jupyterlab numpy pandas matplotlib seaborn\nThis will install Python 3.11, JupyterLab, and the core data science libraries in your base environment."
  },
  {
    "objectID": "course-materials/cheatsheets/setting_up_python.html#alternative-create-a-dedicated-eds-217-environment",
    "href": "course-materials/cheatsheets/setting_up_python.html#alternative-create-a-dedicated-eds-217-environment",
    "title": "EDS 217 Cheatsheet",
    "section": "Alternative: Create a Dedicated EDS 217 Environment",
    "text": "Alternative: Create a Dedicated EDS 217 Environment\nFor better package management, you can create a dedicated conda environment for this course:\n# Create a new environment specifically for EDS 217\nconda create -n eds217_2025 python=3.11\n\n# Activate the environment\nconda activate eds217_2025\n\n# Install packages in the dedicated environment\nconda install jupyter jupyterlab numpy pandas matplotlib seaborn scipy scikit-learn\n\n# Register the environment with Jupyter\npython -m ipykernel install --user --name eds217_2025 --display-name \"Python 3.11 (EDS 217 2025)\"\nWhen using JupyterLab, you can then select “Python 3.11 (EDS 217 2025)” as your kernel."
  },
  {
    "objectID": "course-materials/cheatsheets/setting_up_python.html#step-3-verify-installation",
    "href": "course-materials/cheatsheets/setting_up_python.html#step-3-verify-installation",
    "title": "EDS 217 Cheatsheet",
    "section": "Step 3: Verify Installation",
    "text": "Step 3: Verify Installation\n\nTo verify that Python 3.11 is installed, run:\npython --version\nTo launch JupyterLab, run:\njupyter lab\n\nThis should open JupyterLab in your default web browser. You can now create new notebooks and start coding!"
  },
  {
    "objectID": "course-materials/cheatsheets/setting_up_python.html#additional-notes",
    "href": "course-materials/cheatsheets/setting_up_python.html#additional-notes",
    "title": "EDS 217 Cheatsheet",
    "section": "Additional Notes",
    "text": "Additional Notes\n\nTo update Miniconda and installed packages in the future, use:\nconda update --all\nWhile we’re using the base environment for this quick setup, it’s generally a good practice to create separate environments for different projects. You can explore this concept later as you become more familiar with conda."
  },
  {
    "objectID": "course-materials/cheatsheets/workflow_methods.html",
    "href": "course-materials/cheatsheets/workflow_methods.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "This table maps commonly used pandas DataFrame methods to the steps in the course-specific data science workflow. Each method is linked to its official pandas documentation for easy reference.\n\n\n\nDataFrame Method —————————-\nImport\nExploration\nCleaning\nFiltering/ Selection\nTransforming\nSorting\nGrouping\nAggregating\nVisualizing\n\n\n\n\nread_csv()\n✓\n\n\n\n\n\n\n\n\n\n\nread_excel()\n✓\n\n\n\n\n\n\n\n\n\n\nhead()\n\n✓\n\n\n\n\n\n\n\n\n\ntail()\n\n✓\n\n\n\n\n\n\n\n\n\ninfo()\n\n✓\n✓\n\n\n\n\n\n\n\n\ndescribe()\n\n✓\n\n\n\n\n\n✓\n\n\n\ndtypes\n\n✓\n✓\n\n\n\n\n\n\n\n\nshape\n\n✓\n\n\n\n\n\n\n\n\n\ncolumns\n\n✓\n\n\n\n\n\n\n\n\n\nisnull()\n\n✓\n✓\n\n\n\n\n\n\n\n\nnotnull()\n\n✓\n✓\n\n\n\n\n\n\n\n\ndropna()\n\n\n✓\n\n✓\n\n\n\n\n\n\nfillna()\n\n\n✓\n\n✓\n\n\n\n\n\n\nreplace()\n\n\n✓\n\n✓\n\n\n\n\n\n\nastype()\n\n\n✓\n\n✓\n\n\n\n\n\n\nrename()\n\n\n✓\n\n✓\n\n\n\n\n\n\ndrop()\n\n\n✓\n✓\n✓\n\n\n\n\n\n\nduplicated()\n\n✓\n✓\n\n\n\n\n\n\n\n\ndrop_duplicates()\n\n\n✓\n\n✓\n\n\n\n\n\n\nvalue_counts()\n\n✓\n\n\n\n\n\n✓\n\n\n\nunique()\n\n✓\n\n\n\n\n\n\n\n\n\nnunique()\n\n✓\n\n\n\n\n\n✓\n\n\n\nsample()\n\n✓\n\n✓\n\n\n\n\n\n\n\ncorr()\n\n✓\n\n\n\n\n\n✓\n✓\n\n\ncov()\n\n✓\n\n\n\n\n\n✓\n\n\n\ngroupby()\n\n\n\n\n\n\n✓\n\n\n\n\nagg()\n\n\n\n\n\n\n✓\n✓\n\n\n\napply()\n\n\n\n\n✓\n\n\n\n\n\n\nmerge()\n\n\n\n\n✓\n\n\n\n\n\n\njoin()\n\n\n\n\n✓\n\n\n\n\n\n\nconcat()\n\n\n\n\n✓\n\n\n\n\n\n\npivot()\n\n\n\n\n✓\n\n\n\n\n\n\nmelt()\n\n\n\n\n✓\n\n\n\n\n\n\nsort_values()\n\n\n\n\n\n✓\n\n\n\n\n\nnlargest()\n\n\n\n✓\n\n✓\n\n\n\n\n\nnsmallest()\n\n\n\n✓\n\n✓\n\n\n\n\n\nquery()\n\n\n\n✓\n\n\n\n\n\n\n\neval()\n\n\n\n\n✓\n\n\n\n\n\n\ncut()\n\n\n\n\n✓\n\n\n\n\n\n\nqcut()\n\n\n\n\n✓\n\n\n\n\n\n\nget_dummies()\n\n\n\n\n✓\n\n\n\n\n\n\niloc[]\n\n\n\n✓\n\n\n\n\n\n\n\nloc[]\n\n\n\n✓\n\n\n\n\n\n\n\nplot()\n\n✓\n\n\n\n\n\n\n✓\n\n\n\nNote: This table includes some of the most commonly used DataFrame methods, but it’s not exhaustive. Some methods may be applicable to multiple steps depending on the specific use case."
  },
  {
    "objectID": "course-materials/cheatsheets/workflow_methods.html#pandas-dataframe-methods-in-data-science-workflows",
    "href": "course-materials/cheatsheets/workflow_methods.html#pandas-dataframe-methods-in-data-science-workflows",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "This table maps commonly used pandas DataFrame methods to the steps in the course-specific data science workflow. Each method is linked to its official pandas documentation for easy reference.\n\n\n\nDataFrame Method —————————-\nImport\nExploration\nCleaning\nFiltering/ Selection\nTransforming\nSorting\nGrouping\nAggregating\nVisualizing\n\n\n\n\nread_csv()\n✓\n\n\n\n\n\n\n\n\n\n\nread_excel()\n✓\n\n\n\n\n\n\n\n\n\n\nhead()\n\n✓\n\n\n\n\n\n\n\n\n\ntail()\n\n✓\n\n\n\n\n\n\n\n\n\ninfo()\n\n✓\n✓\n\n\n\n\n\n\n\n\ndescribe()\n\n✓\n\n\n\n\n\n✓\n\n\n\ndtypes\n\n✓\n✓\n\n\n\n\n\n\n\n\nshape\n\n✓\n\n\n\n\n\n\n\n\n\ncolumns\n\n✓\n\n\n\n\n\n\n\n\n\nisnull()\n\n✓\n✓\n\n\n\n\n\n\n\n\nnotnull()\n\n✓\n✓\n\n\n\n\n\n\n\n\ndropna()\n\n\n✓\n\n✓\n\n\n\n\n\n\nfillna()\n\n\n✓\n\n✓\n\n\n\n\n\n\nreplace()\n\n\n✓\n\n✓\n\n\n\n\n\n\nastype()\n\n\n✓\n\n✓\n\n\n\n\n\n\nrename()\n\n\n✓\n\n✓\n\n\n\n\n\n\ndrop()\n\n\n✓\n✓\n✓\n\n\n\n\n\n\nduplicated()\n\n✓\n✓\n\n\n\n\n\n\n\n\ndrop_duplicates()\n\n\n✓\n\n✓\n\n\n\n\n\n\nvalue_counts()\n\n✓\n\n\n\n\n\n✓\n\n\n\nunique()\n\n✓\n\n\n\n\n\n\n\n\n\nnunique()\n\n✓\n\n\n\n\n\n✓\n\n\n\nsample()\n\n✓\n\n✓\n\n\n\n\n\n\n\ncorr()\n\n✓\n\n\n\n\n\n✓\n✓\n\n\ncov()\n\n✓\n\n\n\n\n\n✓\n\n\n\ngroupby()\n\n\n\n\n\n\n✓\n\n\n\n\nagg()\n\n\n\n\n\n\n✓\n✓\n\n\n\napply()\n\n\n\n\n✓\n\n\n\n\n\n\nmerge()\n\n\n\n\n✓\n\n\n\n\n\n\njoin()\n\n\n\n\n✓\n\n\n\n\n\n\nconcat()\n\n\n\n\n✓\n\n\n\n\n\n\npivot()\n\n\n\n\n✓\n\n\n\n\n\n\nmelt()\n\n\n\n\n✓\n\n\n\n\n\n\nsort_values()\n\n\n\n\n\n✓\n\n\n\n\n\nnlargest()\n\n\n\n✓\n\n✓\n\n\n\n\n\nnsmallest()\n\n\n\n✓\n\n✓\n\n\n\n\n\nquery()\n\n\n\n✓\n\n\n\n\n\n\n\neval()\n\n\n\n\n✓\n\n\n\n\n\n\ncut()\n\n\n\n\n✓\n\n\n\n\n\n\nqcut()\n\n\n\n\n✓\n\n\n\n\n\n\nget_dummies()\n\n\n\n\n✓\n\n\n\n\n\n\niloc[]\n\n\n\n✓\n\n\n\n\n\n\n\nloc[]\n\n\n\n✓\n\n\n\n\n\n\n\nplot()\n\n✓\n\n\n\n\n\n\n✓\n\n\n\nNote: This table includes some of the most commonly used DataFrame methods, but it’s not exhaustive. Some methods may be applicable to multiple steps depending on the specific use case."
  },
  {
    "objectID": "course-materials/cheatsheets/workflow_methods.html#key-takeaways",
    "href": "course-materials/cheatsheets/workflow_methods.html#key-takeaways",
    "title": "EDS 217 Cheatsheet",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nImport primarily involves reading data from various sources.\nExploration methods help understand the structure and content of the data.\nCleaning methods focus on handling missing data, duplicates, and data type issues.\nFiltering/Selection methods allow you to subset your data based on various conditions.\nTransforming methods cover a wide range of data manipulation tasks.\nSorting methods help arrange data in a specific order.\nGrouping is often a precursor to aggregation operations.\nAggregating methods compute summary statistics on data.\nVisualizing methods help create graphical representations of the data.\n\nRemember that the applicability of methods can vary depending on the specific project and dataset. This table serves as a general guide to help you navigate the pandas DataFrame methods in the context of your course’s data science workflow. The links to the official documentation provide more detailed information about each method’s usage and parameters."
  },
  {
    "objectID": "course-materials/day2.html#class-materials",
    "href": "course-materials/day2.html#class-materials",
    "title": "Python Data Collections",
    "section": "Class materials",
    "text": "Class materials\n\n\n\n\n\n\n\n\n Session\n Session 1\n Session 2\n\n\n\n\nday 2 / morning\n🐍 Lists\n🐍 Dictionaries\n\n\nday 2 / afternoon\n🙌 Working with Lists, Dictionaries, and Sets\n📝 List and Dictionary Comprehensions"
  },
  {
    "objectID": "course-materials/day2.html#end-of-day-practice",
    "href": "course-materials/day2.html#end-of-day-practice",
    "title": "Python Data Collections",
    "section": "End-of-day practice",
    "text": "End-of-day practice\nComplete the following tasks / activities before heading home for the day!\n\n Day 2 Practice: Python Data Structures Practice"
  },
  {
    "objectID": "course-materials/day2.html#additional-resources",
    "href": "course-materials/day2.html#additional-resources",
    "title": "Python Data Collections",
    "section": "Additional Resources",
    "text": "Additional Resources"
  },
  {
    "objectID": "course-materials/day4.html#class-materials",
    "href": "course-materials/day4.html#class-materials",
    "title": "Working with DataFrames in Pandas",
    "section": "Class materials",
    "text": "Class materials\n\n\n\n\n\n\n\n\n Session\n Session 1\n Session 2\n\n\n\n\nday 4 / morning\n🐼 Intro to DataFrames\n🙌 Coding Colab: Working with DataFrames\n\n\nday 4 / afternoon\n🐼 DataFrame Workflows\n📝 Data Import/Export"
  },
  {
    "objectID": "course-materials/day4.html#end-of-day-practice",
    "href": "course-materials/day4.html#end-of-day-practice",
    "title": "Working with DataFrames in Pandas",
    "section": "End-of-day practice",
    "text": "End-of-day practice\nComplete the following tasks / activities before heading home for the day!\n\n Day 4 Practice: Reading, Visualizing, and Exporting Data in Pandas"
  },
  {
    "objectID": "course-materials/day4.html#additional-resources",
    "href": "course-materials/day4.html#additional-resources",
    "title": "Working with DataFrames in Pandas",
    "section": "Additional Resources",
    "text": "Additional Resources"
  },
  {
    "objectID": "course-materials/day6.html#class-materials",
    "href": "course-materials/day6.html#class-materials",
    "title": "Grouping, Joining and Sorting Data in Pandas",
    "section": "Class materials",
    "text": "Class materials\n\n\n\n\n\n\n\n\n Session\n Session 1\n Session 2\n\n\n\n\nday 6 / morning\n🐼 Grouping, joining, sorting, and applying\n🙌 Coding Colab: Data Manipulation\n\n\nday 6 / afternoon\n🐼 Working with dates\nEnd-of-day practice"
  },
  {
    "objectID": "course-materials/day6.html#end-of-day-practice",
    "href": "course-materials/day6.html#end-of-day-practice",
    "title": "Grouping, Joining and Sorting Data in Pandas",
    "section": "End-of-day practice",
    "text": "End-of-day practice\nComplete the following tasks / activities before heading home for the day!\n\n Day 6 Practice: 🕺 Eurovision Data Science 💃"
  },
  {
    "objectID": "course-materials/day6.html#additional-resources",
    "href": "course-materials/day6.html#additional-resources",
    "title": "Grouping, Joining and Sorting Data in Pandas",
    "section": "Additional Resources",
    "text": "Additional Resources"
  },
  {
    "objectID": "course-materials/day8.html#class-materials",
    "href": "course-materials/day8.html#class-materials",
    "title": "Building a Python Data Science Workflow",
    "section": "Class materials",
    "text": "Class materials\n\n\n\n\n\n\n\n\n Session\n Session 1\n Session 2\n\n\n\n\nday 8 / morning\nWorking on Final Data Science Project (all day)\n\n\n\nday 8 / afternoon"
  },
  {
    "objectID": "course-materials/day8.html#syncing-your-classwork-to-github",
    "href": "course-materials/day8.html#syncing-your-classwork-to-github",
    "title": "Building a Python Data Science Workflow",
    "section": "Syncing your classwork to Github",
    "text": "Syncing your classwork to Github\nHere are some directions for syncing your classwork with a GitHub repository"
  },
  {
    "objectID": "course-materials/day8.html#end-of-day-practice",
    "href": "course-materials/day8.html#end-of-day-practice",
    "title": "Building a Python Data Science Workflow",
    "section": "End-of-day practice",
    "text": "End-of-day practice\nThere are no additional end-of-day tasks / activities today!"
  },
  {
    "objectID": "course-materials/day8.html#additional-resources",
    "href": "course-materials/day8.html#additional-resources",
    "title": "Building a Python Data Science Workflow",
    "section": "Additional Resources",
    "text": "Additional Resources"
  },
  {
    "objectID": "course-materials/interactive-sessions/1a_iPython_JupyterLab.html",
    "href": "course-materials/interactive-sessions/1a_iPython_JupyterLab.html",
    "title": "Interactive Session 1A",
    "section": "",
    "text": "This is a short exercise to introduce you to the IPython REPL within a JupyterLab session hosted on a Posit Workbench server. This exercise will help you become familiar with the interactive environment we will use throughout the class (and throughout your time in the MEDS program) as well as an introduction to some basic Python operations.\n\n\nExercise: Introduction to IPython REPL in JupyterLab\nObjective: Learn how to use the IPython REPL in JupyterLab for basic Python programming and explore some interactive features.\n\n\n\n\n\n\nNote\n\n\n\nThe Read-Eval-Print Loop (REPL) is an interactive programming environment that allows users to execute Python code line-by-line, providing immediate feedback and facilitating rapid testing, debugging, and learning.\n\n\n\n\nStep 1: Access JupyterLab\n\nLog in to the Posit Workbench Server\n\nOpen a web browser and go to workbench-1.bren.ucsb.edu.\nEnter your login credentials to access the server.\n\nSelect JupyterLab\n\nOnce logged in, click on the “New Session” button, and select “JupyterLab” from the list of options\n\n\nStart JupyterLab Session\n\nClick the “Start Session” button in the lower right of the modal window. You don’t need to edit either the Session Name or Cluster.\n\nWait for the Session to Launch\n\nYour browser will auto join the session as soon as the server starts it up.\n\n\n\n\n\nStep 2: Open the IPython REPL\nWhen the session launches you should see an interface that looks like this:\n\n\nStart a Terminal\n\nSelect “Terminal” from the list of available options in the Launcher pane. This will open a new terminal tab.\n\nLaunch IPython\n\nIn the terminal, type ipython and press Enter to start the IPython REPL.\n\n\n\n\n\nStep 3: Basic IPython Commands\nIn the IPython REPL, try the following commands to get familiar with the environment:\n\nBasic Arithmetic Operations\n\nCalculate the sum of two numbers:\n3 + 5\nMultiply two numbers:\n4 * 7\nDivide two numbers:\n10 / 2\n\nVariable Assignment\n\nAssign a value to a variable and use it in a calculation:\nx = 10\ny = 5\nresult = x * y\nresult\n\nBuilt-in Functions\n\nUse a built-in function to find the maximum of a list of numbers:\nnumbers = [3, 9, 1, 6, 2]\nmax(numbers)\n\nInteractive Help\n\nUse the help() function to get more information about a built-in function:\nhelp(print)\nUse the ? to get a quick description of an object or function:\nlen?\n\n\n\n\n\nStep 4: Explore IPython Features\n\nTab Completion\n\nStart typing a command or variable name and press Tab to auto-complete or view suggestions:\nnum # Press Tab here\n\nMagic Commands\n\nUse the %timeit magic command to time the execution of a statement:\n%timeit sum(range(1000))\n\nHistory\n\nView the command history using the %history magic command:\n%history\n\nClear the Console\n\nClear the current console session with:\n%clear\n\n\n\n\n\nStep 5: Exit the IPython REPL\n\nTo exit the IPython REPL, type exit() or press Ctrl+D.\n\n\n\n\nWrap-Up\nCongratulations! You have completed the introduction to the IPython REPL in JupyterLab. You learned how to perform basic operations, use interactive help, explore magic commands, and utilize IPython features.\nFeel free to explore more IPython functionalities or ask questions if you need further assistance.\n\n\nEnd interactive session 1A"
  },
  {
    "objectID": "course-materials/lectures/data_types.html#types-of-data-in-python",
    "href": "course-materials/lectures/data_types.html#types-of-data-in-python",
    "title": "Basic Data Types in Python",
    "section": "Types of Data in Python",
    "text": "Types of Data in Python\nPython categorizes data into two main types:\n\nValues: Singular items like numbers or strings.\nCollections: Groupings of values, like lists or dictionaries.\n\nMutable vs Immutable\n\nMutable: Objects whose content can be changed after creation.\nImmutable: Objects that cannot be altered after they are created."
  },
  {
    "objectID": "course-materials/lectures/data_types.html#overview-of-main-data-types",
    "href": "course-materials/lectures/data_types.html#overview-of-main-data-types",
    "title": "Basic Data Types in Python",
    "section": "Overview of Main Data Types",
    "text": "Overview of Main Data Types\n\n\n\n\n\n\n\n\nCategory\nMutable\nImmutable\n\n\n\n\nValues\n-\nint, float, complex, str\n\n\nCollections\nlist, dict, set, bytearray\ntuple, frozenset"
  },
  {
    "objectID": "course-materials/lectures/data_types.html#numeric-types",
    "href": "course-materials/lectures/data_types.html#numeric-types",
    "title": "Basic Data Types in Python",
    "section": "Numeric Types",
    "text": "Numeric Types\nIntegers (int)\n\nUse: Counting, indexing, and more.\nConstruction: x = 5\nImmutable: Cannot change the value of x without creating a new int."
  },
  {
    "objectID": "course-materials/lectures/data_types.html#numeric-types-continued",
    "href": "course-materials/lectures/data_types.html#numeric-types-continued",
    "title": "Basic Data Types in Python",
    "section": "Numeric Types (continued)",
    "text": "Numeric Types (continued)\nFloating-Point Numbers (float)\n\nUse: Representing real numbers for measurements, fractions, etc.\nConstruction: y = 3.14\nImmutable: Like integers, any change creates a new float."
  },
  {
    "objectID": "course-materials/lectures/data_types.html#text-type",
    "href": "course-materials/lectures/data_types.html#text-type",
    "title": "Basic Data Types in Python",
    "section": "Text Type",
    "text": "Text Type\nStrings (str)\n\nUse: Handling textual data.\nConstruction: s = \"Data Science\"\nImmutable: Modifying s requires creating a new string."
  },
  {
    "objectID": "course-materials/lectures/data_types.html#sequence-types",
    "href": "course-materials/lectures/data_types.html#sequence-types",
    "title": "Basic Data Types in Python",
    "section": "Sequence Types",
    "text": "Sequence Types\nLists (list)\n\nUse: Storing an ordered collection of items.\nConstruction: my_list = [1, 2, 3]\nMutable: Items can be added, removed, or changed."
  },
  {
    "objectID": "course-materials/lectures/data_types.html#sequence-types-continued",
    "href": "course-materials/lectures/data_types.html#sequence-types-continued",
    "title": "Basic Data Types in Python",
    "section": "Sequence Types (continued)",
    "text": "Sequence Types (continued)\nTuples (tuple)\n\nUse: Immutable lists. Often used where a fixed, unchangeable sequence is needed.\nConstruction: my_tuple = (1, 2, 3)\nImmutable: Cannot alter the contents once created."
  },
  {
    "objectID": "course-materials/lectures/data_types.html#set-types",
    "href": "course-materials/lectures/data_types.html#set-types",
    "title": "Basic Data Types in Python",
    "section": "Set Types",
    "text": "Set Types\nSets (set)\n\nUse: Unique collection of items, great for membership testing, removing duplicates.\nConstruction: my_set = {1, 2, 3}\nMutable: Can add or remove items."
  },
  {
    "objectID": "course-materials/lectures/data_types.html#set-types-continued",
    "href": "course-materials/lectures/data_types.html#set-types-continued",
    "title": "Basic Data Types in Python",
    "section": "Set Types (continued)",
    "text": "Set Types (continued)\nFrozen Sets (frozenset)\n\nUse: Immutable version of sets.\nConstruction: my_frozenset = frozenset([1, 2, 3])\nImmutable: Safe for use as dictionary keys."
  },
  {
    "objectID": "course-materials/lectures/data_types.html#mapping-types",
    "href": "course-materials/lectures/data_types.html#mapping-types",
    "title": "Basic Data Types in Python",
    "section": "Mapping Types",
    "text": "Mapping Types\nDictionaries (dict)\n\nUse: Key-value pairs for fast lookup and data management.\nConstruction: my_dict = {'key': 'value'}\nMutable: Add, remove, or change associations."
  },
  {
    "objectID": "course-materials/lectures/data_types.html#conclusion",
    "href": "course-materials/lectures/data_types.html#conclusion",
    "title": "Basic Data Types in Python",
    "section": "Conclusion",
    "text": "Conclusion\nUnderstanding these basic types is crucial for data handling and manipulation in Python, especially in data science where the type of data dictates the analysis technique. As we move into more advanced Python we will get to know more complex data types.\nFor more information, you can always refer to the Python official documentation."
  },
  {
    "objectID": "course-materials/live-coding/3a_control_flows.html#overview",
    "href": "course-materials/live-coding/3a_control_flows.html#overview",
    "title": "Live Coding Session 3A",
    "section": "Overview",
    "text": "Overview\nIn this session, we will be exploring Control Flows - if-elif, for, while and other ways of altering the flow of code execution. Live coding is a great way to learn programming as it allows you to see the process of writing code in real-time, including how to deal with unexpected issues and debug errors."
  },
  {
    "objectID": "course-materials/live-coding/3a_control_flows.html#objectives",
    "href": "course-materials/live-coding/3a_control_flows.html#objectives",
    "title": "Live Coding Session 3A",
    "section": "Objectives",
    "text": "Objectives\n\nUnderstand the fundamentals of flow control in Python.\nApply if-elif-else constructions in practical examples.\nUse for and while loops to iterate through collections.\nDevelop the ability to troubleshoot and debug in a live setting."
  },
  {
    "objectID": "course-materials/live-coding/3a_control_flows.html#getting-started",
    "href": "course-materials/live-coding/3a_control_flows.html#getting-started",
    "title": "Live Coding Session 3A",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nSet up your notebook:\nPlease set up your Jupyter Notebook with the following structure. We’ll fill in the content together during our session.\n\n    ### Introduction to Control Flows\n\n    &lt;insert code cell below&gt; \n\n    ### Conditionals\n\n    #### Basic If Statement\n\n    &lt;insert code cell below&gt; \n    \n    #### Adding Else\n\n    &lt;insert code cell below&gt; \n    \n    #### Using Elif\n\n    &lt;insert code cell below&gt; \n    \n    ### Loops\n\n    #### For Loops\n\n    &lt;insert code cell below&gt; \n    \n    #### While Loops\n\n    &lt;insert code cell below&gt; \n    \n    ### Applying Control Flows in Data Science\n\n    &lt;insert code cell below&gt; \n    \n    ### Conclusion\n\n   &lt;insert code cell below&gt; \n    \n\n\n\n\n\n\nCaution\n\n\n\nDon’t forget to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nRemember, we’ll be coding together, so don’t worry about filling in the content now. Just set up the structure, and we’ll dive into the details during our session!\n\nParticipation:\n\nTry to code along with me during the session.\nFeel free to ask questions at any time. Remember, if you have a question, others probably do too!\n\nResources:\n\nI will be sharing snippets of code and notes. Make sure to take your own notes and save snippets in your notebook for future reference.\nCheck out our class control flows cheatsheet."
  },
  {
    "objectID": "course-materials/live-coding/3a_control_flows.html#session-format",
    "href": "course-materials/live-coding/3a_control_flows.html#session-format",
    "title": "Live Coding Session 3A",
    "section": "Session Format",
    "text": "Session Format\n\nIntroduction\n\nA brief discussion about the topic in Python programming and its importance in data science.\n\n\n\nDemonstration\n\nI will demonstrate code examples live. Follow along and write the code into your own Jupyter notebook.\n\n\n\nPractice\n\nYou will have the opportunity to try exercises on your own to apply what you’ve learned.\n\n\n\nQ&A\n\nWe will have a Q&A session at the end where you can ask specific questions about the code, concepts, or issues encountered during the session."
  },
  {
    "objectID": "course-materials/live-coding/3a_control_flows.html#after-the-session",
    "href": "course-materials/live-coding/3a_control_flows.html#after-the-session",
    "title": "Live Coding Session 3A",
    "section": "After the Session",
    "text": "After the Session\n\nReview your notes and try to replicate the exercises on your own.\nExperiment with the code by modifying parameters or adding new features to deepen your understanding.\nCheck out our class flow control cheatsheet."
  },
  {
    "objectID": "course-materials/interactive-sessions/2c_exceptions_and_errors.html",
    "href": "course-materials/interactive-sessions/2c_exceptions_and_errors.html",
    "title": "Interactive Session 2C",
    "section": "",
    "text": "Objective:\nThis session aims to help you understand how to interpret error messages in Python. By generating errors in a controlled environment, you’ll learn how to read error reports, identify the source of the problem, and correct your code. This is an essential skill for debugging and improving your Python programming abilities.\nEstimated Time: 45-60 minutes"
  },
  {
    "objectID": "course-materials/interactive-sessions/2c_exceptions_and_errors.html#part-1-introduction-to-python-errors",
    "href": "course-materials/interactive-sessions/2c_exceptions_and_errors.html#part-1-introduction-to-python-errors",
    "title": "Interactive Session 2C",
    "section": "Part 1: Introduction to Python Errors",
    "text": "Part 1: Introduction to Python Errors\n\n1.1 Generating a Syntax Error\nIn Python, a syntax error occurs when the code you write doesn’t conform to the rules of the language.\n\nStep 1: Run the following code in a Jupyter notebook cell to generate a syntax error.\nprint(\"Hello World\nStep 2: Observe the error message. It should look something like this:\nFile \"&lt;ipython-input-1&gt;\", line 1\n    print(\"Hello World\n                        ^\nSyntaxError: EOL while scanning string literal\nStep 3: Explanation: The error message indicates that the End Of Line (EOL) was reached while the string literal was still open. A string literal is what is created inside the open \" and close \". The caret (^) points to where Python expected the closing quote.\nStep 4: Fix the Error: Correct the code by adding the missing closing quotation mark.\nprint(\"Hello World\")"
  },
  {
    "objectID": "course-materials/interactive-sessions/2c_exceptions_and_errors.html#part-2-name-errors-with-variables",
    "href": "course-materials/interactive-sessions/2c_exceptions_and_errors.html#part-2-name-errors-with-variables",
    "title": "Interactive Session 2C",
    "section": "Part 2: Name Errors with Variables",
    "text": "Part 2: Name Errors with Variables\n\n2.1 Using an Undefined Variable\nA NameError occurs when you try to use a variable that hasn’t been defined.\n\nStep 1: Run the following code to generate a NameError.\nprint(variable)\nStep 2: Observe the error message.\nNameError: name 'variable' is not defined\nStep 3: Explanation: Python is telling you that the variable variable has not been defined. This means you are trying to use a variable that Python doesn’t recognize.\nStep 4: Fix the Error: Define the variable before using it.\nvariable = \"I'm now defined!\"\nprint(variable)\n\n\n\n\n\n\n\nCommon NameError patterns in Python\n\n\n\nA NameError often occurs when Python can’t find a variable or function you’re trying to use. This is usually because of:\n\nTypos in Function or Variable Names:\n\nIf you mistype a function or variable name, Python will raise a NameError because it doesn’t recognize the name.\nExample:\nprnt(\"Hello, World!\")  # NameError: name 'prnt' is not defined\n\nFix: Correct the typo to print(\"Hello, World!\").\n\n\nUsing Literals as Variables:\n\nA NameError can also happen if you accidentally try to use a string or number as if it were a variable.\nExample:\n\"Hello\" = 5  # NameError: can't assign to literal\n\nFix: Make sure you’re using valid variable names and not trying to assign values to literals.\n\n\n\nRemember: Always double-check your spelling and ensure that you’re using variable names correctly!"
  },
  {
    "objectID": "course-materials/interactive-sessions/2c_exceptions_and_errors.html#part-3-type-errors-with-functions",
    "href": "course-materials/interactive-sessions/2c_exceptions_and_errors.html#part-3-type-errors-with-functions",
    "title": "Interactive Session 2C",
    "section": "Part 3: Type Errors with Functions",
    "text": "Part 3: Type Errors with Functions\n\n3.1 Passing Incorrect Data Types\nA TypeError occurs when an operation or function is applied to an object of an inappropriate type.\n\nStep 1: Run the following code to generate a TypeError.\nnumber = 5\nprint(number + \"10\")\nStep 2: Observe the error message.\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\nStep 3: Explanation: The error indicates that you are trying to add an integer (int) and a string (str), which is not allowed in Python.\nStep 4: Fix the Error: Convert the string \"10\" to an integer or the integer number to a string.\nprint(number + 10)  # Correct approach 1\n\n# or\n\nprint(str(number) + \"10\")  # Correct approach 2"
  },
  {
    "objectID": "course-materials/interactive-sessions/2c_exceptions_and_errors.html#part-4-index-errors-with-lists",
    "href": "course-materials/interactive-sessions/2c_exceptions_and_errors.html#part-4-index-errors-with-lists",
    "title": "Interactive Session 2C",
    "section": "Part 4: Index Errors with Lists",
    "text": "Part 4: Index Errors with Lists\n\n4.1 Accessing an Invalid Index\nAn IndexError occurs when you try to access an index that is out of the range of a list.\n\nStep 1: Run the following code to generate an IndexError.\nmy_list = [1, 2, 3]\nprint(my_list[5])\nStep 2: Observe the error message.\nIndexError: list index out of range\nStep 3: Explanation: Python is telling you that the index 5 is out of range for the list my_list, which only has indices 0, 1, 2.\nStep 4: Fix the Error: Access a valid index or use dynamic methods to avoid hardcoding indices.\nprint(my_list[2])  # Last valid index\n\n# or\n\nprint(my_list[-1])  # Access the last element using negative indexing"
  },
  {
    "objectID": "course-materials/interactive-sessions/2c_exceptions_and_errors.html#part-5-attribute-errors",
    "href": "course-materials/interactive-sessions/2c_exceptions_and_errors.html#part-5-attribute-errors",
    "title": "Interactive Session 2C",
    "section": "Part 5: Attribute Errors",
    "text": "Part 5: Attribute Errors\n\n5.1 Using Attributes Incorrectly\nAn AttributeError occurs when you try to access an attribute or method that doesn’t exist on the object.\n\nStep 1: Run the following code to generate an AttributeError.\nmy_string = \"Hello\"\nmy_string.append(\" World\")\nStep 2: Observe the error message.\nAttributeError: 'str' object has no attribute 'append'\nStep 3: Explanation: Python is telling you that the str object (a string) does not have an append method, which is a method for lists.\nStep 4: Fix the Error: Use string concatenation instead of append.\nmy_string = \"Hello\"\nmy_string = my_string + \" World\"\nprint(my_string)"
  },
  {
    "objectID": "course-materials/interactive-sessions/2c_exceptions_and_errors.html#part-6-tracing-errors-through-a-function-call-stack",
    "href": "course-materials/interactive-sessions/2c_exceptions_and_errors.html#part-6-tracing-errors-through-a-function-call-stack",
    "title": "Interactive Session 2C",
    "section": "Part 6: Tracing Errors Through a Function Call Stack",
    "text": "Part 6: Tracing Errors Through a Function Call Stack\n\n6.1 Understanding a Complicated Error Stack Trace\nErrors can sometimes appear deep within a function call, triggered by code that was written earlier in your script. When this happens, understanding the stack trace (the sequence of function calls leading to the error) is crucial for identifying the root cause. In this part of the exercise, you’ll explore an example where an error in a plotting function arises from an earlier mistake in your code.\n\nStep 1: Run the following code, which attempts to plot a simple line graph using Matplotlib.\n\nimport matplotlib.pyplot as plt\n\ndef generate_plot(data):\n    plt.plot(data)\n    plt.show()\n\n# Step 2: Introduce an error\nmy_data = [1, 2, \"three\", 4, 5]  # Mixing strings and integers in the list\n\n# Step 3: Call the function to generate the plot\ngenerate_plot(my_data)\n\nStep 2: Observe the error message.\n\nFile \"&lt;ipython-input-1&gt;\", line 5, in generate_plot\n    plt.plot(data)\n...\nFile \"/path/to/matplotlib/lines.py\", line XYZ, in _xy_from_xy\n    raise ValueError(\"some explanation about incompatible types\")\nValueError: could not convert string to float: 'three'\n\nStep 3: Explanation: This error occurs because the plot function in Matplotlib expects numerical data to plot. The error message points to a deeper issue in the lines.py file inside the Matplotlib library, but the actual problem originates from your my_data list, which includes a string (“three”) instead of a numeric value.\nStep 4: Trace the Error:\n\nThe error originates in the plt.plot(data) function call.\nMatplotlib’s internal functions (_xy_from_xy in this case) try to process the data but encounter an issue when they can’t convert the string “three” into a float.\n\nStep 5: Fix the Error: Correct the data by ensuring all elements are numeric.\nmy_data = [1, 2, 3, 4, 5]  # Correcting the list to contain only integers\ngenerate_plot(my_data)  # Now this will work without an error"
  },
  {
    "objectID": "course-materials/interactive-sessions/2c_exceptions_and_errors.html#part-7-tracing-errors-in-jupyter-notebooks",
    "href": "course-materials/interactive-sessions/2c_exceptions_and_errors.html#part-7-tracing-errors-in-jupyter-notebooks",
    "title": "Interactive Session 2C",
    "section": "Part 7: Tracing Errors in Jupyter Notebooks",
    "text": "Part 7: Tracing Errors in Jupyter Notebooks\nWhen you run code in a Jupyter Notebook, the Python interpreter refers to the code in the notebook cells as it generates a stack trace when an error occurs. Here’s how Jupyter Notebooks handle this:\n\nHow Jupyter Notebooks Generate Stack Traces\n\nCell Execution:\n\nEach time you run a cell in a Jupyter Notebook, the code in that cell is executed by the Python interpreter. The code from each cell is treated as part of a sequential script, but each cell is an individual execution block.\n\nInput Label:\n\nJupyter assigns each cell an input label, such as In [1]:, In [2]:, etc. This label is used to identify the specific cell where the code was executed.\n\nStack Trace Generation:\n\nWhen an error occurs, Python generates a stack trace that shows the sequence of function calls leading to the error. In a Jupyter Notebook, this stack trace includes references to the notebook cells that were executed.\nThe stack trace will point to the line number within the cell and the input label, such as In [2], indicating where in your notebook the error originated.\n\nExample Stack Trace in Jupyter:\n\nSuppose you have the following code in a cell labeled In [2]:\ndef divide(x, y):\n    return x / y\n\ndivide(10, 0)\nRunning this code will generate a ZeroDivisionError, and the stack trace might look like this:\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\n&lt;ipython-input-2-d7d8f8a6c1c1&gt; in &lt;module&gt;\n      2     return x / y\n      3 \n----&gt; 4 divide(10, 0)\n      5 \n\n&lt;ipython-input-2-d7d8f8a6c1c1&gt; in divide(x, y)\n      1 def divide(x, y):\n----&gt; 2     return x / y\n      3 \n      4 divide(10, 0)\nExplanation:\n\nThe Traceback (most recent call last) shows the series of calls leading to the error.\nThe &lt;ipython-input-2-d7d8f8a6c1c1&gt; refers to the code in cell In [2].\nThe stack trace pinpoints the exact line where the error occurred within that cell.\n\n\nMultiple Cell References:\n\nIf your code calls functions defined in different cells, the stack trace will show references to multiple cells. For example, if a function is defined in one cell and then called in another, the stack trace will include both cells in the sequence of calls.\n\nLimitations:\n\nThe stack trace in Jupyter Notebooks is specific to the cells that have been executed. If you modify a cell and re-run it, the new code is associated with that cell’s input label, and previous stack traces will not reflect those changes.\n\n\n\n\nSummary:\nIn Jupyter Notebooks, stack traces refer to the specific cells (In [X]) where the code was executed. The stack trace will show you the input label of the cell and the line number where the error occurred, helping you to quickly locate and fix issues in your notebook. Understanding how Jupyter references your code in stack traces is crucial for effective debugging.\n\n\nGeneral Summary of Stack Traces\n\nWhat to Look For: In complex stack traces, start by looking at the error message itself, which often appears at the bottom of the stack. Work your way backward through the stack to identify where in your code the problem originated.\nTracing Function Calls: Understand how data flows through your functions. An error in a deeply nested function may often be triggered by an incorrect input or state set earlier in the code."
  },
  {
    "objectID": "course-materials/interactive-sessions/2c_exceptions_and_errors.html#error-summary",
    "href": "course-materials/interactive-sessions/2c_exceptions_and_errors.html#error-summary",
    "title": "Interactive Session 2C",
    "section": "Error Summary",
    "text": "Error Summary\n\nAlways read the error message carefully; it usually points directly to the problem.\nSyntaxErrors - and, to a lesser extent, NameErrors are often due to small mistakes like typos, missing parentheses, or missing quotes.\nTypeErrors often occur when trying to perform operations on incompatible data types.\nAttributeErrors occur when you are trying to use a method that doesn’t exist for an object. These can also show up due to typos in your code that make the interpreter think you are trying to call a method.\nWhile every error type has a specific meaning, always check your code for typos when trying to debug an error. Many typos do not prevent the interpreter from running your code and the eventual error caused by a typo might be hard to interpret!\n\n\nBy the end of this session, you should feel more comfortable identifying and fixing common Python errors. This skill is critical for debugging and developing more complex programs in the future.\n\n\nEnd interactive session 2C"
  },
  {
    "objectID": "course-materials/live-coding/5a_selecting_and_filtering.html#overview",
    "href": "course-materials/live-coding/5a_selecting_and_filtering.html#overview",
    "title": "Live Coding Session 5A",
    "section": "Overview",
    "text": "Overview\nIn this session, we will be exploring how to select and filter data from DataFrames."
  },
  {
    "objectID": "course-materials/live-coding/5a_selecting_and_filtering.html#objectives",
    "href": "course-materials/live-coding/5a_selecting_and_filtering.html#objectives",
    "title": "Live Coding Session 5A",
    "section": "Objectives",
    "text": "Objectives\n\nApply various indexing methods to select rows and columns in dataframes.\nUse boolean logic to filter data based on values\nDevelop the ability to troubleshoot and debug in a live setting."
  },
  {
    "objectID": "course-materials/live-coding/5a_selecting_and_filtering.html#getting-started",
    "href": "course-materials/live-coding/5a_selecting_and_filtering.html#getting-started",
    "title": "Live Coding Session 5A",
    "section": "Getting Started",
    "text": "Getting Started\nWe will be using the data stored in the csv at this url:\n\nurl = 'https://bit.ly/eds217-studentdata'\n\nTo get the most out of this session, please follow these guidelines:\n\nPrepare Your Environment:\n\nMake sure JupyterLab is up and running on your machine.\nOpen a new Jupyter notebook where you can write your own code as we go along.\nMake sure to name the notebook something informative so you can refer back to it later.\n\nSetup Your Notebook:\nBefore we begin the live coding session, please set up your Jupyter notebook with the following structure. This will help you organize your notes and code as we progress through the lesson.\n\nCreate a new Jupyter notebook.\nIn the first cell, create a markdown cell with the title of our session:\n\n# Basic Pandas Selection and Filtering\n\nBelow that, create markdown cells for each of the following topics we’ll cover. Leave empty code cells between each markdown cell where you’ll write your code during the session:\n\n# Introduction to Pandas Selection and Filtering\n\n## 1. Setup\n\n[Empty Code Cell]\n\n## 2. Basic Selection\n\n[Empty Code Cell]\n\n## 3. Filtering Based on Column Values\n\n### 3a. Single Condition Filtering\n\n[Empty Code Cell]\n\n### 3b. Multiple Conditions with Logical Operators\n\n[Empty Code Cell]\n\n### 3c. Using the filter command\n\n[ Emptyh Code Cell]\n\n## 4. Combining Selection and Filtering\n\n[Empty Code Cell]\n\n## 5. Using .isin() for Multiple Values\n\n[Empty Code Cell]\n\n## 6. Filtering with String Methods\n\n[Empty Code Cell]\n\n## 7. Advanced Selection: .loc vs .iloc\n\n[Empty Code Cell]\n\n## Conclusion\nAs we progress through the live coding session, you’ll fill in the code cells with the examples we work on together.\nFeel free to add additional markdown cells for your own notes or observations throughout the session.\n\nBy setting up your notebook this way, you’ll have a clear structure to follow along with the lesson and easily reference specific topics later for review. Remember, you can always add more cells or modify the structure as needed during the session!\n\nParticipation:\n\nTry to code along with me during the session.\nFeel free to ask questions at any time. Remember, if you have a question, others probably do too!\n\nResources:\n\nI will be sharing snippets of code and notes. Make sure to take your own notes and save snippets in your notebook for future reference.\nCheck out our class data selection and filtering cheatsheet."
  },
  {
    "objectID": "course-materials/cheatsheets/functions.html",
    "href": "course-materials/cheatsheets/functions.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "In Python, a function is defined using the def keyword, followed by the function name and parentheses () that may include parameters.\n\n\nCode\ndef function_name(parameters):\n    # Function body\n    return result\n\n\n\n\n\n\n\nCode\ndef celsius_to_fahrenheit(celsius):\n    \"\"\"Convert Celsius to Fahrenheit.\"\"\"\n    fahrenheit = (celsius * 9/5) + 32\n    return fahrenheit\n\n\n\n\n\nCall a function by using its name followed by parentheses, and pass arguments if the function requires them.\n\n\nCode\ntemperature_celsius = 25\ntemperature_fahrenheit = celsius_to_fahrenheit(temperature_celsius)\nprint(temperature_fahrenheit)  # Output: 77.0\n\n\n77.0\n\n\n\n\n\n\n\n\n\n\nCode\ndef kilometers_to_miles(kilometers):\n    \"\"\"Convert kilometers to miles.\"\"\"\n    miles = kilometers * 0.621371\n    return miles\n\n# Usage\ndistance_km = 10\ndistance_miles = kilometers_to_miles(distance_km)\nprint(distance_miles)  # Output: 6.21371\n\n\n6.21371\n\n\n\n\n\ndef mps_to_kmph(mps):\n    \"\"\"Convert meters per second to kilometers per hour.\"\"\"\n    kmph = mps * 3.6\n    return kmph\n\n# Usage\nspeed_mps = 5\nspeed_kmph = mps_to_kmph(speed_mps)\nprint(speed_kmph)  # Output: 18.0\n\n\n\n\n\n\nYou can return multiple values from a function by using a tuple.\nimport statistics\n\ndef calculate_mean_std(data):\n    \"\"\"Calculate mean and standard deviation of a dataset.\"\"\"\n    mean = statistics.mean(data)\n    std_dev = statistics.stdev(data)\n    return mean, std_dev\n\n# Usage\ndata = [12, 15, 20, 22, 25]\nmean, std_dev = calculate_mean_std(data)\nprint(f\"Mean: {mean}, Standard Deviation: {std_dev}\")\n\n\n\n\nYou can set default values for parameters, making them optional when calling the function.\n\n\ndef convert_temperature(temp, from_unit='C', to_unit='F'):\n    \"\"\"Convert temperature between Celsius and Fahrenheit.\"\"\"\n    if from_unit == 'C' and to_unit == 'F':\n        return (temp * 9/5) + 32\n    elif from_unit == 'F' and to_unit == 'C':\n        return (temp - 32) * 5/9\n    else:\n        return temp  # No conversion needed\n\n# Usage\ntemp_in_fahrenheit = convert_temperature(25)  # Defaults to C to F\ntemp_in_celsius = convert_temperature(77, from_unit='F', to_unit='C')\nprint(temp_in_fahrenheit)  # Output: 77.0\nprint(temp_in_celsius)     # Output: 25.0\n\n\n\n\nYou can call a function using keyword arguments to make it clearer which arguments are being set, especially useful when many parameters are involved.\n# Call using keyword arguments\ntemp = convert_temperature(temp=25, from_unit='C', to_unit='F')\n\n\n\nA higher-order function is a function that can take other functions as arguments or return them as results.\n\n\ndef apply_conversion(conversion_func, data):\n    \"\"\"Apply a conversion function to a list of data.\"\"\"\n    return [conversion_func(value) for value in data]\n\n# Convert a list of temperatures from Celsius to Fahrenheit\ntemperatures_celsius = [0, 20, 30, 40]\ntemperatures_fahrenheit = apply_conversion(celsius_to_fahrenheit, temperatures_celsius)\nprint(temperatures_fahrenheit)  # Output: [32.0, 68.0, 86.0, 104.0]\n\n\n\n\n\n\nDegree days are a measure of heat accumulation used to predict plant and animal development rates.\ndef calculate_degree_days(daily_temps, base_temp=10):\n    \"\"\"Calculate degree days for a series of daily temperatures.\"\"\"\n    degree_days = 0\n    for temp in daily_temps:\n        if temp &gt; base_temp:\n            degree_days += temp - base_temp\n    return degree_days\n\n# Usage\ndaily_temps = [12, 15, 10, 18, 20, 7]\ndegree_days = calculate_degree_days(daily_temps)\nprint(degree_days)  # Output: 35\n\n\n\n\nFunctions encapsulate reusable code logic and can simplify complex operations.\nParameters allow for input variability, while return values provide output.\nUse default parameters and keyword arguments to enhance flexibility and readability.\nHigher-order functions enable more abstract and powerful code structures."
  },
  {
    "objectID": "course-materials/cheatsheets/functions.html#basics-of-functions",
    "href": "course-materials/cheatsheets/functions.html#basics-of-functions",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "In Python, a function is defined using the def keyword, followed by the function name and parentheses () that may include parameters.\n\n\nCode\ndef function_name(parameters):\n    # Function body\n    return result\n\n\n\n\n\n\n\nCode\ndef celsius_to_fahrenheit(celsius):\n    \"\"\"Convert Celsius to Fahrenheit.\"\"\"\n    fahrenheit = (celsius * 9/5) + 32\n    return fahrenheit\n\n\n\n\n\nCall a function by using its name followed by parentheses, and pass arguments if the function requires them.\n\n\nCode\ntemperature_celsius = 25\ntemperature_fahrenheit = celsius_to_fahrenheit(temperature_celsius)\nprint(temperature_fahrenheit)  # Output: 77.0\n\n\n77.0"
  },
  {
    "objectID": "course-materials/cheatsheets/functions.html#common-unit-conversions",
    "href": "course-materials/cheatsheets/functions.html#common-unit-conversions",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Code\ndef kilometers_to_miles(kilometers):\n    \"\"\"Convert kilometers to miles.\"\"\"\n    miles = kilometers * 0.621371\n    return miles\n\n# Usage\ndistance_km = 10\ndistance_miles = kilometers_to_miles(distance_km)\nprint(distance_miles)  # Output: 6.21371\n\n\n6.21371\n\n\n\n\n\ndef mps_to_kmph(mps):\n    \"\"\"Convert meters per second to kilometers per hour.\"\"\"\n    kmph = mps * 3.6\n    return kmph\n\n# Usage\nspeed_mps = 5\nspeed_kmph = mps_to_kmph(speed_mps)\nprint(speed_kmph)  # Output: 18.0"
  },
  {
    "objectID": "course-materials/cheatsheets/functions.html#handling-multiple-return-values",
    "href": "course-materials/cheatsheets/functions.html#handling-multiple-return-values",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "You can return multiple values from a function by using a tuple.\nimport statistics\n\ndef calculate_mean_std(data):\n    \"\"\"Calculate mean and standard deviation of a dataset.\"\"\"\n    mean = statistics.mean(data)\n    std_dev = statistics.stdev(data)\n    return mean, std_dev\n\n# Usage\ndata = [12, 15, 20, 22, 25]\nmean, std_dev = calculate_mean_std(data)\nprint(f\"Mean: {mean}, Standard Deviation: {std_dev}\")"
  },
  {
    "objectID": "course-materials/cheatsheets/functions.html#default-parameters",
    "href": "course-materials/cheatsheets/functions.html#default-parameters",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "You can set default values for parameters, making them optional when calling the function.\n\n\ndef convert_temperature(temp, from_unit='C', to_unit='F'):\n    \"\"\"Convert temperature between Celsius and Fahrenheit.\"\"\"\n    if from_unit == 'C' and to_unit == 'F':\n        return (temp * 9/5) + 32\n    elif from_unit == 'F' and to_unit == 'C':\n        return (temp - 32) * 5/9\n    else:\n        return temp  # No conversion needed\n\n# Usage\ntemp_in_fahrenheit = convert_temperature(25)  # Defaults to C to F\ntemp_in_celsius = convert_temperature(77, from_unit='F', to_unit='C')\nprint(temp_in_fahrenheit)  # Output: 77.0\nprint(temp_in_celsius)     # Output: 25.0"
  },
  {
    "objectID": "course-materials/cheatsheets/functions.html#using-keyword-arguments",
    "href": "course-materials/cheatsheets/functions.html#using-keyword-arguments",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "You can call a function using keyword arguments to make it clearer which arguments are being set, especially useful when many parameters are involved.\n# Call using keyword arguments\ntemp = convert_temperature(temp=25, from_unit='C', to_unit='F')"
  },
  {
    "objectID": "course-materials/cheatsheets/functions.html#higher-order-functions",
    "href": "course-materials/cheatsheets/functions.html#higher-order-functions",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "A higher-order function is a function that can take other functions as arguments or return them as results.\n\n\ndef apply_conversion(conversion_func, data):\n    \"\"\"Apply a conversion function to a list of data.\"\"\"\n    return [conversion_func(value) for value in data]\n\n# Convert a list of temperatures from Celsius to Fahrenheit\ntemperatures_celsius = [0, 20, 30, 40]\ntemperatures_fahrenheit = apply_conversion(celsius_to_fahrenheit, temperatures_celsius)\nprint(temperatures_fahrenheit)  # Output: [32.0, 68.0, 86.0, 104.0]"
  },
  {
    "objectID": "course-materials/cheatsheets/functions.html#practical-example-climate-data-analysis",
    "href": "course-materials/cheatsheets/functions.html#practical-example-climate-data-analysis",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Degree days are a measure of heat accumulation used to predict plant and animal development rates.\ndef calculate_degree_days(daily_temps, base_temp=10):\n    \"\"\"Calculate degree days for a series of daily temperatures.\"\"\"\n    degree_days = 0\n    for temp in daily_temps:\n        if temp &gt; base_temp:\n            degree_days += temp - base_temp\n    return degree_days\n\n# Usage\ndaily_temps = [12, 15, 10, 18, 20, 7]\ndegree_days = calculate_degree_days(daily_temps)\nprint(degree_days)  # Output: 35\n\n\n\n\nFunctions encapsulate reusable code logic and can simplify complex operations.\nParameters allow for input variability, while return values provide output.\nUse default parameters and keyword arguments to enhance flexibility and readability.\nHigher-order functions enable more abstract and powerful code structures."
  },
  {
    "objectID": "course-materials/live-coding/4d_data_import_export.html#overview",
    "href": "course-materials/live-coding/4d_data_import_export.html#overview",
    "title": "Live Coding Session 4D",
    "section": "Overview",
    "text": "Overview\nIn this session, we will be exploring data import using the read_csv() function in pandas. Live coding is a great way to learn programming as it allows you to see the process of writing code in real-time, including how to deal with unexpected issues and debug errors."
  },
  {
    "objectID": "course-materials/live-coding/4d_data_import_export.html#objectives",
    "href": "course-materials/live-coding/4d_data_import_export.html#objectives",
    "title": "Live Coding Session 4D",
    "section": "Objectives",
    "text": "Objectives\n\nUnderstand the fundamentals of flow control in Python.\nUse read_csv() options to handle different .csv file structures.\nLearn how to parse dates and handle missing data during import.\nLearn how to filter columns and handle large files.\n\nDevelop the ability to troubleshoot and debug in a live setting."
  },
  {
    "objectID": "course-materials/live-coding/4d_data_import_export.html#getting-started",
    "href": "course-materials/live-coding/4d_data_import_export.html#getting-started",
    "title": "Live Coding Session 4D",
    "section": "Getting Started",
    "text": "Getting Started\nTo get the most out of this session, please follow these guidelines:\nPrepare Your Environment: - Log into our server and start JupyterLab. - Open a new Jupyter notebook where you can write your own code as we go along. - Make sure to name the notebook something informative so you can refer back to it later.\n\nStep 1: Create a New Notebook\n\nOpen Jupyter Lab or Jupyter Notebook.\nCreate a new Python notebook.\nRename your notebook to pd_read_csv.ipynb.\n\n\n\nStep 2: Import Required Libraries\nIn the first cell of your notebook, import the necessary libraries:\n\nimport pandas as pd\nimport numpy as np\n\n\n\nStep 3: Set Up Data URLs\nTo ensure we’re all working with the same data, copy and paste the following URLs into a new code cell and run the cell (SHIFT-ENTER):\n\n# URLs for different CSV files we'll be using\nurl_basic = 'https://bit.ly/eds217-basic'\nurl_missing = 'https://bit.ly/eds217-missing'\nurl_dates = 'https://bit.ly/eds217-dates'\nurl_no_header = 'https://bit.ly/eds217-noheader'\nurl_tsv = 'https://bit.ly/eds217-tabs'\nurl_large = 'https://bit.ly/eds217-large'\n\n\n\nStep 4: Prepare Markdown Cells for Notes\nCreate several markdown cells throughout your notebook to take notes during the session. Here are some suggested headers:\n\nBasic Usage and Column Selection\nHandling Missing Data\nParsing Dates\nWorking with Files Without Headers\nWorking with Tab-Separated Values (TSV) Files\nHandling Large Files: Reading a Subset of Data\n\n\n\nStep 5: Create Code Cells for Each Topic\nUnder each markdown header, create empty code cells where you’ll write and execute code during the live session.\n\n\nStep 6: Final Preparations\n\nEnsure you have a stable internet connection to access the CSV files.\nHave the Pandas documentation page open in a separate tab for quick reference: https://pandas.pydata.org/docs/\n\n\n\nReady to Go!\nYou’re now set up and ready to follow along with the live coding session. Remember to actively code along and take notes in your markdown cells. Don’t hesitate to ask questions during the session!\nHappy coding!"
  },
  {
    "objectID": "course-materials/live-coding/4d_data_import_export.html#session-format",
    "href": "course-materials/live-coding/4d_data_import_export.html#session-format",
    "title": "Live Coding Session 4D",
    "section": "Session Format",
    "text": "Session Format\n\nIntroduction\n\nBrief discussion about the topic and its importance in data science.\n\n\n\nDemonstration\n\nI will demonstrate code examples live. Follow along and write the code into your own Jupyter notebook.\n\n\n\nPractice\n\nYou will have the opportunity to try exercises on your own to apply what you’ve learned.\n\n\n\nQ&A\n\nWe will have a Q&A session at the end where you can ask specific questions about the code, concepts, or issues encountered during the session."
  },
  {
    "objectID": "course-materials/live-coding/4d_data_import_export.html#after-the-session",
    "href": "course-materials/live-coding/4d_data_import_export.html#after-the-session",
    "title": "Live Coding Session 4D",
    "section": "After the Session",
    "text": "After the Session\n\nReview your notes and try to replicate the exercises on your own.\nExperiment with the code by modifying parameters or adding new features to deepen your understanding.\nCheck out our class read_csv() cheatsheet."
  },
  {
    "objectID": "course-materials/coding-colabs/4b_pandas_dataframes.html#introduction",
    "href": "course-materials/coding-colabs/4b_pandas_dataframes.html#introduction",
    "title": "Coding Colab",
    "section": "Introduction",
    "text": "Introduction\nIn this collaborative coding exercise, you’ll work with a partner to practice importing, cleaning, exploring, and analyzing DataFrames using pandas. You’ll be working with a dataset containing yearly visitor information about national parks in the United States.\nHelpful class CheatSheets:\nPandas DataFrames\nPandas PDF Cheat Sheet\nDataFrame Workflows"
  },
  {
    "objectID": "course-materials/coding-colabs/4b_pandas_dataframes.html#setup",
    "href": "course-materials/coding-colabs/4b_pandas_dataframes.html#setup",
    "title": "Coding Colab",
    "section": "Setup",
    "text": "Setup\nFirst, let’s import the necessary libraries and load our dataset.\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Load the dataset\nurl = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-09-17/national_parks.csv\"\nparks_df = pd.read_csv(url)"
  },
  {
    "objectID": "course-materials/coding-colabs/4b_pandas_dataframes.html#task-1-data-exploration-and-cleaning",
    "href": "course-materials/coding-colabs/4b_pandas_dataframes.html#task-1-data-exploration-and-cleaning",
    "title": "Coding Colab",
    "section": "Task 1: Data Exploration and Cleaning",
    "text": "Task 1: Data Exploration and Cleaning\nWith your partner, explore the DataFrame and perform some initial cleaning. Create cells in your notebook that provide the following information\n\n\n\n\n\n\nTip\n\n\n\nUse print() statements and/or f-strings to create your output in a way that makes it easy to understand your results.\n\n\n\nHow many rows and columns does the DataFrame have?\nWhat are the column names?\nWhat data types are used in each column?\nAre there any missing values in the DataFrame?\nRemove the rows where year is Total (these are summary rows we don’t need for our analysis).\nConvert the year column to numeric type."
  },
  {
    "objectID": "course-materials/coding-colabs/4b_pandas_dataframes.html#task-2-basic-filtering-and-analysis",
    "href": "course-materials/coding-colabs/4b_pandas_dataframes.html#task-2-basic-filtering-and-analysis",
    "title": "Coding Colab",
    "section": "Task 2: Basic Filtering and Analysis",
    "text": "Task 2: Basic Filtering and Analysis\nNow, let’s practice some basic filtering and analysis operations:\n\nCreate a new DataFrame containing only data for the years 2000-2015 and only data for National Parks (unit_type is National Park)\nFind the total number of visitors across all National Parks for each year from 2000-2015.\nCalculate the average yearly visitors for each National Park during the 2000-2015 period.\nIdentify the top 5 most visited National Parks (based on total visitors) during the 2000-2015 period."
  },
  {
    "objectID": "course-materials/coding-colabs/4b_pandas_dataframes.html#task-3-thinking-in-dataframes",
    "href": "course-materials/coding-colabs/4b_pandas_dataframes.html#task-3-thinking-in-dataframes",
    "title": "Coding Colab",
    "section": "Task 3: Thinking in DataFrames",
    "text": "Task 3: Thinking in DataFrames\n\nIn 2016, a blog post from 538.com explored these data. Take a look at the graphics in the post that use our data and discuss with your partner what steps and functions you think would be necessary to filter, group, and aggregate your dataframe in order to make any of the plots. See if you can make “rough drafts” of any of them using the simple DataFrame.plot() function."
  },
  {
    "objectID": "course-materials/coding-colabs/4b_pandas_dataframes.html#conclusion",
    "href": "course-materials/coding-colabs/4b_pandas_dataframes.html#conclusion",
    "title": "Coding Colab",
    "section": "Conclusion",
    "text": "Conclusion\nGreat job working through these exercises! You’ve practiced importing data, cleaning a dataset, exploring DataFrames, and performing various filtering and analysis operations using pandas. These skills are fundamental to data analysis in Python and will be valuable as you continue to work with more complex datasets."
  },
  {
    "objectID": "course-materials/interactive-sessions/2a_getting_help.html",
    "href": "course-materials/interactive-sessions/2a_getting_help.html",
    "title": "Interactive Session 2A",
    "section": "",
    "text": "Objective: Learn how to get help, work with variables, and explore methods available for different Python objects in Jupyter Notebooks.\nEstimated Time: 45-60 minutes"
  },
  {
    "objectID": "course-materials/interactive-sessions/2a_getting_help.html#getting-help-in-python",
    "href": "course-materials/interactive-sessions/2a_getting_help.html#getting-help-in-python",
    "title": "Interactive Session 2A",
    "section": "1. Getting Help in Python",
    "text": "1. Getting Help in Python\n\nUsing help()\n\nIn a Jupyter Notebook cell, type:\n\n\nCode\n  #| echo: false\n\nhelp(len)\n\n\nHelp on built-in function len in module builtins:\n\nlen(obj, /)\n    Return the number of items in a container.\n\n\n\nRun the cell to see detailed information about the len() function.\n\n\n\nTrying help() Yourself\n\nUse the help() function on other built-in functions like print or sum.\n\n\n\nUsing ? and ??\n\nType:\nRun the cell to see quick documentation.\nTry:\nThis gives more detailed information, including source code (if available)."
  },
  {
    "objectID": "course-materials/interactive-sessions/2a_getting_help.html#working-with-variables",
    "href": "course-materials/interactive-sessions/2a_getting_help.html#working-with-variables",
    "title": "Interactive Session 2A",
    "section": "2. Working with Variables",
    "text": "2. Working with Variables\n\nCreating Variables\n\nIn a new cell, create a few variables:\nUse type() to check the data type of each variable:\n\n\nCode\ntype(a)\ntype(b)\ntype(c)\n\n\nstr\n\n\n\n\n\nExploring Variables\n\nExperiment with creating your own variables and checking their types.\nChange the values and data types and see what happens."
  },
  {
    "objectID": "course-materials/interactive-sessions/2a_getting_help.html#exploring-methods-available-for-objects",
    "href": "course-materials/interactive-sessions/2a_getting_help.html#exploring-methods-available-for-objects",
    "title": "Interactive Session 2A",
    "section": "3. Exploring Methods Available for Objects",
    "text": "3. Exploring Methods Available for Objects\n\nUsing dir()\n\nUse dir() to explore available methods for objects:\n\n\nCode\ndir(a)\ndir(b)\ndir(c)\n\n\n['__add__',\n '__class__',\n '__contains__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getnewargs__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rmod__',\n '__rmul__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'capitalize',\n 'casefold',\n 'center',\n 'count',\n 'encode',\n 'endswith',\n 'expandtabs',\n 'find',\n 'format',\n 'format_map',\n 'index',\n 'isalnum',\n 'isalpha',\n 'isascii',\n 'isdecimal',\n 'isdigit',\n 'isidentifier',\n 'islower',\n 'isnumeric',\n 'isprintable',\n 'isspace',\n 'istitle',\n 'isupper',\n 'join',\n 'ljust',\n 'lower',\n 'lstrip',\n 'maketrans',\n 'partition',\n 'removeprefix',\n 'removesuffix',\n 'replace',\n 'rfind',\n 'rindex',\n 'rjust',\n 'rpartition',\n 'rsplit',\n 'rstrip',\n 'split',\n 'splitlines',\n 'startswith',\n 'strip',\n 'swapcase',\n 'title',\n 'translate',\n 'upper',\n 'zfill']\n\n\n\n\n\nUsing help() with Methods\n\nPick a method from the list returned by dir() and use help() to learn more about it:\n\n\nCode\nhelp(c.upper)\n\n\nHelp on built-in function upper:\n\nupper() method of builtins.str instance\n    Return a copy of the string converted to uppercase.\n\n\n\n\n\n\nExploring Methods\n\nTry calling a method on your variables:\n\n\n'HELLO, WORLD!'\n\n\n\n\n```\n\nEnd interactive session 2A"
  },
  {
    "objectID": "course-materials/coding-colabs/5c_cleaning_data.html",
    "href": "course-materials/coding-colabs/5c_cleaning_data.html",
    "title": "Day 5: 🙌 Coding Colab",
    "section": "",
    "text": "In this collaborative coding exercise, you will work together and apply your new data cleaning skills to a simple dataframe that has a suprising number of problems."
  },
  {
    "objectID": "course-materials/coding-colabs/5c_cleaning_data.html#introduction",
    "href": "course-materials/coding-colabs/5c_cleaning_data.html#introduction",
    "title": "Day 5: 🙌 Coding Colab",
    "section": "",
    "text": "In this collaborative coding exercise, you will work together and apply your new data cleaning skills to a simple dataframe that has a suprising number of problems."
  },
  {
    "objectID": "course-materials/coding-colabs/5c_cleaning_data.html#resources",
    "href": "course-materials/coding-colabs/5c_cleaning_data.html#resources",
    "title": "Day 5: 🙌 Coding Colab",
    "section": "Resources",
    "text": "Resources\nHere’s our course cheatsheet on cleaning data:\n\nPandas Cleaning Cheatsheet\n\nFeel free to refer to this cheatsheet throughout the exercise if you need a quick reminder about syntax or functionality."
  },
  {
    "objectID": "course-materials/coding-colabs/5c_cleaning_data.html#setup",
    "href": "course-materials/coding-colabs/5c_cleaning_data.html#setup",
    "title": "Day 5: 🙌 Coding Colab",
    "section": "Setup",
    "text": "Setup\nFirst, let’s import the necessary libraries and load an example messy dataframe.\n\nimport pandas as pd\nimport numpy as np\n\nurl = 'https://bit.ly/messy_csv'\nmessy_df = pd.read_csv(url)"
  },
  {
    "objectID": "course-materials/coding-colabs/5c_cleaning_data.html#practical-exercise-cleaning-a-messy-environmental-dataset",
    "href": "course-materials/coding-colabs/5c_cleaning_data.html#practical-exercise-cleaning-a-messy-environmental-dataset",
    "title": "Day 5: 🙌 Coding Colab",
    "section": "Practical Exercise: Cleaning a Messy Environmental Dataset",
    "text": "Practical Exercise: Cleaning a Messy Environmental Dataset\nLet’s apply what we’ve learned so far to clean the messy environmental dataset.\nYour task is to clean this dataframe by\n\nRemoving duplicates\nHandling missing values (either fill or dropna to remove rows with missing data)\nEnsuring consistent data types (dates, strings)\nFormatting the ‘site’ column for consistency\nMaking sure all column names are lower case, without whitespace.\n\nTry to implement these steps using the techniques we’ve learned.\n\nEnd Coding Colab Session (Day 4)"
  },
  {
    "objectID": "course-materials/cheatsheets/first_steps.html",
    "href": "course-materials/cheatsheets/first_steps.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Variables are containers for storing data values.\nPython has no command for declaring a variable: it is created the moment you first assign a value to it.\n\n\n\nCode\nx = 5\nname = \"Alice\"\n\n\n\n\n\n\nPython has various data types including:\n\nint (integer): A whole number, positive or negative, without decimals.\nfloat (floating point number): A number, positive or negative, containing one or more decimals.\nstr (string): A sequence of characters in quotes.\nbool (boolean): Represents True or False.\n\n\n\n\nCode\nage = 30        # int\ntemperature = 20.5  # float\nname = \"Bob\"    # str\nis_valid = True # bool\n\n\n\n\n\n\n\n\n\nUsed with numeric values to perform common mathematical operations:\n\n\n\n\nOperator\nDescription\n\n\n\n\n+\nAddition\n\n\n-\nSubtraction\n\n\n*\nMultiplication\n\n\n/\nDivision\n\n\n%\nModulus\n\n\n**\nExponentiation\n\n\n//\nFloor division\n\n\n\n\n\n\n\n\nCode\nx = 10\ny = 3\nprint(x + y)  # 13\nprint(x - y)  # 7\nprint(x * y)  # 30\nprint(x / y)  # 3.3333\nprint(x % y)  # 1\nprint(x ** y) # 1000\nprint(x // y) # 3\n\n\n13\n7\n30\n3.3333333333333335\n1\n1000\n3\n\n\n\n\n\n\nUsed to combine conditional statements:\n\n\n\n\nOperator\nDescription\n\n\n\n\nand\nReturns True if both statements are true\n\n\nor\nReturns True if one of the statements is true\n\n\nnot\nReverse the result, returns False if the result is true\n\n\n\n\n\n\n\n\nCode\nx = True\ny = False\nprint(x and y)  # False\nprint(x or y)   # True\nprint(not x)    # False\n\n\nFalse\nTrue\nFalse\n\n\n\n\n\n\nStrings in Python are surrounded by either single quotation marks, or double quotation marks.\n\n\n\nCode\nhello = \"Hello\"\nworld = 'World'\n\n# Modern approach (f-strings) - PREFERRED\nprint(f\"{hello} {world}\")  # Hello World\n\n# Older approach (string concatenation) - you'll see this in code\nprint(hello + \" \" + world)  # Hello World\n\n\nHello World\nHello World\n\n\n\nStrings can be indexed with the first character having index 0.\n\n\n\nCode\na = \"Hello, World!\"\nprint(a[1])   # e\n\n\ne\n\n\n\nSlicing strings:\n\n\n\nCode\nb = \"Hello, World!\"\nprint(b[2:5])  # llo\n\n\nllo\n\n\n\n\n\n\n\n\nCode\n# This is a comment\nprint(\"Hello, World!\")  # Prints Hello, World!\n\n\nHello, World!\n\n\nThis cheatsheet covers the very basics to get you started with Python. Experiment with these concepts to understand them better!"
  },
  {
    "objectID": "course-materials/cheatsheets/first_steps.html#variables-and-data-types",
    "href": "course-materials/cheatsheets/first_steps.html#variables-and-data-types",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Variables are containers for storing data values.\nPython has no command for declaring a variable: it is created the moment you first assign a value to it.\n\n\n\nCode\nx = 5\nname = \"Alice\"\n\n\n\n\n\n\nPython has various data types including:\n\nint (integer): A whole number, positive or negative, without decimals.\nfloat (floating point number): A number, positive or negative, containing one or more decimals.\nstr (string): A sequence of characters in quotes.\nbool (boolean): Represents True or False.\n\n\n\n\nCode\nage = 30        # int\ntemperature = 20.5  # float\nname = \"Bob\"    # str\nis_valid = True # bool"
  },
  {
    "objectID": "course-materials/cheatsheets/first_steps.html#basic-operations",
    "href": "course-materials/cheatsheets/first_steps.html#basic-operations",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Used with numeric values to perform common mathematical operations:\n\n\n\n\nOperator\nDescription\n\n\n\n\n+\nAddition\n\n\n-\nSubtraction\n\n\n*\nMultiplication\n\n\n/\nDivision\n\n\n%\nModulus\n\n\n**\nExponentiation\n\n\n//\nFloor division\n\n\n\n\n\n\n\n\nCode\nx = 10\ny = 3\nprint(x + y)  # 13\nprint(x - y)  # 7\nprint(x * y)  # 30\nprint(x / y)  # 3.3333\nprint(x % y)  # 1\nprint(x ** y) # 1000\nprint(x // y) # 3\n\n\n13\n7\n30\n3.3333333333333335\n1\n1000\n3\n\n\n\n\n\n\nUsed to combine conditional statements:\n\n\n\n\nOperator\nDescription\n\n\n\n\nand\nReturns True if both statements are true\n\n\nor\nReturns True if one of the statements is true\n\n\nnot\nReverse the result, returns False if the result is true\n\n\n\n\n\n\n\n\nCode\nx = True\ny = False\nprint(x and y)  # False\nprint(x or y)   # True\nprint(not x)    # False\n\n\nFalse\nTrue\nFalse\n\n\n\n\n\n\nStrings in Python are surrounded by either single quotation marks, or double quotation marks.\n\n\n\nCode\nhello = \"Hello\"\nworld = 'World'\n\n# Modern approach (f-strings) - PREFERRED\nprint(f\"{hello} {world}\")  # Hello World\n\n# Older approach (string concatenation) - you'll see this in code\nprint(hello + \" \" + world)  # Hello World\n\n\nHello World\nHello World\n\n\n\nStrings can be indexed with the first character having index 0.\n\n\n\nCode\na = \"Hello, World!\"\nprint(a[1])   # e\n\n\ne\n\n\n\nSlicing strings:\n\n\n\nCode\nb = \"Hello, World!\"\nprint(b[2:5])  # llo\n\n\nllo"
  },
  {
    "objectID": "course-materials/cheatsheets/first_steps.html#printing-and-commenting",
    "href": "course-materials/cheatsheets/first_steps.html#printing-and-commenting",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Code\n# This is a comment\nprint(\"Hello, World!\")  # Prints Hello, World!\n\n\nHello, World!\n\n\nThis cheatsheet covers the very basics to get you started with Python. Experiment with these concepts to understand them better!"
  },
  {
    "objectID": "course-materials/interactive-sessions/8a_github.html",
    "href": "course-materials/interactive-sessions/8a_github.html",
    "title": "Interactive Session",
    "section": "",
    "text": "This session contains detailed instructions for creating a new GitHub repository and pushing your EDS 217 course work to it. It also covers how to clean your Jupyter notebooks before committing them to ensure your repository is clean and professional.\n\n\n\n\n\n\nWarning\n\n\n\nJupyter notebooks files can be hard to use in github because they contain information about code execution order and output in the file. For this reason, you should clean notebooks before pushing them to a github repo. We will clean them using the nbstripout python package"
  },
  {
    "objectID": "course-materials/interactive-sessions/8a_github.html#introduction",
    "href": "course-materials/interactive-sessions/8a_github.html#introduction",
    "title": "Interactive Session",
    "section": "",
    "text": "This session contains detailed instructions for creating a new GitHub repository and pushing your EDS 217 course work to it. It also covers how to clean your Jupyter notebooks before committing them to ensure your repository is clean and professional.\n\n\n\n\n\n\nWarning\n\n\n\nJupyter notebooks files can be hard to use in github because they contain information about code execution order and output in the file. For this reason, you should clean notebooks before pushing them to a github repo. We will clean them using the nbstripout python package"
  },
  {
    "objectID": "course-materials/interactive-sessions/8a_github.html#steps-to-setting-up-a-github-repo-for-your-coursework",
    "href": "course-materials/interactive-sessions/8a_github.html#steps-to-setting-up-a-github-repo-for-your-coursework",
    "title": "Interactive Session",
    "section": "Steps to Setting up a GitHub repo for your coursework:",
    "text": "Steps to Setting up a GitHub repo for your coursework:\n\nCreate a new repository on GitHub\nInitialize a local Git repository\nClean Jupyter notebooks of output and execution data\nAdd, commit, and push files to a GitHub repository"
  },
  {
    "objectID": "course-materials/interactive-sessions/8a_github.html#creating-a-new-github-repository",
    "href": "course-materials/interactive-sessions/8a_github.html#creating-a-new-github-repository",
    "title": "Interactive Session",
    "section": "Creating a New GitHub Repository",
    "text": "Creating a New GitHub Repository\nLet’s start by creating a new repository on GitHub:\n\nLog in to your GitHub account\nClick the ‘+’ icon in the top-right corner and select ‘New repository’\nName your repository (e.g., “EDS-217-Course-Work”)\nAdd a description (optional)\nChoose to make the repository public or private\nDon’t initialize the repository with a README, .gitignore, or license\nClick ‘Create repository’\n\nAfter creating the repository, you’ll see a page with instructions. We’ll use these in the next steps."
  },
  {
    "objectID": "course-materials/interactive-sessions/8a_github.html#initializing-a-local-git-repository",
    "href": "course-materials/interactive-sessions/8a_github.html#initializing-a-local-git-repository",
    "title": "Interactive Session",
    "section": "Initializing a Local Git Repository",
    "text": "Initializing a Local Git Repository\nNow, let’s set up your local directory as a Git repository:\n\nOpen a terminal on the class workbench server\nNavigate to your course work directory:\n\n\n\nCode\ncd path/to/your/EDS-217\n\n\n\nInitialize the repository:\n\n\n\nCode\ngit init\n\n\n\nAdd your GitHub repository as the remote origin:\n\n\n\nCode\ngit remote add origin https://github.com/your-username/EDS-217-Course-Work.git\n\n\nReplace your-username with your actual GitHub username."
  },
  {
    "objectID": "course-materials/interactive-sessions/8a_github.html#cleaning-jupyter-notebooks",
    "href": "course-materials/interactive-sessions/8a_github.html#cleaning-jupyter-notebooks",
    "title": "Interactive Session",
    "section": "Cleaning Jupyter Notebooks",
    "text": "Cleaning Jupyter Notebooks\nBefore we commit our notebooks, let’s clean them to remove output cells and execution data:\n\nInstall the nbstripout tool if you haven’t already:\n\n\n\nCode\npip install nbstripout\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBy default, the server won’t add a new python package to the main package repository on workbench. For this reason, you will see a warning when running the pip command that looks something like this:\nWARNING: The script nbstripout is installed in '/Users/[your user id]/.local/bin' which is not on PATH\n\n\nTherefore, we need to access the nbsripout command by specifying it’s location in your local user folder:\n\nConfigure nbstripout for your repository:\n\n\n\nCode\n~/.local/bin/nbstripout --install --attributes .gitattributes\n\n\nThis sets up nbstripout to automatically clean your notebooks when you commit them."
  },
  {
    "objectID": "course-materials/interactive-sessions/8a_github.html#adding-committing-and-pushing-files",
    "href": "course-materials/interactive-sessions/8a_github.html#adding-committing-and-pushing-files",
    "title": "Interactive Session",
    "section": "Adding, Committing, and Pushing Files",
    "text": "Adding, Committing, and Pushing Files\nNow we’re ready to add our files to the repository:\n\nAdd all files in the directory:\n\n\n\nCode\ngit add .\n\n\n\nCommit the files:\n\n\n\nCode\ngit commit -m \"Initial commit: Adding EDS 217 course work\"\n\n\n\nPush the files to GitHub:\n\n\n\nCode\ngit push -u origin main\n\n\nNote: If your default branch is named “master” instead of “main”, use git push -u origin master."
  },
  {
    "objectID": "course-materials/interactive-sessions/8a_github.html#verifying-your-repository",
    "href": "course-materials/interactive-sessions/8a_github.html#verifying-your-repository",
    "title": "Interactive Session",
    "section": "Verifying Your Repository",
    "text": "Verifying Your Repository\n\nGo to your GitHub repository page in your web browser\nRefresh the page\nYou should now see all your course files listed in the repository"
  },
  {
    "objectID": "course-materials/interactive-sessions/8a_github.html#conclusion",
    "href": "course-materials/interactive-sessions/8a_github.html#conclusion",
    "title": "Interactive Session",
    "section": "Conclusion",
    "text": "Conclusion\nCongratulations! You’ve successfully created a GitHub repository for your EDS 217 course work, cleaned your Jupyter notebooks, and pushed your files to GitHub. This process helps you maintain a clean, professional repository of your work that you can easily share or refer back to in the future."
  },
  {
    "objectID": "course-materials/interactive-sessions/8a_github.html#additional-resources",
    "href": "course-materials/interactive-sessions/8a_github.html#additional-resources",
    "title": "Interactive Session",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nGitHub Docs: Creating a new repository\nGit Documentation\nnbstripout Documentation"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day3.html#introduction",
    "href": "course-materials/eod-practice/eod-day3.html#introduction",
    "title": "Day 3: Tasks & activities",
    "section": "Introduction",
    "text": "Introduction\nIn this end-of-day activity, we’ll practice the concepts you learned today: control flows and pandas Series analysis. We’ll work with real student test score data to apply if/else statements, loops, and Series operations that you learned in today’s sessions."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day3.html#learning-objectives",
    "href": "course-materials/eod-practice/eod-day3.html#learning-objectives",
    "title": "Day 3: Tasks & activities",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy completing this exercise, you will practice:\n\nControl Flow Concepts (from Sessions 3a & 3b):\n\nUsing if/elif/else statements for decision making\nUsing for loops to iterate through data\nCombining control flows with data analysis\n\nSeries Operations (from Session 3c):\n\nCreating Pandas Series with custom indices\nUsing Series statistical methods (.mean(), .median(), .max(), .min())\nBasic Series indexing and slicing\nApplying NumPy statistical functions to Series"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day3.html#setup",
    "href": "course-materials/eod-practice/eod-day3.html#setup",
    "title": "Day 3: Tasks & activities",
    "section": "Setup",
    "text": "Setup\nLet’s import the libraries we learned about today:\n\n\nCode\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day3.html#part-1-creating-and-analyzing-test-scores",
    "href": "course-materials/eod-practice/eod-day3.html#part-1-creating-and-analyzing-test-scores",
    "title": "Day 3: Tasks & activities",
    "section": "Part 1: Creating and Analyzing Test Scores",
    "text": "Part 1: Creating and Analyzing Test Scores\nLet’s work with a realistic dataset of monthly test scores for a student throughout the academic year.\n\nTask 1: Create the Test Scores Series\nCreate a pandas Series with the following monthly test scores:\n\n\nCode\n# Monthly test scores (September through June)\nscores_data = [78, 85, 92, 88, 79, 83, 91, 87, 89, 94]\nmonths = ['Sep', 'Oct', 'Nov', 'Dec', 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\n\n# Create the Series\nscores = pd.Series(scores_data, index=months)\nprint(\"Monthly Test Scores:\")\nprint(scores)\n\n\nMonthly Test Scores:\nSep    78\nOct    85\nNov    92\nDec    88\nJan    79\nFeb    83\nMar    91\nApr    87\nMay    89\nJun    94\ndtype: int64\n\n\n\n\nTask 2: Basic Statistical Analysis\nUsing the Series methods you learned today, calculate:\n\n\nCode\n# Calculate statistics using Series methods\naverage_score = scores.mean()\nhighest_score = scores.max()\nlowest_score = scores.min()\nmedian_score = scores.median()\n\nprint(f\"Average score: {average_score:.2f}\")\nprint(f\"Highest score: {highest_score}\")\nprint(f\"Lowest score: {lowest_score}\")\nprint(f\"Median score: {median_score:.2f}\")\n\n\nAverage score: 86.60\nHighest score: 94\nLowest score: 78\nMedian score: 87.50\n\n\n\n\nTask 3: Applying Control Flows\nNow let’s use the control flow concepts from today to analyze the scores:\n\n3a. Performance Categories\nUse if/elif/else statements to categorize the average score:\n\n\nCode\n# Categorize performance based on average score\nif average_score &gt;= 90:\n    performance = \"Excellent\"\nelif average_score &gt;= 80:\n    performance = \"Good\"\nelif average_score &gt;= 70:\n    performance = \"Satisfactory\"\nelse:\n    performance = \"Needs Improvement\"\n\nprint(f\"Overall performance: {performance} (Average: {average_score:.2f})\")\n\n\nOverall performance: Good (Average: 86.60)\n\n\n\n\n3b. Month-by-Month Analysis\nUse a for loop to analyze each month’s performance:\n\n\nCode\nprint(\"\\nMonth-by-Month Performance Analysis:\")\nprint(\"=\" * 40)\n\nfor month in scores.index:\n    score = scores[month]\n    \n    # Use if/else to provide feedback for each month\n    if score &gt;= 90:\n        feedback = \"Outstanding!\"\n    elif score &gt;= 85:\n        feedback = \"Great work!\"\n    elif score &gt;= 80:\n        feedback = \"Good job!\"\n    elif score &gt;= 75:\n        feedback = \"Solid effort!\"\n    else:\n        feedback = \"Room for improvement\"\n    \n    print(f\"{month}: {score} - {feedback}\")\n\n\n\nMonth-by-Month Performance Analysis:\n========================================\nSep: 78 - Solid effort!\nOct: 85 - Great work!\nNov: 92 - Outstanding!\nDec: 88 - Great work!\nJan: 79 - Solid effort!\nFeb: 83 - Good job!\nMar: 91 - Outstanding!\nApr: 87 - Great work!\nMay: 89 - Great work!\nJun: 94 - Outstanding!"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day3.html#part-2-series-operations-and-comparisons",
    "href": "course-materials/eod-practice/eod-day3.html#part-2-series-operations-and-comparisons",
    "title": "Day 3: Tasks & activities",
    "section": "Part 2: Series Operations and Comparisons",
    "text": "Part 2: Series Operations and Comparisons\n\nTask 4: Finding Specific Months\nUse basic Series indexing to explore the data:\n\n\nCode\n# Find specific months using indexing (concepts from Session 3c)\nprint(\"Score in December:\", scores['Dec'])\nprint(\"Score in May:\", scores['May'])\n\n# Using basic slicing (taught in Session 3c)\nprint(\"\\nFirst three months:\")\nprint(scores[:3])\n\nprint(\"\\nLast three months:\")\nprint(scores[-3:])\n\n\nScore in December: 88\nScore in May: 89\n\nFirst three months:\nSep    78\nOct    85\nNov    92\ndtype: int64\n\nLast three months:\nApr    87\nMay    89\nJun    94\ndtype: int64\n\n\n\n\nTask 5: Comparing Performance Periods\nLet’s analyze different parts of the year using basic slicing:\n\n\nCode\n# Fall semester (first 4 months)\nfall_scores = scores[:4]\nfall_average = fall_scores.mean()\n\n# Spring semester (last 6 months) \nspring_scores = scores[4:]\nspring_average = spring_scores.mean()\n\nprint(f\"Fall semester average: {fall_average:.2f}\")\nprint(f\"Spring semester average: {spring_average:.2f}\")\n\n# Use control flow to compare performance\nif spring_average &gt; fall_average:\n    improvement = spring_average - fall_average\n    print(f\"Improvement in spring: +{improvement:.2f} points!\")\nelif fall_average &gt; spring_average:\n    decline = fall_average - spring_average\n    print(f\"Performance declined in spring: -{decline:.2f} points\")\nelse:\n    print(\"Performance remained consistent between semesters\")\n\n\nFall semester average: 85.75\nSpring semester average: 87.17\nImprovement in spring: +1.42 points!"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day3.html#part-3-advanced-analysis-with-numpy-functions",
    "href": "course-materials/eod-practice/eod-day3.html#part-3-advanced-analysis-with-numpy-functions",
    "title": "Day 3: Tasks & activities",
    "section": "Part 3: Advanced Analysis with NumPy Functions",
    "text": "Part 3: Advanced Analysis with NumPy Functions\n\nTask 6: Using NumPy Statistical Functions\nApply NumPy functions to our Series (as learned in Session 3c):\n\n\nCode\n# NumPy statistical functions work on Pandas Series!\nnp_mean = np.mean(scores)\nnp_std = np.std(scores)\nnp_sum = np.sum(scores)\n\nprint(f\"Using NumPy functions:\")\nprint(f\"Mean: {np_mean:.2f}\")\nprint(f\"Standard deviation: {np_std:.2f}\")\nprint(f\"Total points: {np_sum}\")\n\n# Determine consistency using standard deviation\nif np_std &lt; 5:\n    consistency = \"Very consistent\"\nelif np_std &lt; 8:\n    consistency = \"Moderately consistent\"\nelse:\n    consistency = \"Inconsistent\"\n\nprint(f\"Performance consistency: {consistency} (std dev: {np_std:.2f})\")\n\n\nUsing NumPy functions:\nMean: 86.60\nStandard deviation: 5.08\nTotal points: 866\nPerformance consistency: Moderately consistent (std dev: 5.08)"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day3.html#part-4-challenge---combining-all-concepts",
    "href": "course-materials/eod-practice/eod-day3.html#part-4-challenge---combining-all-concepts",
    "title": "Day 3: Tasks & activities",
    "section": "Part 4: Challenge - Combining All Concepts",
    "text": "Part 4: Challenge - Combining All Concepts\n\nTask 7: Comprehensive Analysis\nCombine control flows, Series operations, and NumPy functions:\n\n\nCode\nprint(\"Comprehensive Performance Report\")\nprint(\"=\" * 35)\n\n# Count months above/below average using a for loop and control flow\nabove_average_count = 0\nbelow_average_count = 0\n\nprint(f\"Overall average: {average_score:.2f}\")\nprint(\"\\nMonthly performance relative to average:\")\n\nfor month in scores.index:\n    score = scores[month]\n    if score &gt; average_score:\n        status = \"Above average\"\n        above_average_count += 1\n    elif score &lt; average_score:\n        status = \"Below average\"\n        below_average_count += 1\n    else:\n        status = \"At average\"\n    \n    difference = score - average_score\n    print(f\"{month}: {score} ({status}, {difference:+.2f})\")\n\nprint(f\"\\nSummary:\")\nprint(f\"Months above average: {above_average_count}\")\nprint(f\"Months below average: {below_average_count}\")\n\n# Final recommendation using control flow\nif above_average_count &gt; below_average_count:\n    print(\"Recommendation: Strong performance overall! Keep up the good work.\")\nelif below_average_count &gt; above_average_count:\n    print(\"Recommendation: Focus on consistency. Consider study habit adjustments.\")\nelse:\n    print(\"Recommendation: Balanced performance. Work on achieving more peak months.\")\n\n\nComprehensive Performance Report\n===================================\nOverall average: 86.60\n\nMonthly performance relative to average:\nSep: 78 (Below average, -8.60)\nOct: 85 (Below average, -1.60)\nNov: 92 (Above average, +5.40)\nDec: 88 (Above average, +1.40)\nJan: 79 (Below average, -7.60)\nFeb: 83 (Below average, -3.60)\nMar: 91 (Above average, +4.40)\nApr: 87 (Above average, +0.40)\nMay: 89 (Above average, +2.40)\nJun: 94 (Above average, +7.40)\n\nSummary:\nMonths above average: 6\nMonths below average: 4\nRecommendation: Strong performance overall! Keep up the good work."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day3.html#conclusion",
    "href": "course-materials/eod-practice/eod-day3.html#conclusion",
    "title": "Day 3: Tasks & activities",
    "section": "Conclusion",
    "text": "Conclusion\nIn this activity, you practiced:\n✅ Control Flow Concepts: - if/elif/else statements for categorizing and comparing data - for loops for iterating through Series data - Combining conditions with data analysis\n✅ Pandas Series Operations: - Creating Series with custom indices - Statistical methods (.mean(), .max(), .min(), .median()) - Basic indexing (series['key']) and slicing (series[:3])\n✅ NumPy Integration: - Using NumPy statistical functions on Pandas Series - Understanding how NumPy and Pandas work together\nThese are the foundational skills that will support all your future data science work. Great job applying today’s concepts!"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day3.html#additional-practice-optional",
    "href": "course-materials/eod-practice/eod-day3.html#additional-practice-optional",
    "title": "Day 3: Tasks & activities",
    "section": "Additional Practice (Optional)",
    "text": "Additional Practice (Optional)\nTry creating your own Series with different data (maybe daily temperatures, stock prices, or sports scores) and apply the same analysis techniques you used today.\n\nEnd Activity Session (Day 3)"
  },
  {
    "objectID": "course-materials/cheatsheets/data_grouping.html",
    "href": "course-materials/cheatsheets/data_grouping.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Grouping data allows you to split your DataFrame into groups based on one or more columns.\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Sample DataFrame\ndf = pd.DataFrame({\n    'category': ['A', 'B', 'A', 'B', 'A'],\n    'value': [1, 2, 3, 4, 5]\n})\nprint(df)\n\n\n  category  value\n0        A      1\n1        B      2\n2        A      3\n3        B      4\n4        A      5\n\n\n\n\n\n\nCode\n# Group by 'category'\ngrouped = df.groupby('category')"
  },
  {
    "objectID": "course-materials/cheatsheets/data_grouping.html#grouping-data",
    "href": "course-materials/cheatsheets/data_grouping.html#grouping-data",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Grouping data allows you to split your DataFrame into groups based on one or more columns.\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Sample DataFrame\ndf = pd.DataFrame({\n    'category': ['A', 'B', 'A', 'B', 'A'],\n    'value': [1, 2, 3, 4, 5]\n})\nprint(df)\n\n\n  category  value\n0        A      1\n1        B      2\n2        A      3\n3        B      4\n4        A      5\n\n\n\n\n\n\nCode\n# Group by 'category'\ngrouped = df.groupby('category')"
  },
  {
    "objectID": "course-materials/cheatsheets/data_grouping.html#aggregating-data",
    "href": "course-materials/cheatsheets/data_grouping.html#aggregating-data",
    "title": "EDS 217 Cheatsheet",
    "section": "Aggregating Data",
    "text": "Aggregating Data\nAfter grouping, you can apply various aggregation functions to summarize the data within each group.\n\nBasic aggregation\n\n\nCode\n# Basic aggregations\nprint(grouped['value'].mean())\nprint(grouped['value'].sum())\n\n\ncategory\nA    3.0\nB    3.0\nName: value, dtype: float64\ncategory\nA    9\nB    6\nName: value, dtype: int64\n\n\n\n\nDoing multiple aggregations at the same time using agg()\n\n\nCode\n# Multiple aggregations\nprint(grouped['value'].agg(['mean', 'sum', 'count']))\n\n\n          mean  sum  count\ncategory                  \nA          3.0    9      3\nB          3.0    6      2\n\n\n\n\nAggregation using a custom function\n\n\nCode\n# Custom aggregation function\ndef range_func(x):\n    return x.max() - x.min()\n\nprint(grouped['value'].agg(range_func))\n\n\ncategory\nA    4\nB    2\nName: value, dtype: int64"
  },
  {
    "objectID": "course-materials/cheatsheets/data_grouping.html#common-aggregation-functions",
    "href": "course-materials/cheatsheets/data_grouping.html#common-aggregation-functions",
    "title": "EDS 217 Cheatsheet",
    "section": "Common Aggregation Functions",
    "text": "Common Aggregation Functions\n\nmean(): Average\nsum(): Sum of values\ncount(): Count of non-null values\nmin(), max(): Minimum and maximum values\nmedian(): Median value\nstd(), var(): Standard deviation and variance\nfirst(), last(): First and last values in the group"
  },
  {
    "objectID": "course-materials/cheatsheets/data_grouping.html#grouped-operations",
    "href": "course-materials/cheatsheets/data_grouping.html#grouped-operations",
    "title": "EDS 217 Cheatsheet",
    "section": "Grouped Operations",
    "text": "Grouped Operations\nYou can apply operations to each group separately using transform() or apply().\n\nUsing transform() to alter each group in a group by object\n\n\nCode\n# Transform: apply function to each group, return same-sized DataFrame\ndef normalize(x):\n    return (x - x.mean()) / x.std()\n\ndf['value_normalized'] = grouped['value'].transform(normalize)\n\n\n\n\nUsing apply() to alter each group in a group by object\n\n\nCode\n# Apply: apply function to each group, return a DataFrame or Series\ndef group_range(x):\n    return x['value'].max() - x['value'].min()\n\nresult = grouped.apply(group_range)\n\n\n/var/folders/bs/x9tn9jz91cv6hb3q6p4djbmw0000gn/T/ipykernel_55769/114114075.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  result = grouped.apply(group_range)"
  },
  {
    "objectID": "course-materials/cheatsheets/data_grouping.html#pivot-tables",
    "href": "course-materials/cheatsheets/data_grouping.html#pivot-tables",
    "title": "EDS 217 Cheatsheet",
    "section": "Pivot Tables",
    "text": "Pivot Tables\nPivot tables are a powerful tool for reorganizing and summarizing data. They allow you to transform your data from a long format to a wide format, making it easier to analyze and visualize patterns.\n\nWorking with Pivot Tables\n\n\nCode\n# Sample DataFrame\ndf = pd.DataFrame({\n    'date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02'],\n    'product': ['A', 'B', 'A', 'B'],\n    'sales': [100, 150, 120, 180]\n})\nprint(df)\n\n\n         date product  sales\n0  2023-01-01       A    100\n1  2023-01-01       B    150\n2  2023-01-02       A    120\n3  2023-01-02       B    180\n\n\n\nPivot tables with a single aggregation function\n\n\nCode\n# Create a pivot table\npivot_table = pd.pivot_table(df, values='sales', index='date', columns='product', aggfunc='sum')\nprint(pivot_table)\n\n\nproduct       A    B\ndate                \n2023-01-01  100  150\n2023-01-02  120  180\n\n\n\n\nPivot tables with multiple aggregation\n\n\nCode\n# Pivot table with multiple aggregation functions\npivot_multi = pd.pivot_table(df, values='sales', index='date', columns='product', \n                             aggfunc=[np.sum, np.mean])\nprint(pivot_multi)\n\n\n            sum        mean       \nproduct       A    B      A      B\ndate                              \n2023-01-01  100  150  100.0  150.0\n2023-01-02  120  180  120.0  180.0\n\n\n/var/folders/bs/x9tn9jz91cv6hb3q6p4djbmw0000gn/T/ipykernel_55769/1326309547.py:2: FutureWarning: The provided callable &lt;function sum at 0x1133b0b80&gt; is currently using DataFrameGroupBy.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n  pivot_multi = pd.pivot_table(df, values='sales', index='date', columns='product',\n/var/folders/bs/x9tn9jz91cv6hb3q6p4djbmw0000gn/T/ipykernel_55769/1326309547.py:2: FutureWarning: The provided callable &lt;function mean at 0x1133b1f80&gt; is currently using DataFrameGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n  pivot_multi = pd.pivot_table(df, values='sales', index='date', columns='product',"
  },
  {
    "objectID": "course-materials/cheatsheets/data_grouping.html#key-pivot-table-parameters",
    "href": "course-materials/cheatsheets/data_grouping.html#key-pivot-table-parameters",
    "title": "EDS 217 Cheatsheet",
    "section": "Key Pivot Table Parameters",
    "text": "Key Pivot Table Parameters\n\nvalues: Column(s) to aggregate\nindex: Column(s) to use as row labels\ncolumns: Column(s) to use as column labels\naggfunc: Function(s) to use for aggregation (default is mean)\nfill_value: Value to use for missing data\nmargins: Add row/column with subtotals (default is False)\n\nFor more detailed information on grouping, aggregating, and pivot tables in Pandas, refer to the official Pandas documentation."
  },
  {
    "objectID": "course-materials/interactive-sessions/4c_dataframe_workflows.html",
    "href": "course-materials/interactive-sessions/4c_dataframe_workflows.html",
    "title": "Interactive Session",
    "section": "",
    "text": "Every data science project follows the same systematic workflow. Whether you’re analyzing Netflix recommendations, climate research, social media trends, or working on your final project, you’ll use these 9 steps:\n\n\n\n\n\nflowchart LR\n    A[\"1. Import&lt;br/&gt;📂\"] --&gt; B[\"2. Explore&lt;br/&gt;🔍\"] --&gt; C[\"3. Clean&lt;br/&gt;🧹\"]\n    C --&gt; D[\"4. Filter&lt;br/&gt;🎯\"] --&gt; E[\"5. Sort&lt;br/&gt;📊\"]\n    E --&gt; F[\"6. Transform&lt;br/&gt;🔄\"] --&gt; G[\"7. Group&lt;br/&gt;👥\"]\n    G --&gt; H[\"8. Aggregate&lt;br/&gt;📈\"] --&gt; I[\"9. Visualize&lt;br/&gt;📊\"]\n\n    style A fill:#e1f5fe\n    style B fill:#e8f5e8\n    style C fill:#fff3e0\n    style D fill:#f3e5f5\n    style E fill:#e0f2f1\n    style F fill:#fce4ec\n    style G fill:#e8eaf6\n    style H fill:#f1f8e9\n    style I fill:#fff8e1\n\n\n\n\n\n\n\n\n\n\n\n\nWhy This Workflow Matters\n\n\n\nToday: See all 9 steps in action with ocean temperature analysis\nDays 4-7: Master each step individually with detailed sessions\nYour final project: Apply this exact workflow to answer your research question!\n\n\n\n\n\n\n\n\nCourse Integration\n\n\n\nAlmost all pandas functions and dataframe methods fit into one of these 9 categories. For reference, here is a cheatsheet that maps common pandas functions to our workflow steps."
  },
  {
    "objectID": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-data-science-workflow",
    "href": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-data-science-workflow",
    "title": "Interactive Session",
    "section": "",
    "text": "Every data science project follows the same systematic workflow. Whether you’re analyzing Netflix recommendations, climate research, social media trends, or working on your final project, you’ll use these 9 steps:\n\n\n\n\n\nflowchart LR\n    A[\"1. Import&lt;br/&gt;📂\"] --&gt; B[\"2. Explore&lt;br/&gt;🔍\"] --&gt; C[\"3. Clean&lt;br/&gt;🧹\"]\n    C --&gt; D[\"4. Filter&lt;br/&gt;🎯\"] --&gt; E[\"5. Sort&lt;br/&gt;📊\"]\n    E --&gt; F[\"6. Transform&lt;br/&gt;🔄\"] --&gt; G[\"7. Group&lt;br/&gt;👥\"]\n    G --&gt; H[\"8. Aggregate&lt;br/&gt;📈\"] --&gt; I[\"9. Visualize&lt;br/&gt;📊\"]\n\n    style A fill:#e1f5fe\n    style B fill:#e8f5e8\n    style C fill:#fff3e0\n    style D fill:#f3e5f5\n    style E fill:#e0f2f1\n    style F fill:#fce4ec\n    style G fill:#e8eaf6\n    style H fill:#f1f8e9\n    style I fill:#fff8e1\n\n\n\n\n\n\n\n\n\n\n\n\nWhy This Workflow Matters\n\n\n\nToday: See all 9 steps in action with ocean temperature analysis\nDays 4-7: Master each step individually with detailed sessions\nYour final project: Apply this exact workflow to answer your research question!\n\n\n\n\n\n\n\n\nCourse Integration\n\n\n\nAlmost all pandas functions and dataframe methods fit into one of these 9 categories. For reference, here is a cheatsheet that maps common pandas functions to our workflow steps."
  },
  {
    "objectID": "course-materials/interactive-sessions/4c_dataframe_workflows.html#ocean-temperature-analysis-complete-workflow-demo",
    "href": "course-materials/interactive-sessions/4c_dataframe_workflows.html#ocean-temperature-analysis-complete-workflow-demo",
    "title": "Interactive Session",
    "section": "Ocean Temperature Analysis: Complete Workflow Demo",
    "text": "Ocean Temperature Analysis: Complete Workflow Demo\nIn this session, we’ll systematically work through every step of the data science workflow using ocean temperature data. You’ll see exactly how professional data scientists approach problems, and by the end, you’ll have completed your first full data science project!\nResearch Question: Which ocean has the warmest average temperatures, and how do temperatures change between seasons?\nLet’s systematically work through our 9-step workflow!"
  },
  {
    "objectID": "course-materials/interactive-sessions/4c_dataframe_workflows.html#setting-up-our-environment",
    "href": "course-materials/interactive-sessions/4c_dataframe_workflows.html#setting-up-our-environment",
    "title": "Interactive Session",
    "section": "Setting up our environment",
    "text": "Setting up our environment\nFirst, let’s import the libraries we know from previous sessions:\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n\n\n\n\n\nLibraries We’re Using\n\n\n\n\npandas (pd): For working with DataFrames (from Sessions 4a & 4b)\nmatplotlib (plt): For creating charts and graphs (from Session 4c)"
  },
  {
    "objectID": "course-materials/interactive-sessions/4c_dataframe_workflows.html#workflow-progress-tracker",
    "href": "course-materials/interactive-sessions/4c_dataframe_workflows.html#workflow-progress-tracker",
    "title": "Interactive Session",
    "section": "Workflow Progress Tracker",
    "text": "Workflow Progress Tracker\nAs we work through each step, we’ll track our progress through the complete data science workflow:\n\n\n\n\n\n\nWorkflow Progress\n\n\n\nOcean Temperature Analysis - Workflow Steps\n☐ Step 1: Import - Load our ocean data\n☐ Step 2: Explore - Discover what we have\n☐ Step 3: Clean - Fix any problems\n☐ Step 4: Filter - Focus on specific data\n☐ Step 5: Sort - Find temperature patterns\n☐ Step 6: Transform - Create new insights\n☐ Step 7: Group - Organize by categories\n☐ Step 8: Aggregate - Calculate summaries\n☐ Step 9: Visualize - Present our results\nGoal: Complete systematic data science analysis"
  },
  {
    "objectID": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-1-import-data",
    "href": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-1-import-data",
    "title": "Interactive Session",
    "section": "📂 Step 1: Import Data",
    "text": "📂 Step 1: Import Data\n✅ Workflow Step 1: Getting our data into Python\nThe first step in every data science project is getting your data into Python. We’ll use pd.read_csv() - the same function you learned in Session 4a!\n\n\nCode\n# Step 1: Import our ocean temperature data\ndf = pd.read_csv('ocean_temperatures_simple.csv')\nprint(\"✅ Step 1 Complete: Data imported successfully!\")\nprint(f\"📊 Loaded {len(df)} rows of ocean temperature data\")\n\n\n✅ Step 1 Complete: Data imported successfully!\n📊 Loaded 30 rows of ocean temperature data\n\n\n\n\n\n\n\n\nReal Data Science Connection\n\n\n\nProfessional data scientists start every project the same way - importing data! Whether it’s: - Climate data from NASA - User behavior from websites\n- Financial data from banks - Your final project data\nYou always start with: pd.read_csv() or similar import functions\n\n\n🔮 Coming Attractions: Later in the course, you’ll learn to import Excel files, JSON data, and even data from databases!"
  },
  {
    "objectID": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-2-explore-data",
    "href": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-2-explore-data",
    "title": "Interactive Session",
    "section": "🔍 Step 2: Explore Data",
    "text": "🔍 Step 2: Explore Data\n✅ Workflow Step 2: Discovering what we have\nBefore we can analyze data, we need to understand what we’re working with. Let’s use the exploration methods you learned in Session 4a:\n\n\nCode\nprint(\"🔍 EXPLORING OUR OCEAN DATA\")\nprint(\"=\" * 40)\n\nprint(\"\\n📋 First few rows:\")\nprint(df.head())\n\nprint(f\"\\n📊 DataFrame info:\")\ndf.info()\n\nprint(f\"\\n📈 Summary statistics:\")\nprint(df.describe())\n\nprint(\"\\n❓ Missing values check:\")\nprint(df.isna().sum())\n\nprint(\"\\n✅ Step 2 Complete: We now understand our data!\")\n\n\n🔍 EXPLORING OUR OCEAN DATA\n========================================\n\n📋 First few rows:\n         date  location  temperature  salinity  depth\n0  2021-01-15   Pacific         18.5      34.2     50\n1  2021-01-15  Atlantic         22.1      35.1      0\n2  2021-01-15    Indian         20.0      34.8    100\n3  2021-01-15  Southern         15.2      34.0    200\n4  2021-01-15    Arctic         12.1      33.5     50\n\n📊 DataFrame info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 30 entries, 0 to 29\nData columns (total 5 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   date         30 non-null     object \n 1   location     30 non-null     object \n 2   temperature  30 non-null     float64\n 3   salinity     30 non-null     float64\n 4   depth        30 non-null     int64  \ndtypes: float64(2), int64(1), object(2)\nmemory usage: 1.3+ KB\n\n📈 Summary statistics:\n       temperature   salinity       depth\ncount    30.000000  30.000000   30.000000\nmean     19.283333  34.433333   80.000000\nstd       4.621843   0.616068   68.982756\nmin      11.500000  33.300000    0.000000\n25%      15.400000  34.025000   50.000000\n50%      19.200000  34.350000   50.000000\n75%      22.925000  35.000000  100.000000\nmax      27.100000  35.400000  200.000000\n\n❓ Missing values check:\ndate           0\nlocation       0\ntemperature    0\nsalinity       0\ndepth          0\ndtype: int64\n\n✅ Step 2 Complete: We now understand our data!\n\n\n\n\n\n\n\n\nWhat We Discovered\n\n\n\nOur ocean dataset contains: - 5 oceans: Pacific, Atlantic, Indian, Southern, Arctic - Temperature measurements in degrees Celsius\n- Salinity measurements (salt content) - Depth measurements where samples were taken - 30 total measurements across different dates\nThis is exactly what real data scientists do first!\n\n\n🔮 Coming Attractions: In Day 5, you’ll learn advanced exploration techniques like correlation analysis and custom statistics!"
  },
  {
    "objectID": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-3-clean-data",
    "href": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-3-clean-data",
    "title": "Interactive Session",
    "section": "🧹 Step 3: Clean Data",
    "text": "🧹 Step 3: Clean Data\n✅ Workflow Step 3: Fixing problems in our data\nGood news! Our ocean data is already clean - no missing values to worry about. But let’s see what cleaning looks like:\n\n\nCode\nprint(\"🧹 CLEANING OUR DATA\")\nprint(\"=\" * 30)\n\n# Check for missing values (we already did this, but let's confirm)\nmissing_data = df.isna().sum()\nprint(\"Missing values per column:\")\nprint(missing_data)\n\nif missing_data.sum() == 0:\n    print(\"\\n🎉 Great news! Our data is already clean!\")\n    df_cleaned = df.copy()  # Make a copy for consistency\nelse:\n    print(f\"\\n🔧 Cleaning needed...\")\n    df_cleaned = df.dropna().copy()  # Remove rows with missing values\n    print(f\"Removed {len(df) - len(df_cleaned)} rows with missing data\")\n\nprint(f\"\\n✅ Step 3 Complete: Clean dataset with {len(df_cleaned)} rows ready for analysis!\")\n\n\n🧹 CLEANING OUR DATA\n==============================\nMissing values per column:\ndate           0\nlocation       0\ntemperature    0\nsalinity       0\ndepth          0\ndtype: int64\n\n🎉 Great news! Our data is already clean!\n\n✅ Step 3 Complete: Clean dataset with 30 rows ready for analysis!\n\n\n\n\n\n\n\n\nWhy Cleaning Matters\n\n\n\nIn real data science projects, you’ll spend 50-80% of your time cleaning data! Common problems include: - Missing values (what we just checked for) - Duplicate entries - Incorrect data types - Outliers and errors\nThe .dropna() method you just learned will be one of your most-used tools!\n\n\n🔮 Coming Attractions: In Day 5, you’ll learn advanced cleaning techniques like handling duplicates and fixing data types!"
  },
  {
    "objectID": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-4-filter-data",
    "href": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-4-filter-data",
    "title": "Interactive Session",
    "section": "🎯 Step 4: Filter Data",
    "text": "🎯 Step 4: Filter Data\n✅ Workflow Step 4: Focusing on what matters for our question\nLet’s focus on specific data to answer our research question. We’ll filter for just the Pacific Ocean to start:\n\n\nCode\nprint(\"🎯 FILTERING OUR DATA\")\nprint(\"=\" * 30)\n\n# Filter for just Pacific Ocean data (using boolean indexing from Session 4b)\npacific_data = df_cleaned[df_cleaned['location'] == 'Pacific']\n\nprint(\"Pacific Ocean measurements:\")\nprint(pacific_data)\nprint(f\"\\n📊 Found {len(pacific_data)} Pacific Ocean measurements\")\n\n# Let's also look at summer data (June measurements)\nsummer_data = df_cleaned[df_cleaned['date'].str.contains('06-15')]\nprint(f\"\\n🌞 Summer measurements (June): {len(summer_data)} rows\")\n\nprint(\"\\n✅ Step 4 Complete: Focused on specific data for our analysis!\")\n\n\n🎯 FILTERING OUR DATA\n==============================\nPacific Ocean measurements:\n          date location  temperature  salinity  depth\n0   2021-01-15  Pacific         18.5      34.2     50\n5   2021-06-15  Pacific         24.3      34.5     50\n10  2021-12-15  Pacific         19.1      34.3     50\n15  2022-01-15  Pacific         18.2      34.1     50\n20  2022-06-15  Pacific         24.8      34.6     50\n25  2022-12-15  Pacific         19.3      34.4     50\n\n📊 Found 6 Pacific Ocean measurements\n\n🌞 Summer measurements (June): 10 rows\n\n✅ Step 4 Complete: Focused on specific data for our analysis!\n\n\n\n\n\n\n\n\nFiltering in Real Data Science\n\n\n\nProfessional data scientists constantly filter data to focus on specific questions: - Netflix: “Show me viewing data for comedy movies” - Climate research: “Focus on temperature data from Arctic regions”\n- Your final project: “Filter for data relevant to your specific question”\nThe boolean indexing you just used (df[df['column'] == value]) is a fundamental skill!\n\n\n🔮 Coming Attractions: In Day 5, you’ll learn complex filtering with multiple conditions using & and | operators!"
  },
  {
    "objectID": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-5-sort-data",
    "href": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-5-sort-data",
    "title": "Interactive Session",
    "section": "📊 Step 5: Sort Data",
    "text": "📊 Step 5: Sort Data\n✅ Workflow Step 5: Organizing data to find patterns\nSorting helps us find the highest and lowest values. Let’s find the warmest and coldest ocean measurements:\n\n\nCode\nprint(\"📊 SORTING OUR DATA\")\nprint(\"=\" * 30)\n\n# Sort by temperature (warmest first) using .sort_values() from Session 4b\nsorted_by_temp = df_cleaned.sort_values('temperature', ascending=False)\n\nprint(\"🔥 TOP 5 WARMEST measurements:\")\nprint(sorted_by_temp[['location', 'temperature', 'date']].head())\n\nprint(\"\\n🧊 TOP 5 COLDEST measurements:\")\nprint(sorted_by_temp[['location', 'temperature', 'date']].tail())\n\nprint(\"\\n✅ Step 5 Complete: Found temperature patterns by sorting!\")\n\n\n📊 SORTING OUR DATA\n==============================\n🔥 TOP 5 WARMEST measurements:\n    location  temperature        date\n21  Atlantic         27.1  2022-06-15\n6   Atlantic         26.8  2021-06-15\n22    Indian         25.5  2022-06-15\n7     Indian         25.2  2021-06-15\n20   Pacific         24.8  2022-06-15\n\n🧊 TOP 5 COLDEST measurements:\n   location  temperature        date\n9    Arctic         14.8  2021-06-15\n4    Arctic         12.1  2021-01-15\n19   Arctic         11.9  2022-01-15\n29   Arctic         11.8  2022-12-15\n14   Arctic         11.5  2021-12-15\n\n✅ Step 5 Complete: Found temperature patterns by sorting!\n\n\n\n\n\n\n\n\nInsights from Sorting\n\n\n\nWhat we discovered: - 🔥 Warmest: Atlantic Ocean (27.1°C in summer) - 🧊 Coldest: Arctic Ocean (11.5°C in winter) - 📈 Pattern: Atlantic and Pacific are warmest, Arctic is coldest\nThis is how data scientists find patterns - sorting reveals extremes and trends!\n\n\n🔮 Coming Attractions: In Day 6, you’ll learn to sort by multiple columns and create hierarchical sorting!"
  },
  {
    "objectID": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-6-transform-data",
    "href": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-6-transform-data",
    "title": "Interactive Session",
    "section": "🔄 Step 6: Transform Data",
    "text": "🔄 Step 6: Transform Data\n✅ Workflow Step 6: Creating new insights from existing data\nLet’s create new information that will help answer our research question:\n\n\nCode\nprint(\"🔄 TRANSFORMING OUR DATA\")\nprint(\"=\" * 35)\n\n# Create a new column: temperature in Fahrenheit (simple math from Session 4b)\ndf_cleaned['temperature_f'] = (df_cleaned['temperature'] * 9/5) + 32\n\n# Create a season category based on the date\ndef get_season(date_str):\n    if '01-15' in date_str or '12-15' in date_str:\n        return 'Winter'\n    elif '06-15' in date_str:\n        return 'Summer'\n    else:\n        return 'Other'\n\ndf_cleaned['season'] = df_cleaned['date'].apply(get_season)\n\n# Show our new columns\nprint(\"New columns added:\")\nprint(df_cleaned[['location', 'temperature', 'temperature_f', 'season']].head())\n\nprint(f\"\\n📈 Original columns: 5\")\nprint(f\"📈 After transformation: {len(df_cleaned.columns)} columns\")\nprint(\"\\n✅ Step 6 Complete: Created new insights from our data!\")\n\n\n🔄 TRANSFORMING OUR DATA\n===================================\nNew columns added:\n   location  temperature  temperature_f  season\n0   Pacific         18.5          65.30  Winter\n1  Atlantic         22.1          71.78  Winter\n2    Indian         20.0          68.00  Winter\n3  Southern         15.2          59.36  Winter\n4    Arctic         12.1          53.78  Winter\n\n📈 Original columns: 5\n📈 After transformation: 7 columns\n\n✅ Step 6 Complete: Created new insights from our data!\n\n\n\n\n\n\n\n\nWhy Transform Data?\n\n\n\nTransformation creates new insights: - 🌡️ Temperature in Fahrenheit: Makes data accessible to different audiences - 🗓️ Season categories: Helps us compare winter vs summer patterns - 📊 New calculations: Ratios, categories, derived metrics\nReal data scientists spend lots of time creating these “feature engineering” transformations!\n\n\n🔮 Coming Attractions: In Day 6, you’ll learn advanced transformations and custom functions!"
  },
  {
    "objectID": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-7-group-data",
    "href": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-7-group-data",
    "title": "Interactive Session",
    "section": "👥 Step 7: Group Data",
    "text": "👥 Step 7: Group Data\n✅ Workflow Step 7: Organizing by categories to find patterns\nNow we’ll group our data by categories to compare different oceans and seasons:\n\n\nCode\nprint(\"👥 GROUPING OUR DATA\")\nprint(\"=\" * 30)\n\n# Group by ocean location (using .groupby() from Session 4b)\nby_ocean = df_cleaned.groupby('location')\n\nprint(\"📊 Number of measurements per ocean:\")\nprint(by_ocean.size())\n\n# Group by season to compare winter vs summer\nby_season = df_cleaned.groupby('season')\n\nprint(\"\\n📊 Number of measurements per season:\")\nprint(by_season.size())\n\nprint(\"\\n✅ Step 7 Complete: Data organized by meaningful categories!\")\n\n\n👥 GROUPING OUR DATA\n==============================\n📊 Number of measurements per ocean:\nlocation\nArctic      6\nAtlantic    6\nIndian      6\nPacific     6\nSouthern    6\ndtype: int64\n\n📊 Number of measurements per season:\nseason\nSummer    10\nWinter    20\ndtype: int64\n\n✅ Step 7 Complete: Data organized by meaningful categories!\n\n\n\n\n\n\n\n\nWhy Group Data?\n\n\n\nGrouping reveals patterns: - 🌊 By ocean: Compare Pacific vs Atlantic vs Arctic temperatures - 🗓️ By season: See how temperatures change winter to summer\n- 📊 By categories: Any categorical variable can create groups\nThis sets up the next step - calculating summary statistics for each group!\n\n\n🔮 Coming Attractions: In Day 6, you’ll learn to group by multiple columns simultaneously and create complex hierarchical groups!"
  },
  {
    "objectID": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-8-aggregate-data",
    "href": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-8-aggregate-data",
    "title": "Interactive Session",
    "section": "📈 Step 8: Aggregate Data",
    "text": "📈 Step 8: Aggregate Data\n✅ Workflow Step 8: Calculating summary statistics to answer our question\nNow for the exciting part - let’s calculate averages to answer “Which ocean is warmest?”\n\n\nCode\nprint(\"📈 AGGREGATING OUR DATA\")\nprint(\"=\" * 35)\n\n# Calculate average temperature by ocean (using .mean() from Session 4b)\navg_temp_by_ocean = df_cleaned.groupby('location')['temperature'].mean()\n\nprint(\"🌊 AVERAGE TEMPERATURE BY OCEAN:\")\nprint(avg_temp_by_ocean.sort_values(ascending=False))\n\n# Calculate average temperature by season\navg_temp_by_season = df_cleaned.groupby('season')['temperature'].mean()\n\nprint(\"\\n🗓️ AVERAGE TEMPERATURE BY SEASON:\")\nprint(avg_temp_by_season.sort_values(ascending=False))\n\n# Answer our research question!\nwarmest_ocean = avg_temp_by_ocean.max()\nwarmest_ocean_name = avg_temp_by_ocean.idxmax()\n\nprint(f\"\\n🎉 RESEARCH QUESTION ANSWERED!\")\nprint(f\"🏆 Warmest ocean: {warmest_ocean_name} ({warmest_ocean:.1f}°C)\")\n\nprint(\"\\n✅ Step 8 Complete: Found the answer through aggregation!\")\n\n\n📈 AGGREGATING OUR DATA\n===================================\n🌊 AVERAGE TEMPERATURE BY OCEAN:\nlocation\nAtlantic    24.083333\nIndian      22.133333\nPacific     20.700000\nSouthern    16.616667\nArctic      12.883333\nName: temperature, dtype: float64\n\n🗓️ AVERAGE TEMPERATURE BY SEASON:\nseason\nSummer    22.100\nWinter    17.875\nName: temperature, dtype: float64\n\n🎉 RESEARCH QUESTION ANSWERED!\n🏆 Warmest ocean: Atlantic (24.1°C)\n\n✅ Step 8 Complete: Found the answer through aggregation!\n\n\n\n\n\n\n\n\nKey Discovery!\n\n\n\nOur Research Results: - 🥇 Warmest Ocean: Atlantic (24.1°C average) - 🥈 Second Warmest: Pacific (20.7°C average)\n- 🥉 Coldest: Arctic (12.9°C average) - 🌞 Summer is warmer than winter (as expected!)\nThis is exactly how real data science works - use aggregation to answer research questions!\n\n\n🔮 Coming Attractions: In Day 6, you’ll learn advanced aggregation functions like .agg() to calculate multiple statistics at once!"
  },
  {
    "objectID": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-9-visualize-data",
    "href": "course-materials/interactive-sessions/4c_dataframe_workflows.html#step-9-visualize-data",
    "title": "Interactive Session",
    "section": "📊 Step 9: Visualize Data",
    "text": "📊 Step 9: Visualize Data\n✅ Workflow Step 9: Telling our story with charts\nThe final step is creating a chart to communicate our findings clearly:\n\n\nCode\nprint(\"📊 VISUALIZING OUR RESULTS\")\nprint(\"=\" * 35)\n\n# Create average temperature data for plotting\navg_temps = df_cleaned.groupby('location')['temperature'].mean().sort_values(ascending=False)\n\n# Create a bar chart (using matplotlib from Session 4c)\nplt.figure(figsize=(10, 6))\navg_temps.plot(kind='bar', color=['red', 'orange', 'blue', 'green', 'purple'])\nplt.title('🌊 Average Ocean Temperatures: Research Results', fontsize=16, fontweight='bold')\nplt.xlabel('Ocean Location', fontsize=12)\nplt.ylabel('Average Temperature (°C)', fontsize=12)\nplt.xticks(rotation=45)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\n\n# Add our research conclusion to the plot\nplt.figtext(0.5, 0.02, '🏆 Research Conclusion: Atlantic Ocean is the warmest on average!', \n            ha='center', fontsize=12, fontweight='bold')\n\nplt.show()\n\nprint(\"\\n✅ Step 9 Complete: Story told through visualization!\")\n\n\n📊 VISUALIZING OUR RESULTS\n===================================\n\n\n/var/folders/bs/x9tn9jz91cv6hb3q6p4djbmw0000gn/T/ipykernel_55814/1715597996.py:15: UserWarning: Glyph 127754 (\\N{WATER WAVE}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/Users/kellycaylor/mambaforge/envs/eds217_2025/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 127942 (\\N{TROPHY}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/Users/kellycaylor/mambaforge/envs/eds217_2025/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 127754 (\\N{WATER WAVE}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n\n\n\n\n✅ Step 9 Complete: Story told through visualization!\n\n\n\n\n\n\n\n\n🎉 CONGRATULATIONS! 🎉\n\n\n\nYou just completed your first full data science project!\n🌊 Research Question: Which ocean has the warmest average temperatures?\n📊 Answer: Atlantic Ocean (24.1°C average)\n🏆 Method: Complete 9-step data science workflow!\nYou’ve completed the full workflow - you’re officially a data scientist! 🎓"
  },
  {
    "objectID": "course-materials/interactive-sessions/4c_dataframe_workflows.html#what-you-accomplished-today",
    "href": "course-materials/interactive-sessions/4c_dataframe_workflows.html#what-you-accomplished-today",
    "title": "Interactive Session",
    "section": "🎯 What You Accomplished Today",
    "text": "🎯 What You Accomplished Today\n\n✅ Complete Workflow Mastery\nYou just used the exact same process that professional data scientists use every day:\n\n✅ Imported real ocean temperature data\n✅ Explored to understand what you had\n\n✅ Cleaned (lucky us - data was already clean!)\n✅ Filtered to focus on specific questions\n✅ Sorted to find temperature patterns\n✅ Transformed data to create new insights\n\n✅ Grouped by meaningful categories\n✅ Aggregated to calculate summary statistics\n✅ Visualized results with a professional chart\n\n\n\n🔮 Your Data Science Journey Continues\nNext Week - Individual Step Mastery: - Day 5: Advanced filtering and transformation techniques - Day 6: Complex grouping and aggregation methods\n- Day 7: Professional data visualization with seaborn\nYour Final Project: Use this exact 9-step workflow to answer your own research question!\n\n\n🔄 The Workflow You Can Always Apply\nWhenever you encounter a new dataset or research question, systematically work through these 9 steps: 1. Import → 2. Explore → 3. Clean → 4. Filter → 5. Sort → 6. Transform → 7. Group → 8. Aggregate → 9. Visualize\nThis is your systematic approach to data science success! 🎯\n\n🎉 End interactive session 4C - You’re now a data scientist! 🎉"
  },
  {
    "objectID": "course-materials/cheatsheets/read_csv.html",
    "href": "course-materials/cheatsheets/read_csv.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Code\nimport pandas as pd\nfrom io import StringIO\n\n# Create a simple CSV string\ncsv_data = \"\"\"\nname,age,city\nAlice,28,New York\nBob,35,San Francisco\nCharlie,42,Chicago\n\"\"\"\n\n# Read the CSV data\ndf = pd.read_csv(StringIO(csv_data))\nprint(df)\n\n\n      name  age           city\n0    Alice   28       New York\n1      Bob   35  San Francisco\n2  Charlie   42        Chicago"
  },
  {
    "objectID": "course-materials/cheatsheets/read_csv.html#basic-usage-of-pd.read_csv",
    "href": "course-materials/cheatsheets/read_csv.html#basic-usage-of-pd.read_csv",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Code\nimport pandas as pd\nfrom io import StringIO\n\n# Create a simple CSV string\ncsv_data = \"\"\"\nname,age,city\nAlice,28,New York\nBob,35,San Francisco\nCharlie,42,Chicago\n\"\"\"\n\n# Read the CSV data\ndf = pd.read_csv(StringIO(csv_data))\nprint(df)\n\n\n      name  age           city\n0    Alice   28       New York\n1      Bob   35  San Francisco\n2  Charlie   42        Chicago"
  },
  {
    "objectID": "course-materials/cheatsheets/read_csv.html#selecting-specific-columns",
    "href": "course-materials/cheatsheets/read_csv.html#selecting-specific-columns",
    "title": "EDS 217 Cheatsheet",
    "section": "Selecting Specific Columns",
    "text": "Selecting Specific Columns\n\nUsing the usecols Parameter\n\n\nCode\n# Read only specific columns\ndf = pd.read_csv(StringIO(csv_data), usecols=['name', 'city'])\nprint(df)\n\n\n      name           city\n0    Alice       New York\n1      Bob  San Francisco\n2  Charlie        Chicago"
  },
  {
    "objectID": "course-materials/cheatsheets/read_csv.html#naming-columns",
    "href": "course-materials/cheatsheets/read_csv.html#naming-columns",
    "title": "EDS 217 Cheatsheet",
    "section": "Naming Columns",
    "text": "Naming Columns\n\nUsing the names Parameter\n\n\nCode\n# Rename columns while reading\ndf = pd.read_csv(StringIO(csv_data), names=['full_name', 'years', 'location'])\nprint(df)\n\n\n  full_name years       location\n0      name   age           city\n1     Alice    28       New York\n2       Bob    35  San Francisco\n3   Charlie    42        Chicago"
  },
  {
    "objectID": "course-materials/cheatsheets/read_csv.html#specifying-an-index",
    "href": "course-materials/cheatsheets/read_csv.html#specifying-an-index",
    "title": "EDS 217 Cheatsheet",
    "section": "Specifying an Index",
    "text": "Specifying an Index\n\nUsing the index_col Parameter\n\n\nCode\n# Set 'name' column as index\ndf = pd.read_csv(StringIO(csv_data), index_col='name')\nprint(df)\n\n\n         age           city\nname                       \nAlice     28       New York\nBob       35  San Francisco\nCharlie   42        Chicago"
  },
  {
    "objectID": "course-materials/cheatsheets/read_csv.html#parsing-dates",
    "href": "course-materials/cheatsheets/read_csv.html#parsing-dates",
    "title": "EDS 217 Cheatsheet",
    "section": "Parsing Dates",
    "text": "Parsing Dates\n\nAutomatic Date Parsing\n\n\nCode\ncsv_data_with_dates = \"\"\"\ndate,event\n2023-01-15,Conference\n2023-02-28,Workshop\n2023-03-10,Seminar\n\"\"\"\n\ndf = pd.read_csv(StringIO(csv_data_with_dates), parse_dates=['date'])\nprint(df.dtypes)\nprint(df)\n\n\ndate     datetime64[ns]\nevent            object\ndtype: object\n        date       event\n0 2023-01-15  Conference\n1 2023-02-28    Workshop\n2 2023-03-10     Seminar\n\n\n\n\nCustom Date Parsing\n\n\nCode\ncsv_data_custom_dates = \"\"\"\ndate,event\n15/01/2023,Conference\n28/02/2023,Workshop\n10/03/2023,Seminar\n\"\"\"\n\ndf = pd.read_csv(StringIO(csv_data_custom_dates), parse_dates=['date'], date_format='%d/%m/%Y')\nprint(df.dtypes)\nprint(df)\n\n\ndate     datetime64[ns]\nevent            object\ndtype: object\n        date       event\n0 2023-01-15  Conference\n1 2023-02-28    Workshop\n2 2023-03-10     Seminar"
  },
  {
    "objectID": "course-materials/cheatsheets/read_csv.html#handling-headers",
    "href": "course-materials/cheatsheets/read_csv.html#handling-headers",
    "title": "EDS 217 Cheatsheet",
    "section": "Handling Headers",
    "text": "Handling Headers\n\nCSV with Multi-line Header\n\n\nCode\ncsv_data_with_header = \"\"\"\n# This file contains employee data\n# Created by: HR Department, Last updated: 2023-08-23\nEmployee ID,Name,Department,Salary\n101,John Doe,Marketing,50000\n102,Jane Smith,Engineering,60000\n103,Mike Johnson,Sales,55000\n\"\"\"\n\n# Read CSV ignoring the first two lines\ndf = pd.read_csv(StringIO(csv_data_with_header), header=2)\nprint(df)\n\n# Read CSV treating the first row as header and skipping the next two\ndf_alt = pd.read_csv(StringIO(csv_data_with_header), header=0, skiprows=2)\nprint(\"\\nAlternative method:\")\nprint(df_alt)\n\n\n   Employee ID          Name   Department  Salary\n0          101      John Doe    Marketing   50000\n1          102    Jane Smith  Engineering   60000\n2          103  Mike Johnson        Sales   55000\n\nAlternative method:\n                         # Created by: HR Department  Last updated: 2023-08-23\nEmployee ID Name                          Department                    Salary\n101         John Doe                       Marketing                     50000\n102         Jane Smith                   Engineering                     60000\n103         Mike Johnson                       Sales                     55000\n\n\n\n\nCSV with No Header\n\n\nCode\ncsv_data_no_header = \"\"\"\nAlice,28,New York\nBob,35,San Francisco\nCharlie,42,Chicago\n\"\"\"\n\ndf = pd.read_csv(StringIO(csv_data_no_header), header=None, names=['name', 'age', 'city'])\nprint(df)\n\n\n      name  age           city\n0    Alice   28       New York\n1      Bob   35  San Francisco\n2  Charlie   42        Chicago"
  },
  {
    "objectID": "course-materials/cheatsheets/read_csv.html#dealing-with-missing-data",
    "href": "course-materials/cheatsheets/read_csv.html#dealing-with-missing-data",
    "title": "EDS 217 Cheatsheet",
    "section": "Dealing with Missing Data",
    "text": "Dealing with Missing Data\n\nCustomizing NA Values\n\n\nCode\ncsv_data_missing = \"\"\"\nname,age,city\nAlice,28,New York\nBob,N/A,San Francisco\nCharlie,42,Unknown\n\"\"\"\n\ndf = pd.read_csv(StringIO(csv_data_missing), na_values=['N/A', 'Unknown'])\nprint(df)\n\n\n      name   age           city\n0    Alice  28.0       New York\n1      Bob   NaN  San Francisco\n2  Charlie  42.0            NaN"
  },
  {
    "objectID": "course-materials/cheatsheets/read_csv.html#coercing-columns-to-specific-data-types",
    "href": "course-materials/cheatsheets/read_csv.html#coercing-columns-to-specific-data-types",
    "title": "EDS 217 Cheatsheet",
    "section": "Coercing Columns to Specific Data Types",
    "text": "Coercing Columns to Specific Data Types\n\nUsing the dtype Parameter\n\n\nCode\ncsv_data_types = \"\"\"\nid,name,score\n1,Alice,85.5\n2,Bob,92.0\n3,Charlie,78.5\n\"\"\"\n\ndf = pd.read_csv(StringIO(csv_data_types), dtype={'id': int, 'name': str, 'score': float})\nprint(df.dtypes)\nprint(df)\n\n\nid         int64\nname      object\nscore    float64\ndtype: object\n   id     name  score\n0   1    Alice   85.5\n1   2      Bob   92.0\n2   3  Charlie   78.5"
  },
  {
    "objectID": "course-materials/cheatsheets/read_csv.html#reading-large-csv-files",
    "href": "course-materials/cheatsheets/read_csv.html#reading-large-csv-files",
    "title": "EDS 217 Cheatsheet",
    "section": "Reading Large CSV Files",
    "text": "Reading Large CSV Files\n\nUsing chunksize for Memory Efficiency\n\n\nCode\nimport numpy as np\n\n# Generate a large CSV string (100,000 rows)\nnp.random.seed(0)\nlarge_csv_data = \"id,value\\n\" + \"\\n\".join([f\"{i},{np.random.rand()}\" for i in range(100000)])\n\n# Read the large CSV in chunks\nchunk_size = 20000\nchunks = pd.read_csv(StringIO(large_csv_data), chunksize=chunk_size)\n\n# Process each chunk\nfor i, chunk in enumerate(chunks):\n    print(f\"Chunk {i+1}:\")\n    print(chunk.head())\n    print(f\"Chunk shape: {chunk.shape}\")\n    print(\"\\n\")\n\n\nChunk 1:\n   id     value\n0   0  0.548814\n1   1  0.715189\n2   2  0.602763\n3   3  0.544883\n4   4  0.423655\nChunk shape: (20000, 2)\n\n\nChunk 2:\n          id     value\n20000  20000  0.392173\n20001  20001  0.041157\n20002  20002  0.923301\n20003  20003  0.406235\n20004  20004  0.944282\nChunk shape: (20000, 2)\n\n\nChunk 3:\n          id     value\n40000  40000  0.369256\n40001  40001  0.211326\n40002  40002  0.476905\n40003  40003  0.082234\n40004  40004  0.237659\nChunk shape: (20000, 2)\n\n\nChunk 4:\n          id     value\n60000  60000  0.927955\n60001  60001  0.902937\n60002  60002  0.427617\n60003  60003  0.510806\n60004  60004  0.583200\nChunk shape: (20000, 2)\n\n\nChunk 5:\n          id     value\n80000  80000  0.011097\n80001  80001  0.001770\n80002  80002  0.155055\n80003  80003  0.316761\n80004  80004  0.651845\nChunk shape: (20000, 2)\n\n\n\n\nRemember, these examples use StringIO to simulate reading from a file. When working with actual CSV files, you would replace StringIO(csv_data) with the file path or URL."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day2.html#introduction",
    "href": "course-materials/eod-practice/eod-day2.html#introduction",
    "title": "Day 2: Tasks & Activities",
    "section": "Introduction",
    "text": "Introduction\nWelcome to the end-of-day exercise for Day 2! Today, we’ll be putting into practice what you’ve learned about Python data structures, particularly lists and dictionaries. This exercise allows you to work with real data from your classmates while reinforcing key concepts."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day2.html#learning-objectives",
    "href": "course-materials/eod-practice/eod-day2.html#learning-objectives",
    "title": "Day 2: Tasks & Activities",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this exercise, you should be able to:\n\nCreate and manipulate lists and dictionaries in Python\nUse list and dictionary methods effectively\nIterate through data structures using loops\nApply basic data analysis techniques using Python data structures"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day2.html#setup",
    "href": "course-materials/eod-practice/eod-day2.html#setup",
    "title": "Day 2: Tasks & Activities",
    "section": "Setup",
    "text": "Setup\nFirst, let’s import the necessary libraries:\n\n\nCode\n# We won't use the random library until the end of this exercise, \n# but it's always good to put imported libraries at the top of your notebook.\nimport random"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day2.html#part-1-data-collection",
    "href": "course-materials/eod-practice/eod-day2.html#part-1-data-collection",
    "title": "Day 2: Tasks & Activities",
    "section": "Part 1: Data Collection",
    "text": "Part 1: Data Collection\nIn this first part, we’ll create data structures based on information from your classmates.\n\nTask 1: Create a List of Classmates\nCreate a list containing the names of at least 4 of your classmates in this course.\n\n\nCode\n# Your code here\n\n\n\n\nTask 2: Create a Dictionary of Classmate Information\nNow, let’s create a dictionary where the keys are your classmates’ names, and the values are another dictionary containing information about them. For each classmate, include the following information:\n\nFavorite color (favorite_color)\nNumber of pets (number_of_pets)\nPreferred study snack (preferred_study_snack)\n\n\n\nCode\n# Your code here"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day2.html#part-2-data-structure-manipulation",
    "href": "course-materials/eod-practice/eod-day2.html#part-2-data-structure-manipulation",
    "title": "Day 2: Tasks & Activities",
    "section": "Part 2: Data Structure Manipulation",
    "text": "Part 2: Data Structure Manipulation\nNow that we have our data structures, let’s practice manipulating them.\n\nTask 3: List Operations\nUsing the list of classmate names you created in Task 1, perform the following operations:\n\nAdd a new classmate to the end of the list\nRemove the second classmate from the list\nSort the list alphabetically\nFind and print the index of a specific classmate\n\n\n\nCode\n# Your code here\n\n\n\n\nTask 4: Dictionary Operations\nUsing the dictionary of classmate information from Task 2, perform the following operations:\n\nAdd a new key-value pair for each classmate: “favorite_study_spot”\nUpdate the “number of pets” for one classmate\nCreate a list of all the favorite colors your classmates mentioned\n\n\n\nCode\n# Your code here"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day2.html#part-3-fun-with-random-selections",
    "href": "course-materials/eod-practice/eod-day2.html#part-3-fun-with-random-selections",
    "title": "Day 2: Tasks & Activities",
    "section": "Part 3: Fun with Random Selections",
    "text": "Part 3: Fun with Random Selections\nLet’s add a fun element to our exercise using the random module. Before we dive into the main task, let’s look at how we can use the random library to select random items from a dictionary.\n\nExample: Random Selection from a Dictionary\nHere’s a simple example of how to select random items from a dictionary:\n\n\nCode\nimport random\n\n# Sample dictionary\nfruit_colors = {\n    \"apple\": \"red\",\n    \"banana\": \"yellow\",\n    \"grape\": \"purple\",\n    \"kiwi\": \"brown\",\n    \"orange\": \"orange\"\n}\n\n# Select a single random key-value pair\nrandom_fruit, random_color = random.choice(list(fruit_colors.items()))\nprint(f\"Randomly selected fruit: {random_fruit}\")\nprint(f\"Its color: {random_color}\")\n\n# To get just a random key:\nrandom_fruit = random.choice(list(fruit_colors.keys()))\nprint(f\"Another randomly selected fruit: {random_fruit}\")\n\n# To select multiple random items:\nnum_selections = 3\nrandom_fruits = random.sample(list(fruit_colors.keys()), num_selections)\nprint(f\"Randomly selected {num_selections} fruits: {random_fruits}\")\n\n\nRandomly selected fruit: kiwi\nIts color: brown\nAnother randomly selected fruit: banana\nRandomly selected 3 fruits: ['banana', 'apple', 'kiwi']\n\n\nThis example demonstrates how to:\n\nConvert a dictionary to a list of key-value pairs or keys\nUse random.choice() to select a single random item from a list\nUse random.sample() to select multiple unique random items from a list\n\nNote: random.choice() selects a single item, while random.sample() can select multiple unique items. For our snack-sharing task below, random.sample() might be more useful!\n\n\nTask 5: Random Snack Sharing\nCreate a function that randomly selects a classmate to share their snack with another random classmate. Print out the results as “Name will share [snack] with Name”.\n#| echo: true\ndef assign_random_snacks(classmate_info):\n    # Your code here\n    print(f\"{sharer} will share {snack} with {receiver}\")\n\n# Test your function\nassign_random_snacks(classmate_info)"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day2.html#conclusion",
    "href": "course-materials/eod-practice/eod-day2.html#conclusion",
    "title": "Day 2: Tasks & Activities",
    "section": "Conclusion",
    "text": "Conclusion\nGreat job completing this exercise! You’ve practiced creating and manipulating lists and dictionaries, performed basic data analysis, and even created a fun random snack-sharing function. These skills will be invaluable as you continue your journey in Python programming and data science.\nRemember, the key to mastering these concepts is practice. Feel free to modify this exercise with your own data or ideas to further reinforce your learning."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day2.html#additional-resources",
    "href": "course-materials/eod-practice/eod-day2.html#additional-resources",
    "title": "Day 2: Tasks & Activities",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPython Lists (Python.org)\nPython Dictionaries (Python.org)\nRandom Module (Python.org)\n\nDon’t forget to check out our course cheatsheets for quick reference on lists and dictionaries!"
  },
  {
    "objectID": "course-materials/interactive-sessions/7b_visualizations_2.html#getting-started",
    "href": "course-materials/interactive-sessions/7b_visualizations_2.html#getting-started",
    "title": "Interactive Session 7B",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "course-materials/interactive-sessions/7b_visualizations_2.html#using-the-seaborn-library",
    "href": "course-materials/interactive-sessions/7b_visualizations_2.html#using-the-seaborn-library",
    "title": "Interactive Session 7B",
    "section": "Using the Seaborn Library",
    "text": "Using the Seaborn Library\nThis session provides a deeper introduction to the Seaborn visualization library.\nSeaborn helps you explore and understand your data. Its plotting functions operate on dataframes and arrays containing whole datasets and internally perform the necessary semantic mapping and statistical aggregation to produce informative plots. Its dataset-oriented, declarative API lets you focus on what the different elements of your plots mean, rather than on the details of how to draw them.\nHere’s an example of seaborne’s capabilities.\n\n\nCode\n%matplotlib inline\n# Import seaborn\nimport seaborn as sns\n\n# Apply the default theme\nsns.set_theme()\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\n\n# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"sex\", style=\"smoker\", size=\"size\",\n)\n\ntips.head()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBehind the scenes, seaborn uses matplotlib to draw its plots. The plot above shows the relationship between five variables in the built-in tips dataset using a single call to the seaborn function relplot().\nNotice that you only need to provide the names of the variables and their roles in the plot.\nThis interface is different from matplotlib, in that you do not need to specify attributes of the plot elements in terms of the color values or marker codes.\nBehind the scenes, seaborn handled the translation from values in the dataframe to arguments that matplotlib understands. This declarative approach lets you stay focused on the questions that you want to answer, rather than on the details of how to control matplotlib.\n\nSeaborn relplot()\nThe function relplot() is named that way because it is designed to visualize many different statistical relationships. While scatter plots are often effective, relationships where one variable represents a measure of time are better represented by a line. The relplot() function has a convenient kind parameter that lets you easily switch to this alternate representation:\n\n\nCode\ndots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n\n\n\n\n\n\n\n\n\nIf you compare the two calls to relplot() in the two examples so far, you will see that the size and style parameters are used in both the scatter plots (first example) and line plots (second example). However, they affect the two visualizations differently.\nIn a scatter plot, the size and style arguments affect marker area and symbol representation.\nIn a line plot, the size and style arguments alter the line width and dashing.\nAllowing the same arguments (syntax) to change meaning (semantics) across different contexts is more characteristic of natural languages than formal ones. In this case, seaborn is attempting to allow you to write in a “grammar of graphics”, which is the same concept underlying ggplot created by Hadley Wickham.\nThe benefit of adopting this less formal specification is that you do not need to worry about as many syntax details and instead can focus more on the overall structure of the plot and the information you want it to convey.\n\n\nComparing matplotlib to seaborn\nA focus of today’s activities is translation, so let’s look at translating some of the examples from yesterday’s matplotlib exercise into seaborn.\nFirst, as always, let’s import our important packages:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\nBasic line plots (sns.lineplot)\nLet’s use a couple of common, predictable functions for an example, \\(y_{\\sin} = \\sin{(x)}\\) and \\(y_{\\cos} = \\cos{(x)}\\):\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Generate a 1D array with 300 points between -5 and 5\nx = np.linspace(-5,5,300)\n# Generate sine wave\nysin = np.sin(x)\n# Generate cosine wave\nycos = np.cos(x)\n\n# Now let's make a dataframe from these arrays:\ndf = pd.DataFrame({\n    'x': x,\n    'ysin': ysin,\n    'ycos': ycos\n    })\n\n\nWe can plot these on the same figure without instancing plt.figure() as follows:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Plot sine wave\nplt.plot(x,ysin)\n# Plot cosine wave\nplt.plot(x,ycos)\n\n\n\n\n\n\n\n\n\nSeaborn uses the lineplot command to plot line plots:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nsns.lineplot(data=df,x='x',y='ysin')\nsns.lineplot(data=df,x='x',y='ycos')\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with real data\n\n\nWorking with real-world data usually complicates things, and plotting is no exception. In particular, working with time series can get a bit messy. Let’s take a look at some data on solar radiation as an example.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Import data\nbsrn = pd.read_csv(\n    'https://bit.ly/bsrn_data',\n    index_col=0,\n    parse_dates=True\n    )\n\nprint(bsrn.head())\n\n\n                     H_m  SWD_Wm2  STD_SWD  DIR_Wm2  STD_DIR  DIF_Wm2  \\\nDATE                                                                    \n2019-10-01 00:00:00    2     -3.0      0.0      0.0      0.0     -3.0   \n2019-10-01 00:01:00    2     -3.0      0.0      0.0      0.0     -3.0   \n2019-10-01 00:02:00    2     -3.0      0.0      0.0      0.0     -3.0   \n2019-10-01 00:03:00    2     -3.0      0.0      0.0      0.0     -3.0   \n2019-10-01 00:04:00    2     -3.0      0.0      0.0      0.0     -3.0   \n\n                     STD_DIF  LWD_Wm2  STD_LWD  SWU_Wm2  LWU_Wm2  T_degC  \\\nDATE                                                                       \n2019-10-01 00:00:00      0.0    300.0      0.1        0      383    16.2   \n2019-10-01 00:01:00      0.0    300.0      0.3        0      383    16.4   \n2019-10-01 00:02:00      0.0    300.0      0.2        0      383    16.5   \n2019-10-01 00:03:00      0.0    300.0      0.1        0      383    16.5   \n2019-10-01 00:04:00      0.0    300.0      0.1        0      383    16.8   \n\n                       RH  P_hPa  \nDATE                              \n2019-10-01 00:00:00  30.7    966  \n2019-10-01 00:01:00  30.7    966  \n2019-10-01 00:02:00  30.5    966  \n2019-10-01 00:03:00  30.4    966  \n2019-10-01 00:04:00  30.5    966  \n\n\nNow that we’ve imported our data, let’s make a quick plot of incoming shortwave radiation over time.\n\n✏️ Apply it. Translate the cell below into seaborn using the sns.lineplot command.\n\n\n\nCode\n# # Initialize empty figure\n# fig = plt.figure()\n# # Plot incoming SW radiation\n# plt.plot(bsrn.index,bsrn.SWD_Wm2)\n# # Label y-axis\n# plt.ylabel(r'Incoming SW radiation (W m$^{-2}$)')\n\n\nYou should end up with something that looks like this:\n\n\nText(0, 0.5, 'Incoming SW radiation (W m$^{-2}$)')\n\n\n\n\n\n\n\n\n\nThe x-axis looks rather messy because the tick labels are timestamps, which are, by nature, very long. Luckily, there a few approaches you can use to wrangle your x-axis labels when working with long timeseries.\n\n\nTechnique 1: Use figsize to alter the aspect ratio and layout.\nThe default figure size and aspect ratio aren’t that great for long time series, which are usually a wider aspect ratio (think old TV shows vs. widescreen movies). So, often, you can make things work a lot better by just tinkering with the figure size using the plt.figure() command and a figsize argument:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Example 1: Without explicitly setting figsize\nsns.lineplot(data=bsrn, x='DATE', y='SWD_Wm2')\nplt.ylabel(r'Incoming SW radiation (W m$^{-2}$)')\nplt.title('Default Figure Size')\nplt.show()\n\n# Example 2: With explicitly setting figsize\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=bsrn, x='DATE', y='SWD_Wm2')\nplt.ylabel(r'Incoming SW radiation (W m$^{-2}$)')\nplt.title('Custom Figure Size (12x6 inches)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe wider aspect ratio creates more room for your crowded x-axes labels! However, you can still see that the labels are still almost running together on the far right of the figure.\n\n\nTechnique 2: Rotate X-axis Labels\nAnother simple approach is to rotate the x-axis labels:\n\n\nCode\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=bsrn, x='DATE', y='SWD_Wm2')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTechnique 3: Use Fewer X-axis Labels\nIf your dataset spans a long time period, you might want to show fewer labels:\n\n\nCode\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=bsrn, x='DATE', y='SWD_Wm2')\n\n# Show only 5 evenly spaced labels\nplt.gca().xaxis.set_major_locator(plt.MaxNLocator(5))\n\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTechnique 4: Use a Time-based Moving Average\nIf you have too many data points, you might want to resample your data. Here we create a new column from the dataframe index (which contains our dates). We then resample the dataframe to get hourly averages of the data and plot these instead of the raw data, which is collected every minute\n\n\nCode\n# Ensure DATE is a datetime column\nbsrn['DATE'] = pd.to_datetime(bsrn.index)\n\n# Resample to monthly mean\nbsrn_monthly = bsrn.resample('h', on='DATE').mean().reset_index()\n\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=bsrn_monthly, x='DATE', y='SWD_Wm2')\n\nplt.xticks(rotation=45, ha='right')\nplt.title('BSRN Hourly Average Time Series')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAdvanced Axis Labeling: Date Locators and Formatters\nWhile beyond the scope of this course, date locators and formatters in Matplotlib are powerful tools for customizing time-based axes in your plots. They allow you to:\n\nControl which dates are shown on the axis (locators)\nDetermine how those dates are displayed (formatters)\n\nThese tools are particularly useful when working with time series data spanning different time scales (e.g., hours, days, months, years).\nIf you’re interested in learning more about these advanced techniques, here are some helpful resources:\n\nMatplotlib Date Formatting\n\nOfficial examples of date formatting in Matplotlib\n\nDate tick labels\n\nConcise date formatters in Matplotlib\n\nCustomizing Date Ticks\n\nExamples of customizing date ticks\n\nPython Graph Gallery - Time Series\n\nVarious examples of time series plots, some using date locators and formatters\n\n\nRemember, while these tools can create more polished and precise time-based plots, the techniques we’ve covered in this course are sufficient for many basic time series visualizations.\n\n\n\n✏️ Try it. Add a cell to your notebook and add code for the following exercise."
  },
  {
    "objectID": "course-materials/interactive-sessions/7b_visualizations_2.html#practice",
    "href": "course-materials/interactive-sessions/7b_visualizations_2.html#practice",
    "title": "Interactive Session 7B",
    "section": "📚  Practice ",
    "text": "📚  Practice \nPlot temperature and relative humidity (ideally using subplots) over the month of October 2019 at the BSRN station. Be sure to format the timestamps and include axis labels, a title, and a legend, if necessary.\n\n\nCode\n# Add your code here!\n\n# Step 1: Filter the dataframe to October 2019. You can use .loc to filter, as the index of the dataframe is already a datetime. Just filter from the start to end date that you want.\n\n# Step 2: Create a figure with two subplots using the `plt.subplots` command.\n\n\nHere’s one way your plots may turn out:\n\n\n\n\n\n\n\n\n\n\nEnd interactive session 7B"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day1.html#objective",
    "href": "course-materials/eod-practice/eod-day1.html#objective",
    "title": "Day 1: Tasks & activities",
    "section": "Objective",
    "text": "Objective\nIn this exercise, you will work with climate data using the Python data science workflow. You’ll load the data into a pandas DataFrame, perform basic exploration and cleaning, and create visualizations. This hands-on practice will help you understand how Python can be used for data analysis, with comparisons to similar tasks in R. Think of this as a movie trailer for the skills you’ll build over the next week.\n\n🎬 “Coming Attractions” Approach\n\nYour job: Copy, paste, and run the code exactly as written\nOur job: Show you what’s happening (not how it works yet!)\nThe goal: Get excited about what you’ll learn and see the big picture\n\n\n\n\n\n\n\nDon’t Panic! 🚀\n\n\n\nYou’re not expected to understand every line of code today. By next Friday, you’ll know exactly how all of this works. For now, just enjoy the ride and see what’s possible!"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day1.html#background-and-data-source",
    "href": "course-materials/eod-practice/eod-day1.html#background-and-data-source",
    "title": "Day 1: Tasks & activities",
    "section": "Background and Data Source",
    "text": "Background and Data Source\nOur data comes from the Arctic Long Term Ecological Research station. The Arctic Long Term Ecological Research (ARC LTER) site is part of a network of sites established by the National Science Foundation to support long-term ecologicalLooking South of Toolik Field Station research in the United States. The research site is located in the foothills region of the Brooks Range, North Slope of Alaska (68° 38’N, 149° 36.4’W, elevation 720 m). The Arctic LTER project’s goal is to understand and predict the effects of environmental change on arctic landscapes, both natural and anthropogenic. Researchers at the site use long-term monitoring and surveys of natural variation of ecosystem characteristics, experimental manipulation of ecosystems (years to decades) and modeling at ecosystem and watershed scales to gain an understanding of the controls of ecosystem structure and function. The data and insights gained are provided to federal, Alaska state and North Slope Borough officials who regulate the lands on the North Slope and through this web site.\nWe will be using some basic weather data downloaded from Toolik Station:\n\nToolik Station Meteorological Data: toolik_weather.csv Shaver, G. 2019. A multi-year DAILY weather file for the Toolik Field Station at Toolik Lake, AK starting 1988 to present. ver 4. Environmental Data Initiative. https://doi.org/10.6073/pasta/ce0f300cdf87ec002909012abefd9c5c (Accessed 2021-08-08).\n\nI have already downloaded this data and placed in our course repository, where we can access it easily using its github raw url.\nLet’s dive into the exercise!\n\n🗓️ When You’ll Master These Skills\n\n\n\nWhat you’ll see today\nWhen you’ll learn it\nWhat we’ll cover\n\n\n\n\nimport pandas as pd\nDay 3-4\nData structures and DataFrames\n\n\npd.read_csv()\nDay 4\nLoading data from files\n\n\ndf.head(), df.info()\nDay 4\nData exploration methods\n\n\ndf.groupby()\nDay 6\nData aggregation and grouping\n\n\nplt.plot(), plt.bar()\nDay 7\nData visualization"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day1.html#instructions",
    "href": "course-materials/eod-practice/eod-day1.html#instructions",
    "title": "Day 1: Tasks & activities",
    "section": "Instructions",
    "text": "Instructions\n\nSetup and Data Loading\n\nOpen JupyterLab and Start a New Notebook\n\n\n\nImport Libraries\n\nImport the necessary libraries to work with data (pandas) and create plots (matplotlib.pyplot). Use the standard python conventions that import pandas as pd and import matplotlib.pyplot as plt\n\n\n🎬 Copy and paste this code:\nimport pandas as pd\nimport matplotlib.pyplot as plt\nWhat just happened? We imported two powerful libraries! pandas is like Excel but supercharged for data analysis, and matplotlib creates beautiful(ish) plots. 🎓 Coming up: You’ll learn about Python imports and libraries on Days 2-3.\n\n\nLoad the Data\nOur data is located at:\nhttps://raw.githubusercontent.com/environmental-data-science/eds217-day0-comp/main/data/raw_data/toolik_weather.csv\n\nCreate a variable called url that stores the URL provided above as a string.\nUse the pandas library’s read_csv() function from pandas to load the data from the URL into a new DataFrame called df. Any pandas function will always be called using the pd object and dot notation: pd.read_csv().\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe read_csv() function can do a ton of different things, but today all you need to know is that it can take a url to a csv file as it’s only input.\n\n\n🎬 Copy and paste this code:\nurl = 'https://raw.githubusercontent.com/environmental-data-science/eds217-day0-comp/main/data/raw_data/toolik_weather.csv'\ndf = pd.read_csv(url)\n\n\n\n\n\n\nR vs Python: Data Loading\n\n\n\nThis is just like df &lt;- read.csv(url) in R! Both pandas DataFrames and R data.frames are tabular data structures. The main syntax difference is Python’s dot notation: pd.read_csv() vs R’s read.csv(). Both can read directly from URLs, which is incredibly convenient for reproducible research!\n\n\nWhat just happened? We loaded over 15,000 rows of climate data from the internet in one line! The data is now stored in a “DataFrame” called df. 🎓 Coming up: Day 4 will teach you all about loading and working with data files.\n\n\n\nData Exploration\n\nPreview the Data\n\nUse the head() method to display the first few rows of the DataFrame df.\n\n\n🎬 Copy and paste this code:\ndf.head()\n\n\n\n\n\n\nNote\n\n\n\nBecause the head() function is a method of a DataFrame, you will call it using dot notation and the dataframe you just created: df.head()\n\n\nWhat just happened? We previewed the first 5 rows of our 15,000+ row dataset! You can see daily weather measurements from Alaska. 🎓 Coming up: Day 4 morning will teach you data exploration methods like this.\n\n\n\n\n\n\nR vs Python: Data Exploration\n\n\n\nThis is exactly like head(df) in R! The key difference is Python’s object-oriented approach: df.head() vs R’s functional approach head(df). Both show you the first few rows, but Python treats the DataFrame as an object that has methods (like .head()) built into it.\n\n\n\n\nCheck for Data Quality\n\nUse the isnull() method combined with sum() to count missing values in each column.\n\n\n🎬 Copy and paste this code:\ndf.isnull().sum()\nWhat just happened? We checked every column for missing data! Looks like our temperature data is complete (0 missing values), which is great. 🎓 Coming up: Day 5 will teach you all about data cleaning and handling missing values.\n\n\n\n\n\n\nR vs Python: Missing Data Check\n\n\n\nIn R, you’d use sum(is.na(df)) to count missing values. Python uses df.isnull().sum() - notice the chaining of methods! This reads left-to-right: “take the DataFrame, check for null values, then sum them up.” Both approaches give you the count of missing values per column.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou should see that the Daily_AirTemp_Mean_C doesn’t have any missing values. This means we can skip the usual step of dealing with missing data. We’ll learn these tools in Python and Pandas later in the course.\n\n\n\n\nGet Data Summary Statistics and Data Descriptions\n\nUse the describe() method to generate summary statistics for numerical columns.\nUse the info() method to get an overview of the DataFrame, including data types and non-null counts. Just like the head() function, these are methods associated with your df object, so you call them with dot notation.\n\n\n🎬 Copy and paste this code:\ndf.describe()\ndf.info()\nWhat just happened? We got instant statistics and information about our entire dataset! You can see temperature ranges, averages, and data types. 🎓 Coming up: Day 4 will teach you how to explore and understand your datasets.\n\n\n\n\n\n\nR vs Python: Data Summary\n\n\n\nThese are like summary(df) and str(df) in R. Python’s .describe() gives you the statistical summary (like summary()) while .info() shows the structure (like str()). Notice how Python uses dot notation - the DataFrame object has these methods built in, whereas R uses separate functions that take the data frame as input.\n\n\n\n\n\n\nData Analysis\n\nCalculate Monthly Average Temperature\n\nNow for some real data analysis - let’s find average temperatures by month!\n🎬 Copy and paste this code:\nmonthly = df.groupby('Month')\nmonthly_means = monthly['Daily_AirTemp_Mean_C'].mean()\nWhat just happened? We grouped 15,000+ daily temperature readings by month and calculated averages! This turned years of daily data into 12 monthly summaries. 🎓 Coming up: Day 6 will teach you all about grouping and aggregating data like this.\n\n\n\n\n\n\nR vs Python: Data Grouping\n\n\n\nThis is exactly like using df %&gt;% group_by(Month) %&gt;% summarize(mean_temp = mean(Daily_AirTemp_Mean_C)) in dplyr! Both approaches group data and calculate statistics. Python’s syntax is df.groupby('column')['target_column'].function(), while R uses the pipe operator %&gt;% to chain operations. Both are powerful for data aggregation!\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can do analysis on a specific column in a dataframe using [column_nanme] notation: my_df[\"column A\"].mean() would give the average value of “column A” (if there was a column with that name in the dataframe). In the coming days, we will spend a lot of time learning how to select and subset data in dataframes!\n\n\n\n\nPlot Monthly Average Temperature\n\nTime to turn numbers into pictures! Let’s plot the monthly temperature patterns.\n🎬 Copy and paste this code:\nplt.plot(monthly_means)\nNow let’s make it even better with labels:\nmonths = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\nplt.bar(months, monthly_means)\nWhat just happened? You used a basic plotting function to make a data visualization. The bar chart clearly shows Alaska’s extreme seasonal temperature differences. 🎓 Coming up: Day 7 will teach you how to create amazing visualizations and customize them.\n\n\n\n\n\n\nR vs Python: Data Visualization\n\n\n\nThis is like creating plots with ggplot(df, aes(x=Month, y=temp)) + geom_bar() in R! Python’s matplotlib uses a more direct approach: plt.plot() and plt.bar() create plots immediately. Both are powerful - ggplot2 uses a “grammar of graphics” approach while matplotlib is more imperative. You’ll learn both have their strengths!\n\n\n\n\nAnalyze Temperature Trends Over Years\n\nLet’s explore how temperatures have changed over the decades!\n🎬 Copy and paste this code:\nyear = df.groupby('Year')\nyearly_means = year['Daily_AirTemp_Mean_C'].mean()\nplt.plot(yearly_means)\nAnd as a bar chart:\nyear_list = df['Year'].unique()\nplt.bar(year_list, yearly_means)\nWhat just happened? You analyzed climate trends across multiple decades! You can see how Arctic temperatures have varied over time - real climate science! 🎓 Coming up: This combines Day 6 skills (grouping data) with Day 7 skills (visualization).\n\n\n\n\n\n\nR vs Python: Time Series Analysis\n\n\n\nThis is just like grouping by year in R and plotting the results! Whether you use df %&gt;% group_by(Year) %&gt;% summarize() in R or df.groupby('Year').mean() in Python, you’re doing the same analytical thinking. The syntax differs, but the data science concepts are identical.\n\n\n\n\nSaving Analyses and Figures\nData scientists always save their analyses for future use.\n🎬 Copy and paste this code:\nmonthly_means.to_csv(\"monthly_means.csv\", header=True)\nWhat just happened? You saved your analysis results to a file that you (or other scientists) can use later! This is how research becomes reproducible. 🎓 Coming up: Day 4 will teach you all about importing, exporting, and managing data files.\n\n\n\n\n\n\nR vs Python: Data Export\n\n\n\nThis is just like write.csv(monthly_means, \"monthly_means.csv\") in R! Python uses the object-oriented approach where the data (your Series monthly_means) has a method .to_csv() built into it. R uses a function that takes the data as input. Both create the exact same CSV file - just different syntax approaches to the same goal.\n\n\n\nExample to_csv() Output:\nIf you inspect the monthly_means.csv file using the file browser in JupyterLab, it will look something like this:\nMonth,Daily_AirTemp_Mean_C\n1,-20.561290322580643\n2,-23.94107142857143\n3,-17.806451612903224\n4,-15.25294117647059\n5,-0.8758190327613105\n6,8.76624"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day1.html#conclusion",
    "href": "course-materials/eod-practice/eod-day1.html#conclusion",
    "title": "Day 1: Tasks & activities",
    "section": "Conclusion",
    "text": "Conclusion\nWe will spend the rest of the course learning more about each of the steps we just went through. And of course, we have a lot more to learn about the essentials of the Python programming language over the next 8 days of class.\nTake some time now to reflect on what you’ve learned today, and to add some additional comments and notes in your code to follow up on in the coming days.\nBy the end of the course you will be writing your own Python data science workflows just like this one… hopefully many of the “code strangers” you’ve just met will have become good friends!\n\n🎉🎉 Congratulations! You made it to the end of a Python data science workflow…🎉🎉\n🎉🎉..and the end of the first day of EDS 217!! 🎉🎉\n\n\nEnd Activity Session (Day 1)"
  },
  {
    "objectID": "course-materials/interactive-sessions/7a_visualizations_1.html#getting-started",
    "href": "course-materials/interactive-sessions/7a_visualizations_1.html#getting-started",
    "title": "Interactive Session 7A",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S)."
  },
  {
    "objectID": "course-materials/interactive-sessions/7a_visualizations_1.html#introduction",
    "href": "course-materials/interactive-sessions/7a_visualizations_1.html#introduction",
    "title": "Interactive Session 7A",
    "section": "Introduction",
    "text": "Introduction\nThere are extensive options for plotting in Python – some favorites include statistical visualizations in Seaborn and interactive plots for web applications in Bokeh. The original and fundamental library for visualizations in Python, however, is matplotlib`.\nMatplotlib was the first plotting library developed for Python and remains the most widely used library for data visualization in the Python community. Designed to resemble graphics in MATLAB, matplotlib is reminiscent of MATLAB in both appearance and functionality. As a result, it is not the easiest library to work with, and deviates from the object-oriented syntax we are familiar with in Python.\nThis session will serve as an introduction to plotting in Python using matplotlib. The nature of matplotlib – and figure-making in general – is such that the easiest way to learn is by following examples. As such, this session is structured a bit differently than the others, so be sure to look carefully at the coded examples. Finally, the best way to learn advanced functions and find help with matplotlib is by exploring the examples in the gallery.\n\nInstructions\nWe will work through this notebook together. To run a cell, click on the cell and press “Shift” + “Enter” or click the “Run” button in the toolbar at the top.\n\n\n\n\n\n\nNote\n\n\n\n🐍     This symbol designates an important note about Python structure, syntax, or another quirk."
  },
  {
    "objectID": "course-materials/interactive-sessions/7a_visualizations_1.html#introduction-to-matplotlib",
    "href": "course-materials/interactive-sessions/7a_visualizations_1.html#introduction-to-matplotlib",
    "title": "Interactive Session 7A",
    "section": "Introduction to matplotlib",
    "text": "Introduction to matplotlib\n\n\n\n\nmatplotlib logo\n\n\n\nAs always, we will begin by importing the required libraries and packages. For plotting, itself, we will use a module of the matplotlib library called pyplot. The pyplot module consists of a collection of functions to display and edit figures. As you advance with Python and with data analysis, you may want to explore additional features of matplotlib, but pyplot will suit the vast majority of your plotting needs at this stage.\nThe standard import statement for matplotlib.pyplot is:\nimport matplotlib.pyplot as plt\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\nAnatomy of a matplotlib plot\nThe core components of a matplotlib plot are the Figure and the Axes. The Figure is the overall window upon which all components are drawn. It serves as the blank container for plots, as well as other things, such as a legend, color bar, etc. You can (and will) create multiple independent figures, and each figure can hold multiple Axes. To the figure you will add Axes, the area where the data are actually plotted and where associated ticks, labels, etc. live.\nWhen working with a single plot, we will mostly deal with the Figure object and its routines, but we will see the Axes become important as we increase the complexity of our plots.\n\n\n\n\nObject heirarchy in matplotlib\n\n\n\n\n\nBasic plotting\nWe will start with the most basic plotting routines: plt.plot() and plt.scatter(). The first, plt.plot(), is used to generate a connected line plot (with optional markers for individual data points). plt.scatter(), as the name suggests, is used to generate a scatter plot.\nEach time you want to create a new figure, it is wise to first initialize a new instance of the matplotlib.figure.Figure class on which to plot our data. While this is not required to display the plot, if you subsequently plot additional data without a new Figure instance, all data will be plotted on the same figure. For example, let’s generate a few functions, \\(y_{\\sin} = \\sin{(x)}\\) and \\(y_{\\cos} = \\cos{(x)}\\):\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Generate a 1D array with 300 points between -5 and 5\nx = np.linspace(-5,5,300)\n# Generate sine wave\nysin = np.sin(x)\n# Generate cosine wave\nycos = np.cos(x)\n\n\nWe can plot these on the same figure without instancing plt.figure() as follows:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Plot sine wave\nplt.plot(x,ysin)\n# Plot cosine wave\nplt.plot(x,ycos)\n\n\n\n\n\n\n\n\n\nTo create multiple graphs in separate figure windows, however, you need to create new Figure instances as follows:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Plot sine wave\nfig1 = plt.figure()\nplt.plot(x,ysin)\n\n# Plot cosine wave\nfig2 = plt.figure()\nplt.plot(x,ycos)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis also allows you to access the Figure object later by refering to the variable fig. Thus, even when you want to plot all data on a single plot, it is best to always start by initializing a new Figure.\nTo generate a scatter plot instead of a line, we can use plt.scatter():\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Generate new x and y with fewer points for legibility\n# np.linspace(lower, upper, n): \n#     Creates n points between lower and upper, including both bounds.\n\nxscat = np.linspace(-5,5,25)\nyscat = np.sin(xscat)\n\n# Plot sine function as scatter plot\nplt.scatter(xscat,yscat)\n\n\n\n\n\n\n\n\n\nYou can also create a scatter plot using plt.plot() with keyword arguments, which allow you to change things like the color, style, and size of the lines and markers. We will explore some of these keyword arguments in the next section."
  },
  {
    "objectID": "course-materials/interactive-sessions/7a_visualizations_1.html#plt.plot-keyword-arguments",
    "href": "course-materials/interactive-sessions/7a_visualizations_1.html#plt.plot-keyword-arguments",
    "title": "Interactive Session 7A",
    "section": "plt.plot() Keyword arguments",
    "text": "plt.plot() Keyword arguments\nIn addition to the required x and y parameters, there are a number of optional keyword arguments that can be passed to the matplotlib plotting functions. Here, we will consider a few of the most useful: color, marker, and linestyle.\n\nColors\nThe first thing you might wish to control is the color of your plot. Matplotlib accepts several different color definitions to the color keyword argument, which is a feature of most plotting functions.\nFirst, colors can be passed as strings according to their HTML/CSS names. For example:\n\nUsing HTML string names to assign colors\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Specifying color with a string:\ny = ysin\nplt.plot(x, y, 'green')\n\n\n\n\n\n\n\n\n\nIn total, there are 140 colors allowed in HTML; their names are shown below.\n\n\n\n\nHTML color names\n\n\n\nAs you can see in the image above, the basic colors can also be defined by a single-letter shortcut. These are shown in the table below.\n\n\n\nLetter code\nColor name\n\n\n\n\n'r'\nred\n\n\n'g'\ngreen\n\n\n'b'\nblue\n\n\n'c'\ncyan\n\n\n'm'\nmagenta\n\n\n'y'\nyellow\n\n\n'k'\nblack\n\n\n'w'\nwhite\n\n\n\n\n\nUsing RGB(A) tuples to assign colors\nAnother way of specifying colors is to use an RGB(A) tuple, where the brightness of each channel (R, G, or B, which correspond to red, green, and blue) is given as a float between 0 and 1.\n\n\n\n\n\n\nUsing alpha for transparency\n\n\n\nAn optional fourth value, A or alpha, value can be passed to specify the opacity of the line or marker.\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Specifying color with an RGB tuple:\nplt.plot(x, y, color=(0.2,0.7,1.0))\n\n\n\n\n\n\n\n\n\nA grayscale value can be used by passing a number between 0 and 1 as a string. In this representation, '0.0' corresponds to black and '1.0' corresponds to white.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Specifying greyscale with a intensity value [0-1]:\nplt.plot(x, y, color='0.25')\n\n\n\n\n\n\n\n\n\n\n\nUsing hex codes to define colors\nAnother way to define colors is to use color hex codes, which represent colors as hexadecimals ranging from 0 to FF. Color hex codes consist of a hash character # followed by six hex values (e.g. #AFD645). Hex codes must be passed as strings (e.g. '#AFD645') in matplotlib and are perhaps the most flexible way to select colors.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Specifying color with a hex code:\nplt.plot(x, y, color='#C6E2FF')\n\n\n\n\n\n\n\n\n\n\n\n\nLinestyles\nUsing the linestyle keyword argument, you can change the style of the line plotted using plt.plot(). These can be specified either by their name or a shortcut. A few of the style options (and their matplotlib shortcuts) are shown in the table below. To see a full list of linestyle options, see the docs.\n\n\n\nShort code\nLine style\n\n\n\n\n'-'\nsolid\n\n\n'--'\ndashed\n\n\n':'\ndotted\n\n\n'-.'\ndashdot\n\n\n\nAs we’ve already seen, the default linestyle is solid. The syntax for changing a line’s style is:\nplt.plot(x, y, linestyle='dashed')\nor, more commonly:\nplt.plot(x, y, linestyle='--')\nLet’s adjust the style of our waveform plot using the linestyle keyword argument.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave with different colors + linestyles\nplt.plot(x, np.sin(x - 0), color='darkblue', linestyle='-')\nplt.plot(x, np.sin(x - 1), color='m', linestyle='dashed')\nplt.plot(x, np.sin(x - 2), color=(0.0,0.8,0.81), linestyle=':') \nplt.plot(x, np.sin(x - 3), color='0.65', linestyle='solid')\nplt.plot(x, np.sin(x - 4), color='#B8D62E', linestyle='-.')\n\n\n\n\n\n\n\n\n\n\nMarkers\nMarkers can be used in plt.plot() and plt.scatter(). There are several available markers in matplotlib, and you can also define your own. A few of the most useful are shown in the table below.\n\n\n\nMarker code\nSymbol\nDescription\n\n\n\n\n'o'\n●\ncircle\n\n\n'.'\n⋅\npoint\n\n\n'*'\n★\nstar\n\n\n'+'\n\\(+\\)\nplus\n\n\n'x'\n\\(\\times\\)\nx\n\n\n'^'\n▲\ntriangle\n\n\n's'\n◼\nsquare\n\n\n\nNote that unlike color and linestyle, the marker keyword argument only accepts a code to specify the marker style.\nplt.scatter(x, y, marker='+')\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave as scatter plot with different colors + markers\nplt.scatter(xscat, yscat-0, color='darkblue', marker='o')\nplt.scatter(xscat, yscat-1, color='m', marker='.')\nplt.scatter(xscat, yscat-2, color=(0.0,0.8,0.81), marker='+')\nplt.scatter(xscat, yscat-3, color='0.65', marker='*')\nplt.scatter(xscat, yscat-4, color='#B8D62E', marker='s')\n\n\n\n\n\n\n\n\n\nUsing the marker keyword argument with the plt.plot() function creates a connected line plot, where the data points are designated by markers and connected by lines.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave with different colors + markers\nplt.plot(xscat, np.sin(xscat - 0), color='darkblue', marker='o')\nplt.plot(xscat, np.sin(xscat - 1), color='m', marker='.')\nplt.plot(xscat, np.sin(xscat - 2), color=(0.0,0.8,0.81), marker='+')\nplt.plot(xscat, np.sin(xscat - 3), color='0.65', marker='*')\nplt.plot(xscat, np.sin(xscat - 4), color='#B8D62E', marker='s')\n\n\n\n\n\n\n\n\n\n\n\nExplicit definitions vs. shortcuts\nUp to now, we have used explicit definitions to specify keyword arguments. While this is generally preferable, matplotlib does allow color, linestyle, and marker codes to be combined into a single, non-keyword argument. For example:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Plot a dashed red line\nplt.plot(x, y, 'r--')\n\n\n\n\n\n\n\n\n\nSeveral examples are presented in the cell below.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave with different colors + markers\nplt.plot(xscat, yscat-0, 'b-o')    # Solid blue line with circle markers\nplt.plot(xscat, yscat-1, 'm--*')   # Dashed magenta line with star markers\nplt.plot(xscat, yscat-2, 'c+')     # Cyan plus markers\nplt.plot(xscat, yscat-3, 'k')      # Solid black line\nplt.plot(xscat, yscat-4, 'y-s')    # Solid yellow line with square markers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs you can see, the downside of this method is that you are limited to the eight colors that have a single-letter code. To use other colors, you must use explicitly defined keyword arguments.\n\n\nIn addition to those we explored in this section, other useful keyword arguments include linewidth and markersize, which do exactly what you’d expect them to do. For a full list of keyword arguments (you should know what’s coming by now), see the docs."
  },
  {
    "objectID": "course-materials/interactive-sessions/7a_visualizations_1.html#axes-settings",
    "href": "course-materials/interactive-sessions/7a_visualizations_1.html#axes-settings",
    "title": "Interactive Session 7A",
    "section": "Axes settings",
    "text": "Axes settings\nNext, we will explore how to scale and annotate a plot using axes routines that control what goes on around the edges of the plot.\n\nLimits\nBy default, matplotlib will attempt to determine x- and y-axis limits, which usually work pretty well. Sometimes, however, it is useful to have finer control. The simplest way to adjust the display limits is to use the plt.xlim() and plt.ylim() methods.\nIn the example below, adjust the numbers (these can be int or float values) to see how the plot changes.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, color='darkblue')\n\n# Set axis limits\nplt.xlim(-5,5)\nplt.ylim(-2,2)\n\n\n\n\n\n\n\n\n\n\n\nTicks and Tick Labels\nYou may also find it useful to adjust the ticks and/or tick labels that matplotlib displays by default. The plt.xticks() and plt.yticks() methods allow you to control the locations of both the ticks and the labels on the x- and y-axes, respectively. Both methods accept two list or array-like arguments, as well as optional keyword arguments. The first corresponds to the ticks, while the second controls the tick labels.\n# Set x-axis ticks at 0, 0.25, 0.5, 0.75, 1.0 with all labeled\nplt.xticks([0,0.25,0.5,0.75,1.0])\n# Set y-axis ticks from 0 to 100 with ticks on 10s and labels on 20s\nplt.yticks(np.arange(0,101,10),['0','','20','','40','','60','','80','','100'])\n\n\n\n\n\n\nImportant\n\n\n\nIf the labels are not specified, all ticks will be labeled accordingly. To only label certain ticks, you must pass a list with empty strings in the location of the ticks you wish to leave unlabeled (or the ticks will be labeled in order).\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, color='darkblue')\n\n# Set x-axis limits\nplt.xlim(-5,5)\n\n# Set axis ticks\nplt.xticks([-4,-3,-2,-1,0,1,2,3,4],['-4','','-2','','0','','2','','4'])\nplt.yticks([-1,-0.5,0,0.5,1])\n\n\n([&lt;matplotlib.axis.YTick at 0x1b51ecad0&gt;,\n  &lt;matplotlib.axis.YTick at 0x1b51ea350&gt;,\n  &lt;matplotlib.axis.YTick at 0x1b51bb910&gt;,\n  &lt;matplotlib.axis.YTick at 0x1b51c8690&gt;,\n  &lt;matplotlib.axis.YTick at 0x1b51c3690&gt;],\n [Text(0, -1.0, '−1.0'),\n  Text(0, -0.5, '−0.5'),\n  Text(0, 0.0, '0.0'),\n  Text(0, 0.5, '0.5'),\n  Text(0, 1.0, '1.0')])\n\n\n\n\n\n\n\n\n\nAs with any plot, it is imperative to include x- and y-axis labels. This can be done by passing strings to the plt.xlabel() and plt.ylabel() methods:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, color='darkblue')\n\n# Set x-axis limits\nplt.xlim(-5,5)\n\n# Set axis ticks\nplt.xticks([-4,-3,-2,-1,0,1,2,3,4],['-4','','-2','','0','','2','','4'])\nplt.yticks([-1,-0.5,0,0.5,1])\n\n# Set axis labels\nplt.xlabel('x-axis')\nplt.ylabel('y-axis')\n\n\nText(0, 0.5, 'y-axis')\n\n\n\n\n\n\n\n\n\nA nice feature about matplotlib is that it supports TeX formatting for mathematical expressions. This is quite useful for displaying equations, exponents, units, and other mathematical operators. The syntax for TeX expressions is 'r$TeX expression here$'. For example, we can display the axis labels as \\(x\\) and \\(\\sin{(x)}\\) as follows:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, color='darkblue')\n\n# Set x-axis limits\nplt.xlim(-5,5)\n\n# Set axis ticks\nplt.xticks([-4,-3,-2,-1,0,1,2,3,4],['-4','','-2','','0','','2','','4'])\nplt.yticks([-1,-0.5,0,0.5,1])\n\n# Set axis labels\nplt.xlabel(r'$x$')\nplt.ylabel(r'$\\sin{(x)}$')\n\n\nText(0, 0.5, '$\\\\sin{(x)}$')\n\n\n\n\n\n\n\n\n\n\n\nTitles\nAdding a title to your plot is analogous to labeling the x- and y-axes. The plt.title() method allows you to set the title of your plot by passing a string:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, color='darkblue')\nplt.plot(x, ycos, color='#B8D62E')\n\n# Set x-axis limits\nplt.xlim(-5,5)\n\n# Set axis ticks\nplt.xticks([-4,-3,-2,-1,0,1,2,3,4],['-4','','-2','','0','','2','','4'])\nplt.yticks([-1,-0.5,0,0.5,1])\n\n# Set axis labels\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\n\n# Set title\nplt.title('Sinusoidal functions')\n\n\nText(0.5, 1.0, 'Sinusoidal functions')\n\n\n\n\n\n\n\n\n\n\n\nLegends\nWhen multiple datasets are plotted on the same axes it is often useful to include a legend that labels each line or set of points. Matplotlib has a quick way of displaying a legend using the plt.legend() method. There are multiple ways of specifying the label for each dataset; I prefer to pass a list of strings to plt.legend():\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, color='darkblue')\nplt.plot(x, ycos, color='#B8D62E')\n\n# Set x-axis limits\nplt.xlim(-5,5)\n\n# Set axis ticks\nplt.xticks([-4,-3,-2,-1,0,1,2,3,4],['-4','','-2','','0','','2','','4'])\nplt.yticks([-1,-0.5,0,0.5,1])\n\n# Set axis labels\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\n\n# Set title\nplt.title('Sinusoidal functions')\n\n# Legend\nplt.legend(labels=['sin(x)','cos(x)'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnother way of setting the data labels is to use the label keyword argument in the plt.plot() (or plt.scatter()) function:\n# Plot data\nplt.plot(x1, y1, label='Data1')\nplt.plot(x2, y2, label='Data2')\n\n# Legend\nplt.legend()\nNote that you must still run plt.legend() to display the legend.\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, label='sin(x)', color='darkblue')\nplt.plot(x, ycos, label='cos(x)', color='#B8D62E')\n\n# Set x-axis limits\nplt.xlim(-5,5)\n\n# Set axis ticks\nplt.xticks([-4,-3,-2,-1,0,1,2,3,4],['-4','','-2','','0','','2','','4'])\nplt.yticks([-1,-0.5,0,0.5,1])\n\n# Set axis labels\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\n\n# Set title\nplt.title('Sinusoidal functions')\n\n# Legend\nplt.legend()"
  },
  {
    "objectID": "course-materials/interactive-sessions/7a_visualizations_1.html#subplots-multiple-axes",
    "href": "course-materials/interactive-sessions/7a_visualizations_1.html#subplots-multiple-axes",
    "title": "Interactive Session 7A",
    "section": "Subplots + multiple axes",
    "text": "Subplots + multiple axes\nNow that we’ve established the basics of plotting in matplotlib, let’s get a bit more complicated. Oftentimes, you may want to plot data on multiple axes within the same figure. The easiest way to do this in matplotlib is to use the plt.subplot() function, which takes three non-keyword arguments: nrows, ncols, and index. nrows and ncols correspond to the total number of rows and columns of the entire figure, while index refers to the index position of the current axes. Importantly (and annoyingly), the index for subplots starts in the upper left corner at 1 (not 0)!. The image below contains a few examples of how matplotlib arranges subplots.\n\n\n\n\nmatplotlib subplots\n\n\n\nThe most explicit way of adding subplots is to use the fig.add_subplot() command to initialize new axes as variables. This allows you to access each Axes object later to plot data and adjust the axes parameters.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig = plt.figure()\n# Add four axes\nax1 = fig.add_subplot(2,2,1)\nax2 = fig.add_subplot(2,2,2)\nax3 = fig.add_subplot(2,2,3)\nax4 = fig.add_subplot(2,2,4)\n\n\n\n\n\n\n\n\n\nTo plot data, we use ax.plot() or ax.scatter(). These methods are analogous to plt.plot() and plt.scatter(), but they act on individual Axes, rather than the Figure object.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig = plt.figure()\n# Add four axes\nax1 = fig.add_subplot(2,2,1)\nax2 = fig.add_subplot(2,2,2)\nax3 = fig.add_subplot(2,2,3)\nax4 = fig.add_subplot(2,2,4)\n\n# Plot data\n# Plot sine wave with different colors on different axes\nax1.plot(x, np.sin(x - 0), color='darkblue')\nax2.plot(x, np.sin(x - 1), color='m')\nax3.plot(x, np.sin(x - 2), color=(0.0,0.8,0.81))\nax4.plot(x, np.sin(x - 4), color='#B8D62E')\n\n\n\n\n\n\n\n\n\n\nFigure vs. Axes methods\nPerhaps the trickiest part about subplots – and Axes methods in general – is adjusting the axes settings. While most Figure functions translate directly Axes methods (e.g. plt.plot() \\(\\rightarrow\\) ax.plot(), plt.legend() \\(\\rightarrow\\) ax.legend()), commands to set limits, ticks, labels, and titles are slightly modified. Some important Figure methods and their Axes counterparts are shown in the table below.\n\n\n\nFigure command\nAxes command\n\n\n\n\nplt.xlabel()\nax.set_xlabel()\n\n\nplt.ylabel()\nax.set_ylabel()\n\n\nplt.xlim()\nax.set_xlim()\n\n\nplt.ylim()\nax.set_ylim()\n\n\nplt.xticks()\nax.set_xticks()\n\n\nplt.yticks()\nax.set_yticks()\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese commands are different primarily because the Figure functions are inherited from MATLAB syntax (remember, matplotlib was design to work exactly like matlab functions), while the Axes functions were developed later and are object-oriented. Generally, the arguments are similar – if not identical – between the two.\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig = plt.figure()\n# Add four axes\nax1 = fig.add_subplot(2,2,1)\nax2 = fig.add_subplot(2,2,2)\nax3 = fig.add_subplot(2,2,3)\nax4 = fig.add_subplot(2,2,4)\n\n# Plot data\n# Plot sine wave with different colors on different axes\nax1.plot(x, np.sin(x - 0), color='darkblue')\nax2.plot(x, np.sin(x - 1), color='m')\nax3.plot(x, np.sin(x - 2), color=(0.0,0.8,0.81))\nax4.plot(x, np.sin(x - 4), color='#B8D62E')\n\n# Set axes limits, labels, + ticks\nfor i,ax in enumerate([ax1,ax2,ax3,ax4]):\n    # i is the list index, but subplots count from 1.\n    # so make a new variable to keep track of subplot number:\n    subplot_number =  i + 1 \n    # Set x limits \n    ax.set_xlim(-5,5)\n    # Set title\n    ax.set_title(f'$\\sin{{(x - {i})}}$')\n    # Only label x ticks and x-axis on bottom row\n    if subplot_number &lt; 3:\n        ax.set_xticklabels([])\n    else:\n        ax.set_xlabel('x')\n    # Only label y ticks and y-axis on left column\n    if subplot_number == 1 or subplot_number == 3:\n        ax.set_ylabel('y')\n    else:\n        ax.set_yticklabels([])\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the last example, we included a command, plt.tight_layout(), which automatically formats the figure to fit the window. This is most useful when using an IDE with a separate plotting window, rather than with in-line plots like those in a notebook. To get a sense of what plt.tight_layout() does, try re-running the above cell with this command commented out.\n\n\nTo go beyond regularly gridded subplots and create subplots that span multiple rows and/or columns, check out GridSpec.\n\nEnd interactive session 7A"
  },
  {
    "objectID": "course-materials/cheatsheets/chart_customization.html",
    "href": "course-materials/cheatsheets/chart_customization.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, color='red', linestyle='--', linewidth=2, marker='o', markersize=5)\nplt.title('Customized Line Plot', fontsize=16)\nplt.xlabel('X-axis', fontsize=12)\nplt.ylabel('Y-axis', fontsize=12)\nplt.grid(True, linestyle=':')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.xlim(0, 5)\nplt.ylim(-1.5, 1.5)\nplt.xticks(np.arange(0, 6, 1))\nplt.yticks(np.arange(-1.5, 1.6, 0.5))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.plot(x, np.sin(x), label='sin(x)')\nplt.plot(x, np.cos(x), label='cos(x)')\nplt.legend(fontsize=12, loc='upper right')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.title('Sine Wave', fontsize=16, fontweight='bold')\nplt.text(5, 0.5, 'Peak', fontsize=14, color='red')\nplt.annotate('Trough', xy=(3*np.pi/2, -1), xytext=(6, -0.5),\n             arrowprops=dict(facecolor='black', shrink=0.05))\nplt.show()"
  },
  {
    "objectID": "course-materials/cheatsheets/chart_customization.html#matplotlib-customization",
    "href": "course-materials/cheatsheets/chart_customization.html#matplotlib-customization",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, color='red', linestyle='--', linewidth=2, marker='o', markersize=5)\nplt.title('Customized Line Plot', fontsize=16)\nplt.xlabel('X-axis', fontsize=12)\nplt.ylabel('Y-axis', fontsize=12)\nplt.grid(True, linestyle=':')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.xlim(0, 5)\nplt.ylim(-1.5, 1.5)\nplt.xticks(np.arange(0, 6, 1))\nplt.yticks(np.arange(-1.5, 1.6, 0.5))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.plot(x, np.sin(x), label='sin(x)')\nplt.plot(x, np.cos(x), label='cos(x)')\nplt.legend(fontsize=12, loc='upper right')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.title('Sine Wave', fontsize=16, fontweight='bold')\nplt.text(5, 0.5, 'Peak', fontsize=14, color='red')\nplt.annotate('Trough', xy=(3*np.pi/2, -1), xytext=(6, -0.5),\n             arrowprops=dict(facecolor='black', shrink=0.05))\nplt.show()"
  },
  {
    "objectID": "course-materials/cheatsheets/chart_customization.html#seaborn-customization",
    "href": "course-materials/cheatsheets/chart_customization.html#seaborn-customization",
    "title": "EDS 217 Cheatsheet",
    "section": "Seaborn Customization",
    "text": "Seaborn Customization\n\nSetting the Style\n\n\nCode\nimport seaborn as sns\nimport pandas as pd\n\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"deep\")\n\n\n\n\nLoading and Preparing Data\n\n\nCode\n# Load the tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Display the first few rows and data types\nprint(tips.head())\nprint(\"\\nData Types:\")\nprint(tips.dtypes)\n\n# Select only numeric columns for correlation\nnumeric_columns = tips.select_dtypes(include=[np.number]).columns\ntips_numeric = tips[numeric_columns]\n\n\n   total_bill   tip     sex smoker  day    time  size\n0       16.99  1.01  Female     No  Sun  Dinner     2\n1       10.34  1.66    Male     No  Sun  Dinner     3\n2       21.01  3.50    Male     No  Sun  Dinner     3\n3       23.68  3.31    Male     No  Sun  Dinner     2\n4       24.59  3.61  Female     No  Sun  Dinner     4\n\nData Types:\ntotal_bill     float64\ntip            float64\nsex           category\nsmoker        category\nday           category\ntime          category\nsize             int64\ndtype: object\n\n\n\n\nCustomizing a Scatter Plot\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=tips, x=\"total_bill\", y=\"tip\", hue=\"time\", size=\"size\")\nplt.title(\"Tips vs Total Bill\", fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCustomizing a Box Plot\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=tips, x=\"day\", y=\"total_bill\", palette=\"Set3\")\nplt.title(\"Total Bill by Day\", fontsize=16)\nplt.show()\n\n\n/var/folders/bs/x9tn9jz91cv6hb3q6p4djbmw0000gn/T/ipykernel_60342/2996973332.py:2: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(data=tips, x=\"day\", y=\"total_bill\", palette=\"Set3\")\n\n\n\n\n\n\n\n\n\n\n\nCustomizing a Heatmap (Correlation of Numeric Columns)\n\n\nCode\ncorr = tips_numeric.corr()\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr, annot=True, cmap=\"coolwarm\", linewidths=0.5)\nplt.title(\"Correlation Heatmap of Numeric Columns\", fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCustomizing a Pair Plot\n\n\nCode\nsns.pairplot(tips, hue=\"time\", palette=\"husl\", height=2.5, \n             vars=[\"total_bill\", \"tip\", \"size\"])\nplt.suptitle(\"Pair Plot of Tips Dataset\", y=1.02, fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCustomizing a Regression Plot\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.regplot(data=tips, x=\"total_bill\", y=\"tip\", scatter_kws={\"color\": \"blue\"}, line_kws={\"color\": \"red\"})\nplt.title(\"Regression Plot: Tip vs Total Bill\", fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCustomizing a Categorical Plot\n\n\nCode\nplt.figure(figsize=(12, 6))\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\", hue=\"sex\", kind=\"violin\", split=True)\nplt.title(\"Distribution of Total Bill by Day and Sex\", fontsize=16)\nplt.show()\n\n\n&lt;Figure size 1152x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nRemember, you can always combine Matplotlib and Seaborn customizations for even more control over your visualizations!"
  },
  {
    "objectID": "course-materials/cheatsheets/random_numbers.html",
    "href": "course-materials/cheatsheets/random_numbers.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "For information on the previous np.random API and its use cases, please refer to the NumPy documentation on legacy random generation: NumPy Legacy Random Generation\nThis cheatsheet focuses on the modern Generator-based approach to random number generation in NumPy.\n\n\n\n\nCode\nimport numpy as np\n\n\n\n\n\n\n\nCode\n# Create a Generator with the default BitGenerator\nrng = np.random.default_rng()\n\n# Create a Generator with a specific seed\nrng_seeded = np.random.default_rng(seed=42)\n\n\n\n\n\n\n\n\n\nCode\n# Single random float\nprint(rng.random())\n\n# Array of random floats\nprint(rng.random(5))\n\n\n0.8890775303012091\n[0.65737983 0.65054958 0.42176933 0.95505743 0.0836521 ]\n\n\n\n\n\n\n\nCode\n# Single random integer from 0 to 10 (inclusive)\nprint(rng.integers(11))\n\n# Array of random integers from 1 to 100 (inclusive)\nprint(rng.integers(1, 101, size=5))\n\n\n10\n[61 54 17 49  9]\n\n\n\n\n\n\n\nCode\n# Single value from standard normal distribution\nprint(rng.standard_normal())\n\n# Array from normal distribution with mean=0, std=1\nprint(rng.normal(loc=0, scale=1, size=5))\n\n\n2.195891626775996\n[-0.3747872  -0.42025509 -0.37190524  0.00428236  0.29445587]\n\n\n\n\n\n\n\n\nCode\n# Random choice from an array\narr = np.array([1, 2, 3, 4, 5])\nprint(rng.choice(arr))\n\n# Random sample without replacement\nprint(rng.choice(arr, size=3, replace=False))\n\n\n4\n[2 3 4]\n\n\n\n\n\n\n\nCode\narr = np.arange(10)\nrng.shuffle(arr)\nprint(arr)\n\n\n[0 9 1 6 3 5 2 7 8 4]\n\n\n\n\n\nGenerators provide methods for many other distributions:\n\n\nCode\n# Poisson distribution\nprint(rng.poisson(lam=5, size=3))\n\n# Exponential distribution\nprint(rng.exponential(scale=1.0, size=3))\n\n# Binomial distribution\nprint(rng.binomial(n=10, p=0.5, size=3))\n\n\n[5 4 3]\n[0.39447646 0.17379433 0.83065584]\n[4 7 5]\n\n\n\n\n\nGenerators can fill existing arrays, which can be more efficient:\n\n\nCode\narr = np.empty(5)\nrng.random(out=arr)\nprint(arr)\n\n\n[0.7749086  0.71061749 0.24980754 0.30759777 0.31771172]\n\n\n\n\n\nYou can use different Bit Generators with varying statistical qualities:\n\n\nCode\nfrom numpy.random import PCG64, MT19937\n\nrng_pcg = np.random.Generator(PCG64())\nrng_mt = np.random.Generator(MT19937())\n\nprint(\"PCG64:\", rng_pcg.random())\nprint(\"MT19937:\", rng_mt.random())\n\n\nPCG64: 0.7799345206883059\nMT19937: 0.11528059637311394\n\n\n\n\n\n\n\nCode\n# Save state\nstate = rng.bit_generator.state\n\n# Generate some numbers\nprint(\"Original:\", rng.random(3))\n\n# Restore state and regenerate\nrng.bit_generator.state = state\nprint(\"Restored:\", rng.random(3))\n\n\nOriginal: [0.94623753 0.48542917 0.84675625]\nRestored: [0.94623753 0.48542917 0.84675625]\n\n\n\n\n\nYou can create independent generators from an existing one:\n\n\nCode\nchild1, child2 = rng.spawn(2)\nprint(\"Child 1:\", child1.random())\nprint(\"Child 2:\", child2.random())\n\n\nChild 1: 0.36489335398556755\nChild 2: 0.09512169764230916\n\n\n\n\n\nGenerators are designed to be thread-safe and support “jumping” ahead in the sequence:\n\n\nCode\nrng = np.random.Generator(PCG64())\nrng.bit_generator.advance(1000)  # Jump ahead 1000 steps\n\n\n&lt;numpy.random._pcg64.PCG64 at 0x1b05fec40&gt;\n\n\n\n\n\n\nUse default_rng() to create a Generator unless you have specific requirements for a different Bit Generator.\nSet a seed for reproducibility in scientific computations and testing.\nUse the spawn() method to create independent generators for parallel processing.\nWhen performance is critical, consider using the out parameter to fill existing arrays.\nFor very long periods or when security is important, consider using the PCG64DXSM Bit Generator.\n\nRemember, Generators provide a more robust, flexible, and future-proof approach to random number generation in NumPy. They offer better statistical properties and are designed to work well in both single-threaded and multi-threaded environments."
  },
  {
    "objectID": "course-materials/cheatsheets/random_numbers.html#numpy-generator-based-random-number-generation-cheatsheet",
    "href": "course-materials/cheatsheets/random_numbers.html#numpy-generator-based-random-number-generation-cheatsheet",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "For information on the previous np.random API and its use cases, please refer to the NumPy documentation on legacy random generation: NumPy Legacy Random Generation\nThis cheatsheet focuses on the modern Generator-based approach to random number generation in NumPy.\n\n\n\n\nCode\nimport numpy as np\n\n\n\n\n\n\n\nCode\n# Create a Generator with the default BitGenerator\nrng = np.random.default_rng()\n\n# Create a Generator with a specific seed\nrng_seeded = np.random.default_rng(seed=42)\n\n\n\n\n\n\n\n\n\nCode\n# Single random float\nprint(rng.random())\n\n# Array of random floats\nprint(rng.random(5))\n\n\n0.8890775303012091\n[0.65737983 0.65054958 0.42176933 0.95505743 0.0836521 ]\n\n\n\n\n\n\n\nCode\n# Single random integer from 0 to 10 (inclusive)\nprint(rng.integers(11))\n\n# Array of random integers from 1 to 100 (inclusive)\nprint(rng.integers(1, 101, size=5))\n\n\n10\n[61 54 17 49  9]\n\n\n\n\n\n\n\nCode\n# Single value from standard normal distribution\nprint(rng.standard_normal())\n\n# Array from normal distribution with mean=0, std=1\nprint(rng.normal(loc=0, scale=1, size=5))\n\n\n2.195891626775996\n[-0.3747872  -0.42025509 -0.37190524  0.00428236  0.29445587]\n\n\n\n\n\n\n\n\nCode\n# Random choice from an array\narr = np.array([1, 2, 3, 4, 5])\nprint(rng.choice(arr))\n\n# Random sample without replacement\nprint(rng.choice(arr, size=3, replace=False))\n\n\n4\n[2 3 4]\n\n\n\n\n\n\n\nCode\narr = np.arange(10)\nrng.shuffle(arr)\nprint(arr)\n\n\n[0 9 1 6 3 5 2 7 8 4]\n\n\n\n\n\nGenerators provide methods for many other distributions:\n\n\nCode\n# Poisson distribution\nprint(rng.poisson(lam=5, size=3))\n\n# Exponential distribution\nprint(rng.exponential(scale=1.0, size=3))\n\n# Binomial distribution\nprint(rng.binomial(n=10, p=0.5, size=3))\n\n\n[5 4 3]\n[0.39447646 0.17379433 0.83065584]\n[4 7 5]\n\n\n\n\n\nGenerators can fill existing arrays, which can be more efficient:\n\n\nCode\narr = np.empty(5)\nrng.random(out=arr)\nprint(arr)\n\n\n[0.7749086  0.71061749 0.24980754 0.30759777 0.31771172]\n\n\n\n\n\nYou can use different Bit Generators with varying statistical qualities:\n\n\nCode\nfrom numpy.random import PCG64, MT19937\n\nrng_pcg = np.random.Generator(PCG64())\nrng_mt = np.random.Generator(MT19937())\n\nprint(\"PCG64:\", rng_pcg.random())\nprint(\"MT19937:\", rng_mt.random())\n\n\nPCG64: 0.7799345206883059\nMT19937: 0.11528059637311394\n\n\n\n\n\n\n\nCode\n# Save state\nstate = rng.bit_generator.state\n\n# Generate some numbers\nprint(\"Original:\", rng.random(3))\n\n# Restore state and regenerate\nrng.bit_generator.state = state\nprint(\"Restored:\", rng.random(3))\n\n\nOriginal: [0.94623753 0.48542917 0.84675625]\nRestored: [0.94623753 0.48542917 0.84675625]\n\n\n\n\n\nYou can create independent generators from an existing one:\n\n\nCode\nchild1, child2 = rng.spawn(2)\nprint(\"Child 1:\", child1.random())\nprint(\"Child 2:\", child2.random())\n\n\nChild 1: 0.36489335398556755\nChild 2: 0.09512169764230916\n\n\n\n\n\nGenerators are designed to be thread-safe and support “jumping” ahead in the sequence:\n\n\nCode\nrng = np.random.Generator(PCG64())\nrng.bit_generator.advance(1000)  # Jump ahead 1000 steps\n\n\n&lt;numpy.random._pcg64.PCG64 at 0x1b05fec40&gt;\n\n\n\n\n\n\nUse default_rng() to create a Generator unless you have specific requirements for a different Bit Generator.\nSet a seed for reproducibility in scientific computations and testing.\nUse the spawn() method to create independent generators for parallel processing.\nWhen performance is critical, consider using the out parameter to fill existing arrays.\nFor very long periods or when security is important, consider using the PCG64DXSM Bit Generator.\n\nRemember, Generators provide a more robust, flexible, and future-proof approach to random number generation in NumPy. They offer better statistical properties and are designed to work well in both single-threaded and multi-threaded environments."
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes.html#getting-started",
    "href": "course-materials/interactive-sessions/4a_dataframes.html#getting-started",
    "title": "Interactive Session",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes.html#introduction",
    "href": "course-materials/interactive-sessions/4a_dataframes.html#introduction",
    "title": "Interactive Session",
    "section": "Introduction",
    "text": "Introduction\nIn this interactive session, we’ll explore the basics of working with pandas DataFrames using a dataset of world cities. We’ll cover importing data, basic DataFrame operations, and essential methods for data exploration and manipulation. This session will prepare you for more advanced data analysis tasks and upcoming collaborative coding exercises."
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes.html#learning-objectives",
    "href": "course-materials/interactive-sessions/4a_dataframes.html#learning-objectives",
    "title": "Interactive Session",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, you will be able to:\n\nImport data into a pandas DataFrame\nExplore basic DataFrame properties and methods\nPerform simple data filtering and selection operations\nUse basic aggregation and grouping functions"
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes.html#setting-up",
    "href": "course-materials/interactive-sessions/4a_dataframes.html#setting-up",
    "title": "Interactive Session",
    "section": "Setting Up",
    "text": "Setting Up\nLet’s start by importing the pandas library and loading our dataset.\n\n\nCode\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes.html#basic-data-importing",
    "href": "course-materials/interactive-sessions/4a_dataframes.html#basic-data-importing",
    "title": "Interactive Session",
    "section": "1. Basic Data Importing",
    "text": "1. Basic Data Importing\n\n\nCode\nurl = \"https://raw.githubusercontent.com/datasets/world-cities/master/data/world-cities.csv\"\ncities_df = pd.read_csv(url)"
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes.html#basic-dataframe-exploration",
    "href": "course-materials/interactive-sessions/4a_dataframes.html#basic-dataframe-exploration",
    "title": "Interactive Session",
    "section": "2. Basic DataFrame Exploration",
    "text": "2. Basic DataFrame Exploration\n\nViewing the Data\nLet’s take a look at the first few rows of our DataFrame:\n\n\nCode\nprint(cities_df.head())\n\n\n                 name               country          subcountry  geonameid\n0        les Escaldes               Andorra  Escaldes-Engordany    3040051\n1    Andorra la Vella               Andorra    Andorra la Vella    3041563\n2             Warīsān  United Arab Emirates               Dubai     290503\n3          Umm Suqaym  United Arab Emirates               Dubai     290581\n4  Umm Al Quwain City  United Arab Emirates        UmmalQaywayn     290594\n\n\nTo see the last few rows, we can use:\n\n\nCode\nprint(cities_df.tail())\n\n\n                         name   country                   subcountry  \\\n31694                 Bindura  Zimbabwe          Mashonaland Central   \n31695              Beitbridge  Zimbabwe  Matabeleland South Province   \n31696                 Epworth  Zimbabwe                       Harare   \n31697             Chitungwiza  Zimbabwe                       Harare   \n31698  Harare Western Suburbs  Zimbabwe             Mashonaland West   \n\n       geonameid  \n31694     895061  \n31695     895269  \n31696    1085510  \n31697    1106542  \n31698   13132735  \n\n\n\n\nDataFrame Properties\nNow, let’s explore some basic properties of our DataFrame:\n\n\nCode\n# Number of rows and columns\nprint(\"Shape:\", cities_df.shape)\n\n# Column names\nprint(\"\\nColumns:\", cities_df.columns)\n\n# Data types of each column\nprint(\"\\nData types:\\n\", cities_df.dtypes)\n\n# Summary statistics of numeric columns (if any)\nprint(\"\\nSummary statistics:\\n\", cities_df.describe())\n\n\nShape: (31699, 4)\n\nColumns: Index(['name', 'country', 'subcountry', 'geonameid'], dtype='object')\n\nData types:\n name          object\ncountry       object\nsubcountry    object\ngeonameid      int64\ndtype: object\n\nSummary statistics:\n           geonameid\ncount  3.169900e+04\nmean   3.266489e+06\nstd    2.863419e+06\nmin    4.900000e+02\n25%    1.277083e+06\n50%    2.636503e+06\n75%    3.693436e+06\nmax    1.349420e+07\n\n\n\n\nChecking for Missing Values\nIt’s important to identify any missing data in your DataFrame:\n\n\nCode\nprint(cities_df.isnull().sum())\n\n\nname            0\ncountry         0\nsubcountry    118\ngeonameid       0\ndtype: int64"
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes.html#baisc-cleaning",
    "href": "course-materials/interactive-sessions/4a_dataframes.html#baisc-cleaning",
    "title": "Interactive Session",
    "section": "3. Baisc Cleaning",
    "text": "3. Baisc Cleaning\nRemove rows with missing data in subcountry using dropna() and the subset argument.\n\n\nCode\ncities_df = cities_df.dropna(subset=['subcountry'])"
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes.html#basic-data-selection-and-filtering",
    "href": "course-materials/interactive-sessions/4a_dataframes.html#basic-data-selection-and-filtering",
    "title": "Interactive Session",
    "section": "4. Basic Data Selection and Filtering",
    "text": "4. Basic Data Selection and Filtering\n\nSelecting Columns\nTo select specific columns:\n\n\nCode\n# Select a single column\nprint(cities_df['name'].head())\n\n# Select multiple columns\nprint(cities_df[['name', 'country', 'subcountry']].head())\n\n\n0          les Escaldes\n1      Andorra la Vella\n2               Warīsān\n3            Umm Suqaym\n4    Umm Al Quwain City\nName: name, dtype: object\n                 name               country          subcountry\n0        les Escaldes               Andorra  Escaldes-Engordany\n1    Andorra la Vella               Andorra    Andorra la Vella\n2             Warīsān  United Arab Emirates               Dubai\n3          Umm Suqaym  United Arab Emirates               Dubai\n4  Umm Al Quwain City  United Arab Emirates        UmmalQaywayn\n\n\n\n\nFiltering Rows\nWe can filter rows based on conditions:\n\n\nCode\n# Cities in the United States\nus_cities = cities_df[cities_df['country'] == 'United States']\nprint(us_cities[['name', 'country']].head())\n\n# Cities in California\ncalifornia_cities = cities_df[(cities_df['country'] == 'United States') & (cities_df['subcountry'] == 'California')]\nprint(california_cities[['name', 'country', 'subcountry']].head())\n\n\n             name        country\n27316   Fort Hunt  United States\n27317    Bessemer  United States\n27318     Paducah  United States\n27319  Birmingham  United States\n27320     Cordova  United States\n                name        country  subcountry\n29728       Fillmore  United States  California\n29777       Adelanto  United States  California\n29778         Agoura  United States  California\n29779   Agoura Hills  United States  California\n29780  Agua Caliente  United States  California\n\n\n\n\nCombining Conditions\nWe can use logical operators to combine multiple conditions:\n\n\nCode\n# Cities in Canada that start with the letter 'T'\ncanadian_t_cities = cities_df[(cities_df['country'] == 'Canada') & (cities_df['name'].str.startswith('T'))]\nprint(canadian_t_cities[['name', 'country', 'subcountry']])\n\n\n                        name country        subcountry\n3986  Tam O'Shanter-Sullivan  Canada           Ontario\n3987                Tecumseh  Canada           Ontario\n3988           Templeton-Est  Canada            Quebec\n3989                 Terrace  Canada  British Columbia\n3990              Terrebonne  Canada            Quebec\n3991             The Beaches  Canada           Ontario\n3992                 Thorold  Canada           Ontario\n3993             Thunder Bay  Canada           Ontario\n3994             Tillsonburg  Canada           Ontario\n3995                 Timmins  Canada           Ontario\n3996                 Toronto  Canada           Ontario\n3997          Trois-Rivières  Canada            Quebec\n3998              Tsawwassen  Canada  British Columbia\n4038          Thetford-Mines  Canada            Quebec\n4051       Trinity-Bellwoods  Canada           Ontario\n4080           Taylor-Massey  Canada           Ontario\n4094        Thorncliffe Park  Canada           Ontario"
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes.html#basic-sorting-and-ranking",
    "href": "course-materials/interactive-sessions/4a_dataframes.html#basic-sorting-and-ranking",
    "title": "Interactive Session",
    "section": "5. Basic Sorting and Ranking",
    "text": "5. Basic Sorting and Ranking\nTo sort the DataFrame based on one or more columns:\n\n\nCode\n# Sort cities alphabetically\nsorted_cities = cities_df.sort_values('name')\nprint(sorted_cities[['name', 'country']].head())\n\n# Sort cities by country, then by name\nsorted_cities_by_country = cities_df.sort_values(['country', 'name'])\nprint(sorted_cities_by_country[['name', 'country']].head())\n\n\n                      name      country\n21637       's-Gravenzande  Netherlands\n21636     's-Hertogenbosch  Netherlands\n25121            'Ārdamatā        Sudan\n9057   6th of October City        Egypt\n9688              A Coruña        Spain\n         name      country\n112   Andkhōy  Afghanistan\n111  Asadābād  Afghanistan\n72      Aībak  Afghanistan\n108   Baghlān  Afghanistan\n107     Balkh  Afghanistan"
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes.html#basic-transformations",
    "href": "course-materials/interactive-sessions/4a_dataframes.html#basic-transformations",
    "title": "Interactive Session",
    "section": "6. Basic Transformations",
    "text": "6. Basic Transformations\n\nCreating New Columns\nWe can create new columns based on existing data:\n\n\nCode\n# Create a column for city name length\ncities_df['name_length'] = cities_df['name'].str.len()\n\n# Display the top 5 cities with the longest names\nlong_named_cities = cities_df.nlargest(5, 'name_length')\nprint(long_named_cities[['name', 'country', 'name_length']])\n\n\n                                                    name        country  \\\n22968  Karachi University Employees Co-operative Hous...       Pakistan   \n30524      Diamond Head / Kapahulu / Saint Louis Heights  United States   \n8114             Universitäts- und Hansestadt Greifswald        Germany   \n30676             Aliamanu / Salt Lakes / Foster Village  United States   \n6569               Sandaoling Lutiankuang Wuqi Nongchang          China   \n\n       name_length  \n22968           57  \n30524           45  \n8114            39  \n30676           38  \n6569            37"
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes.html#basic-grouping-and-aggregation",
    "href": "course-materials/interactive-sessions/4a_dataframes.html#basic-grouping-and-aggregation",
    "title": "Interactive Session",
    "section": "7-8: Basic Grouping and Aggregation",
    "text": "7-8: Basic Grouping and Aggregation\nGrouping allows us to perform operations on subsets of the data:\n\n\nCode\n# Number of cities by country\ncities_per_country = cities_df.groupby('country')['name'].count().sort_values(ascending=False)\nprint(cities_per_country.head())\n\n# Number of subcountries (e.g., states, provinces) by country\nsubcountries_per_country = cities_df.groupby('country')['subcountry'].nunique().sort_values(ascending=False)\nprint(subcountries_per_country.head())\n\n\ncountry\nUnited States    3367\nIndia            3312\nBrazil           2111\nChina            1999\nJapan            1293\nName: name, dtype: int64\ncountry\nRussian Federation    83\nTürkiye               81\nThailand              75\nAlgeria               53\nUnited States         51\nName: subcountry, dtype: int64"
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes.html#conclusion",
    "href": "course-materials/interactive-sessions/4a_dataframes.html#conclusion",
    "title": "Interactive Session",
    "section": "Conclusion",
    "text": "Conclusion\nIn this session, we’ve covered the basics of working with pandas DataFrames using a world cities dataset, including:\n\nImporting data\nExploring DataFrame properties\nSelecting and filtering data\nSorting and ranking\nGrouping and aggregation\nCreating new columns\n\nThese skills form the foundation of data analysis with pandas and will be essential for upcoming exercises and projects. Remember, pandas has many more functions and methods that we haven’t covered here. Don’t hesitate to explore the pandas documentation for more advanced features!"
  },
  {
    "objectID": "course-materials/interactive-sessions/4a_dataframes.html#resources",
    "href": "course-materials/interactive-sessions/4a_dataframes.html#resources",
    "title": "Interactive Session",
    "section": "Resources",
    "text": "Resources\n\nEDS 217 Pandas Cheatsheet\nPandas Workflows Functions\nPandas PDF Cheatsheet"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day5.html#introduction",
    "href": "course-materials/eod-practice/eod-day5.html#introduction",
    "title": "Day 5: Tasks & Activities",
    "section": "Introduction",
    "text": "Introduction\nIn this activity, you’ll explore the “Banana Index” dataset, which compares the environmental impact of various food products to that of a banana. These data were developed by the Economist magazine in 2023 and they posted their data to github for us to use. This exercise will help you practice working with pandas DataFrames, data manipulation, and analytical skills while learning about the environmental impacts of food production.\n\nReference:\nThe Economist and Solstad, S., 2023. The Economist’s Banana index. First published in the article “A different way to measure the climate impact of food”, The Economist, April 11, 2023."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day5.html#setup",
    "href": "course-materials/eod-practice/eod-day5.html#setup",
    "title": "Day 5: Tasks & Activities",
    "section": "Setup",
    "text": "Setup\nFirst, let’s import the necessary libraries and load the data:\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\nurl = \"https://github.com/TheEconomist/banana-index-data/releases/download/1.0/bananaindex.csv\"\ndf = pd.read_csv(url)\n\n\n\n\nCode\n# Display the first few rows:\nprint(df.head())\n\n\n          entity  year  emissions_kg  emissions_1000kcal  \\\n0            Ale  2022      0.488690            0.317338   \n1  Almond butter  2022      0.387011            0.067265   \n2    Almond milk  2022      0.655888            2.222230   \n3        Almonds  2022      0.602368            0.105029   \n4    Apple juice  2022      0.458378            0.955184   \n\n   emissions_100g_protein  emissions_100g_fat  land_use_kg  land_use_1000kcal  \\\n0                0.878525            2.424209     0.811485           0.601152   \n1                0.207599            0.079103     7.683045           1.296870   \n2               13.595512            4.057470     1.370106           2.675063   \n3                0.328335            0.119361     8.230927           1.423376   \n4               29.152212           19.754980     0.660629           1.382839   \n\n   Land use per 100 grams of protein  Land use per 100 grams of fat  \\\n0                           1.577687                       3.065766   \n1                           3.608433                       1.495297   \n2                          12.687839                       4.600530   \n3                           4.261040                       1.610136   \n4                          43.232158                      26.246743   \n\n   Bananas index (kg)  Bananas index (1000 kcalories)  \\\n0            0.559558                        0.362340   \n1            0.443134                        0.076804   \n2            0.751002                        2.537364   \n3            0.689721                        0.119923   \n4            0.524851                        1.090638   \n\n   Bananas index (100g protein)  Chart?  type       Banana values  Unnamed: 16  \n0                      0.113771    True     1              Per KG     0.873350  \n1                      0.026885    True     1  Per 1000 kcalories     0.875803  \n2                      1.760651    True     1    Per 100g protein     7.721869  \n3                      0.042520    True     1                 NaN          NaN  \n4                      3.775280    True     1                 NaN          NaN  \n\n\n\n\nCode\n# Display the dataframe info:\nprint(df.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 160 entries, 0 to 159\nData columns (total 17 columns):\n #   Column                             Non-Null Count  Dtype  \n---  ------                             --------------  -----  \n 0   entity                             160 non-null    object \n 1   year                               160 non-null    int64  \n 2   emissions_kg                       160 non-null    float64\n 3   emissions_1000kcal                 160 non-null    float64\n 4   emissions_100g_protein             158 non-null    float64\n 5   emissions_100g_fat                 160 non-null    float64\n 6   land_use_kg                        160 non-null    float64\n 7   land_use_1000kcal                  160 non-null    float64\n 8   Land use per 100 grams of protein  158 non-null    float64\n 9   Land use per 100 grams of fat      160 non-null    float64\n 10  Bananas index (kg)                 160 non-null    float64\n 11  Bananas index (1000 kcalories)     160 non-null    float64\n 12  Bananas index (100g protein)       160 non-null    float64\n 13  Chart?                             160 non-null    bool   \n 14  type                               160 non-null    int64  \n 15  Banana values                      3 non-null      object \n 16  Unnamed: 16                        3 non-null      float64\ndtypes: bool(1), float64(12), int64(2), object(2)\nmemory usage: 20.3+ KB\nNone"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day5.html#tasks",
    "href": "course-materials/eod-practice/eod-day5.html#tasks",
    "title": "Day 5: Tasks & Activities",
    "section": "Tasks",
    "text": "Tasks\n\n1. Data Preparation\n\nSet the index of the DataFrame to be the ‘entity’ column.\nRemove the ‘year’, ‘Banana values’, ‘type’, ‘Unnamed: 16’, and ‘Chart?’ columns.\nDisplay the first few rows of the modified DataFrame.\n\n\n\n2. Exploring Banana Scores\n\nFor each of the pre-computed banana score columns (kg, calories, and protein), show the 10 highest-scoring food products.\nEdit the function below so that is returns the top 10 scores for a given column:\n\n\n\nCode\ndef return_top_ten(df, column):\n    \"\"\" Return the top 10 values of a column \"\"\"\n    pass\n\n\n\n\n\n\n\n\nreturn values from functions\n\n\n\nThe pass in our function is a temporary statement that allows the function to execute but not do anything. You need to remove the pass statement and add a return statement that provides the necessary functionality. For example, if the function was supposed to add 2 to every value of a column, you’d delete the pass statement and add return df[column] * 2\n\n\n\nUse your function to display the results for each of the three Banana index columns.\n\n\n\n3. Common High-Scoring Foods\nIdentify which foods, if any, appear in the top 10 for all three banana score lists (kg, calories, and protein).\n\n\n\n\n\n\nUnpacking iterables using the * operator\n\n\n\nPython sets allow you to quickly determine intersections: in_all_three = set.intersection(seta, setb, setc), or you can use the * operator to unpack a list of sets directly: in_all_three = set.intersection(*list_of_sets)\n\n\n\n\n4. Land Use Analysis\n\nCreate a new column named ‘Bananas index (land use 1000 kcal)’, calculating that food item’s use of land for every 1,000 kcal in comparison to a banana.\n\n\n\n\n\n\n\nTip\n\n\n\nThe data on land_use_1000kcal for bananas is found as the entry for this column in the Bananas row.\n\n\n\nDisplay the 10 foods with the highest land use score.\nCompare this list with the previous top 10 lists. Are there any common foods?\n\n\n\n5. Cheese Analysis\nIdentify the type of cheese with the highest banana score per 1,000 kcal. How does it compare to other cheeses in the dataset?"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day5.html#conclusion",
    "href": "course-materials/eod-practice/eod-day5.html#conclusion",
    "title": "Day 5: Tasks & Activities",
    "section": "Conclusion",
    "text": "Conclusion\nSummarize your findings from this analysis. What insights have you gained about the environmental impact of different foods?\nReflection Questions: - Which pandas methods for data selection and filtering did you find most useful? - How did the data cleaning and manipulation techniques help you explore the dataset? - What patterns did you notice in the environmental impact data? - What aspects of pandas data manipulation do you want to practice more?\n\n\n\n\n\n\nLooking Ahead\n\n\n\nIn Day 7, we’ll learn about creating visualizations with both matplotlib and seaborn to make charts and plots that help communicate these environmental impact findings more effectively!\n\n\n\nEnd Activity Session (Day 5)"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day6.html",
    "href": "course-materials/eod-practice/eod-day6.html",
    "title": "Day 6: Tasks & Activities",
    "section": "",
    "text": "In this exercise, you’ll analyze Eurovision Song Contest data using pandas. You’ll practice grouping, joining, and date manipulation techniques to explore trends in the contest’s history while building your data analysis skills."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day6.html#setup",
    "href": "course-materials/eod-practice/eod-day6.html#setup",
    "title": "Day 6: Tasks & Activities",
    "section": "Setup",
    "text": "Setup\nFirst, import the necessary libraries and load the dataset:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load the dataset\nurl = \"https://github.com/Spijkervet/eurovision-dataset/releases/download/2020.0/contestants.csv\"\neurovision_df = pd.read_csv(url)"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day6.html#task-1-data-exploration-and-cleaning",
    "href": "course-materials/eod-practice/eod-day6.html#task-1-data-exploration-and-cleaning",
    "title": "Day 6: Tasks & Activities",
    "section": "Task 1: Data Exploration and Cleaning",
    "text": "Task 1: Data Exploration and Cleaning\n\nDisplay the first few rows of the dataset.\nCheck the data types of each column.\nIdentify and handle any missing values.\nConvert the ‘year’ column to datetime type."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day6.html#task-2-filtering-and-transformation",
    "href": "course-materials/eod-practice/eod-day6.html#task-2-filtering-and-transformation",
    "title": "Day 6: Tasks & Activities",
    "section": "Task 2: Filtering and Transformation",
    "text": "Task 2: Filtering and Transformation\n\nCreate a new dataframe containing only data from 1990 onwards\n\n\n\n\n\n\n\nImportant\n\n\n\nUse .copy() to make sure you create a new dataframe and not just a view.\n\n\n\nCalculate the difference between final points and semi-final points for each entry and make a histogram of these values using the builtin dataframe .hist() command."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day6.html#task-3-sorting-and-aggregation",
    "href": "course-materials/eod-practice/eod-day6.html#task-3-sorting-and-aggregation",
    "title": "Day 6: Tasks & Activities",
    "section": "Task 3: Sorting and Aggregation",
    "text": "Task 3: Sorting and Aggregation\n\nFind the top 10 countries with the most Eurovision appearances (use the entire dataset for this calculation)\nCalculate the average final points for each country across all years. Make a simple bar plot of these data.\n\n\n\n\n\n\n\nNote\n\n\n\nUse value_counts() for counting appearances and groupby() for calculating averages."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day6.html#task-4-grouping-and-analysis",
    "href": "course-materials/eod-practice/eod-day6.html#task-4-grouping-and-analysis",
    "title": "Day 6: Tasks & Activities",
    "section": "Task 4: Grouping and Analysis",
    "text": "Task 4: Grouping and Analysis\n\nDetermine the country with the highest average final points for each decade.\n\n\n\n\n\n\n\nHint: Grouping Years in Pandas\n\n\n\nWhen working with time series data, it’s often useful to group years into larger intervals like decades, 5-year periods, etc. Here’s a general approach using pandas:\n\nFor decades (10-year intervals):\ndf['decade'] = df['year'].dt.year // 10 * 10\nFor any N-year interval:\nN = 5  # Change this to your desired interval (e.g., 2, 5, 10, 20)\ndf['year_group'] = df['year'].dt.year // N * N\nFor more specific date ranges:\ndf['custom_group'] = pd.cut(df['year'], \n                            bins=[1990, 1995, 2000, 2005, 2010], \n                            labels=['1990-1994', '1995-1999', '2000-2004', '2005-2009'])\n\nRemember: - // is integer division (rounds down) - Multiplying by the interval after division ensures the start year of each group\nThese methods create a new column that you can use with groupby() for aggregations across your chosen time intervals."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day6.html#task-5-joining-data",
    "href": "course-materials/eod-practice/eod-day6.html#task-5-joining-data",
    "title": "Day 6: Tasks & Activities",
    "section": "Task 5: Joining Data",
    "text": "Task 5: Joining Data\n\nRead in a new dataframe that contains population data stored at this url:\n\n\n\nCode\npopulation_url = 'https://bit.ly/euro_pop'\n\n\n\nJoin this data with the Eurovision dataframe.\n\n\n\n\n\n\n\nWarning\n\n\n\nEnsure that country names match exactly between the two dataframes before joining.\n\n\n\nCalculate total entries per capita by country.\nSubsteps:\n3a. Create a new dataframe containing the counts of entries for each county (use value_counts)\n3b. Merge the dataframe of counts of entries for each country with the population dataframe.\n3c. Calculate entries per million population (using entries per million to make the numbers easier to work with)\n3d. Sort the results by entries per capita\n3e. Print the top 10 values"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day6.html#task-6-time-series-analysis",
    "href": "course-materials/eod-practice/eod-day6.html#task-6-time-series-analysis",
    "title": "Day 6: Tasks & Activities",
    "section": "Task 6: Time Series Analysis",
    "text": "Task 6: Time Series Analysis\n\nPlot the trend of maximum final points awarded over the years.\nIdentify any significant changes in the scoring system based on this trend.\n\n(This step simply requires visual interpretation of the plot, but perhaps you could explore if there are actual rules changes underlying observed patterns using google)"
  },
  {
    "objectID": "course-materials/eod-practice/eod-day6.html#task-7-choose-your-own-analysis",
    "href": "course-materials/eod-practice/eod-day6.html#task-7-choose-your-own-analysis",
    "title": "Day 6: Tasks & Activities",
    "section": "Task 7: Choose your own analysis!",
    "text": "Task 7: Choose your own analysis!\nCome up with your own analysis of the Eurovision data that reveals some pattern across the data or through time. Feel free to discuss your ideas with others; often this leads to new ideas or refinement of ones you are already working on."
  },
  {
    "objectID": "course-materials/eod-practice/eod-day6.html#reflection",
    "href": "course-materials/eod-practice/eod-day6.html#reflection",
    "title": "Day 6: Tasks & Activities",
    "section": "Reflection",
    "text": "Reflection\nNow that you’ve completed the Eurovision data analysis exercise, it’s time to reflect on your experience. Add a new markdown cell to your notebook and answer the following questions:\n\nGrouping Operations: Which grouping tasks felt most natural to you? How did breaking complex operations into individual steps help your understanding?\nData Joining: What challenges did you encounter when merging the Eurovision and population datasets? How did the step-by-step approach help?\nDate Manipulation: How comfortable did you feel working with the datetime operations (like creating decades)?\nVisualization with matplotlib: How did creating plots help you understand the patterns in the data?\nReal-world Applications: How could you apply these grouping, joining, and date manipulation skills to environmental data science problems?\nMost Interesting Discovery: What was the most surprising pattern you found in the Eurovision data?\n\n\n\n\n\n\n\nLooking Ahead to Day 7\n\n\n\nTomorrow we’ll learn about advanced visualization with seaborn, which will allow you to create even more sophisticated plots to explore patterns in your data!\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemember, breaking complex operations into clear steps makes your code more readable and helps you debug problems more easily - both crucial skills for data scientists!\n\n\nRemember to document your code, explain your reasoning, and interpret the results of your analysis throughout the exercise."
  },
  {
    "objectID": "course-materials/interactive-sessions/1d_operators_functions.html",
    "href": "course-materials/interactive-sessions/1d_operators_functions.html",
    "title": "Interactive Session 1D",
    "section": "",
    "text": "variables.jpg\nAll programming languages contain the same fundamental tools: variables, operators, and functions. This session covers a first introduction to each of these these basic elements of the Python language."
  },
  {
    "objectID": "course-materials/interactive-sessions/1d_operators_functions.html#getting-started",
    "href": "course-materials/interactive-sessions/1d_operators_functions.html#getting-started",
    "title": "Interactive Session 1D",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab (see the note below if you have trouble!)\nSelect “Rename”\nName your notebook with the format: Session_1D_Operators_and_Functions.ipynb (Replace X with the day number and Y with the session number)\n\n\n\n\n\n\n\n\nRight-clicking in JupyterLab\n\n\n\nSome browsers and operating system combinations will not conceded right-clicking to the JupyterLab interface and will show a system menu when you try to right click. In those cases, usually CTRL-Right Click or OPTION-Right Click will bring up the Jupyter menu.\n\n\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content:\n\n\n# Day 1: Session D - Operators & Functions\n\n[Session Webpage](https://eds-217-essential-python.github.io/course-materials/interactive-sessions/1d_operators_functions.html)\n\nDate: 09/03/2024\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "course-materials/interactive-sessions/1d_operators_functions.html#instructions",
    "href": "course-materials/interactive-sessions/1d_operators_functions.html#instructions",
    "title": "Interactive Session 1D",
    "section": "Instructions",
    "text": "Instructions\nWe will work through this material together, writing a new notebook as we go.\n\n\n\n\n\n\nNote\n\n\n\n🐍     This symbol designates an important note about Python structure, syntax, or another quirk.\n\n\n\n✏️     This symbol designates code you should add to your notebook and run."
  },
  {
    "objectID": "course-materials/interactive-sessions/1d_operators_functions.html#variables-operators",
    "href": "course-materials/interactive-sessions/1d_operators_functions.html#variables-operators",
    "title": "Interactive Session 1D",
    "section": "Variables + Operators",
    "text": "Variables + Operators\nVariables are used in Python to create references to an object (e.g. string, float, DataFrame, etc.). Variables are assigned in Python using =.\n\n\n\n\n\n\nNote\n\n\n\n🐍 Variable names should be chosen carefully and should indicate what the variable is used for. Python etiquette generally dictates using lowercase variable names. Underscores are common. Variable names cannot start with a number. Also, there are several names that cannot be used as variables, as they are reserved for built-in Python commands, functions, etc. We will see examples of these throughout this session.\n\n\n\n\n\nNumbers\nNumbers in Python can be either integers (whole numbers) or floats (floating-point decimal numbers).\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Define variables x and y as integers.\nx = 1\ny = 42\n\n\nThe following syntax is used to define a float:\na = 1.0\nb = 42.0\nc = 23.782043\n\n✏️ Try it. Create a new cell and define variables a, b, and c according to the values above.\n\n\n\nCode\n# Define variables a, b, and c as floats.\na = 1.0\nb = 42.0\nc = 23.782043\n\n\n\n\nArithmetic Operators\nJust like a calculator, basic arithmetic can be done on number variables. Python uses the following symbols:\n\n\n\nSymbol\nTask\n\n\n\n\n+\nAddition\n\n\n-\nSubtraction\n\n\n*\nMultiplication\n\n\n/\nDivision\n\n\n%\nModulus\n\n\n//\nFloor division\n\n\n**\nPower\n\n\n\n\n✏️ Try it. Practice using arithmetic operations by running the code in a new cell. Feel free to add more to test the operators. Use the print() command to output your answers.\n\n\n\nCode\n# Do some math and print the results!\n\n\nNotice that the order of operations applies.\n\n\nCompound Assignment Operators\nCompound assignment operators combine an arithmetic or bitwise operation with assignment in a single statement. They provide a concise way to update a variable’s value based on its current value.\n\nExamples\n\n\nCode\n# Initialize a variable\ncount = 10\n\n# Decrement using compound assignment\ncount -= 1\nprint(f\"After count -= 1: {count}\")  # Output: 9\n\n# Increment using compound assignment\ncount += 2\nprint(f\"After count += 2: {count}\")  # Output: 11\n\n# Multiply using compound assignment\ncount *= 3\nprint(f\"After count *= 3: {count}\")  # Output: 33\n\n\nAfter count -= 1: 9\nAfter count += 2: 11\nAfter count *= 3: 33\n\n\n\n\nExercise\nComplete the following code to achieve the desired output:\n\n\nCode\n# Initialize the score\nscore = 100\n\n# TODO: Use compound assignment to decrease the score by 15\n# Your code here\n\n# TODO: Use compound assignment to double the score\n# Your code here\n\n# TODO: Use compound assignment to divide the score by 5\n# Your code here\n\nprint(f\"Final score: {score}\")  # Expected output: 34.0\n\n\nFinal score: 100\n\n\nTry modifying the initial score value or the operations to see how the result changes!\n\n\n\nBoolean Operators\nBoolean operators evaluate a condition between two operands, returning True if the condition is met and False otherwise. True and False are called booleans.\n\n\n\nSymbol\nTask\n\n\n\n\n==\nEquals\n\n\n!=\nDoes not equal\n\n\n&lt;\nLess than\n\n\n&gt;\nGreater than\n\n\n&lt;=\nLess than or equal to\n\n\n&gt;=\nGreater than or equal to\n\n\n\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nprint(b &gt;= a)\nprint(87 &lt; -2)\nprint(c != 0)\nprint(y == x)\n\n\nTrue\nFalse\nTrue\nFalse\n\n\n\n Built-in Functions \n\nPython has a number of built-in functions. Here we will introduce a few of the useful built-in functions for numerical variables.\nThe type() function is used to check the data type of a variable. For numerical arguments, either float or int is returned.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nprint(type(x))\nprint(type(c))\n\n\n&lt;class 'int'&gt;\n&lt;class 'float'&gt;\n\n\nThe isinstance() function is used to determine whether an argument is in a certain class. It returns a boolean value. Multiple classes can be checked at once.\nisinstance(12, int)\n&gt;&gt;&gt; True\n\nisinstance(12.0, int)\n&gt;&gt;&gt; False\n\nisinstance(12.0, (int, float))\n&gt;&gt;&gt; True\nThe commands int() and float() are used to convert between data types.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nprint(float(y))\nprint(int(c))\n\n\n42.0\n23\n\n\nNotice that when converting a float value to an integer, the int() command always rounds down to the nearest whole number.\nTo round a float to the nearest whole number, use the function round(). You can specify the number of decimal places by adding an integer as an argument to the round() function.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nprint(round(c))\nprint(round(c, 3))\n\n\n24\n23.782\n\n\nTo return the absolute value of a number, use the abs() function.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nprint(abs(c))\nprint(abs(-12))\n\n\n23.782043\n12\n\n\nThe pow() function is an alternative to the ** operator for raising a number to an exponent, i.e. \\(x^y\\).\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\npow(8, 2)\n\n\n64"
  },
  {
    "objectID": "course-materials/interactive-sessions/1d_operators_functions.html#strings",
    "href": "course-materials/interactive-sessions/1d_operators_functions.html#strings",
    "title": "Interactive Session 1D",
    "section": "Strings",
    "text": "Strings\nPieces of text in Python are referred to as strings. Strings are defined with either single or double quotes. The only difference between the two is that it is easier to use an apostrophe with double quotes.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nmytext = 'This is a string.'\nmytext2 = \"This is also a string.\"\n\n\nTo use an apostrophe or single quotes inside a string defined by single quotes (or to use double quotes), use a single backslash ( \\ ) referred to as an “escape” character.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nq1a = \"What is Newton's 1st law of motion?\"\nq1b = 'What is Newton\\'s 1st law of motion?'\n\n# Are q1a and q1b the same?\nq1a == q1b\n\n\nTrue\n\n\nPython has multi-line strings as well, which you can use when documenting your code or handling large quotes or chunks of text. Multi-line strings are started with three quotes (\"\"\") and terminated with three quotes (\"\"\"):\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\n# l(a or (A leaf falls on loneliness) by e.e. cummings\nla = \"\"\"\nl(a\nle\naf\nfa\nll\ns)\none\nl\niness\n\"\"\"\n\n\nMulti-line formatting is preserved in multi-line strings:\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nprint(la)\n\n\n\nl(a\nle\naf\nfa\nll\ns)\none\nl\niness\n\n\n\n\nString Built-in Functions\nJust like the int() and float() commands, the str() command converts a number to a string.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nystr = str(y)\n\n\nThe + operator can be used to combine two or more strings.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\ns = 'isaac' + ' ' + 'newton'\n\n\nThe commands string.upper() and string.capitalize() can be used to convert all letters in the string to uppercase and capitalize the first letter in the string, respectively.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nprint(s.upper())\nprint(s.capitalize())\n\n\nISAAC NEWTON\nIsaac newton"
  },
  {
    "objectID": "course-materials/interactive-sessions/1d_operators_functions.html#formatted-print-statements",
    "href": "course-materials/interactive-sessions/1d_operators_functions.html#formatted-print-statements",
    "title": "Interactive Session 1D",
    "section": "Formatted Print Statements",
    "text": "Formatted Print Statements\nPython’s f-string formatting provides an efficient and readable way to create formatted strings. This is useful for printing variables, formatting numerical output, and displaying messages.\n\nUsing f-strings\nTo use an f-string, place an f before the opening quotation mark of a string literal, and then include variables inside curly braces {}.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nname = \"Alice\"\nage = 30\n\n# Example of an f-string\nprint(f\"My name is {name} and I am {age} years old.\")\n\n\nMy name is Alice and I am 30 years old.\n\n\n\n\nFormatting Numbers\nYou can also format numbers, especially floating-point numbers, within f-strings by specifying format specifiers inside the curly braces:\n\nFixed Point: Use .nf to display a float with n decimal places.\n\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\npi = 3.141592653589793\n\n# Format to 2 decimal places\nprint(f\"The value of pi is approximately {pi:.2f}.\")\n\n\nThe value of pi is approximately 3.14.\n\n\n\nWidth and Alignment: Use formatting options to align text or numbers.\n\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nvalue = 123.456\nprint(f\"Value aligned to 10 spaces: |{value:10.2f}|\")\n\n\nValue aligned to 10 spaces: |    123.46|\n\n\n\n\n\nEnd interactive session 1D"
  },
  {
    "objectID": "course-materials/interactive-sessions/6c_dates.html",
    "href": "course-materials/interactive-sessions/6c_dates.html",
    "title": "Interactive Session 6C",
    "section": "",
    "text": "Your turn: Try parsing the following dates: ‘2023-07-04 14:30:00’, ‘05/07/2023’, ‘June 6th, 2023’\nCode\n# add your code here!"
  },
  {
    "objectID": "course-materials/interactive-sessions/6c_dates.html#getting-started",
    "href": "course-materials/interactive-sessions/6c_dates.html#getting-started",
    "title": "Interactive Session 6C",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "course-materials/interactive-sessions/6c_dates.html#introduction",
    "href": "course-materials/interactive-sessions/6c_dates.html#introduction",
    "title": "Interactive Session 6C",
    "section": "Introduction",
    "text": "Introduction\nIn this interactive session, we’ll explore in more detail to work with dates in pandas, which is a crucial skill for environmental data scientists. We’ll focus on:\n\nParsing dates\nUsing dates as an index for a DataFrame\nSelecting and filtering data based on date ranges\n\nHopefully, by the end of this session, you’ll be more comfortable manipulating time series data in pandas."
  },
  {
    "objectID": "course-materials/interactive-sessions/6c_dates.html#setting-up",
    "href": "course-materials/interactive-sessions/6c_dates.html#setting-up",
    "title": "Interactive Session 6C",
    "section": "Setting Up",
    "text": "Setting Up\nFirst, let’s import the necessary libraries and create a sample dataset.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set a random seed for reproducibility\nnp.random.seed(42)\n\n# Generate a sample dataset\ndate_rng = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\ntemperature = np.random.normal(loc=15, scale=5, size=len(date_rng))\nrainfall = np.random.exponential(scale=5, size=len(date_rng))\n\ndf = pd.DataFrame(data={'date': date_rng, 'temperature': temperature, 'rainfall': rainfall})\nprint(df.head())\n\n\n        date  temperature   rainfall\n0 2023-01-01    17.483571   1.265410\n1 2023-01-02    14.308678  16.514351\n2 2023-01-03    18.238443   0.061145\n3 2023-01-04    22.615149  17.512635\n4 2023-01-05    13.829233   0.220595"
  },
  {
    "objectID": "course-materials/interactive-sessions/6c_dates.html#parsing-dates",
    "href": "course-materials/interactive-sessions/6c_dates.html#parsing-dates",
    "title": "Interactive Session 6C",
    "section": "Parsing Dates",
    "text": "Parsing Dates\nPandas provides powerful tools for parsing dates. Let’s explore some common scenarios:\n\nUsing pd.to_datetime()\nThe pd.to_datetime() function is versatile and can handle various date formats.\n\n\n\n\n\n\nImportant\n\n\n\nIf you are parsing strings with different date formats, use the format='mixed' keyword argument to pd.datetime()\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Parse dates in different formats\ndates = ['2023-07-01', '7/2/23', 'July 3, 2023']\nparsed_dates = pd.to_datetime(dates, format='mixed')\nprint(parsed_dates)\n\n\nDatetimeIndex(['2023-07-01', '2023-07-02', '2023-07-03'], dtype='datetime64[ns]', freq=None)\n\n\n\n\nSpecifying Date Format\nSometimes, you need to specify the format explicitly:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\ncustom_dates = ['01-Jul-2023', '02-Jul-2023', '03-Jul-2023']\nparsed_custom_dates = pd.to_datetime(custom_dates, format='%d-%b-%Y')\nprint(parsed_custom_dates)\n\n\nDatetimeIndex(['2023-07-01', '2023-07-02', '2023-07-03'], dtype='datetime64[ns]', freq=None)"
  },
  {
    "objectID": "course-materials/interactive-sessions/6c_dates.html#date-format-strings-cheatsheet",
    "href": "course-materials/interactive-sessions/6c_dates.html#date-format-strings-cheatsheet",
    "title": "Interactive Session 6C",
    "section": "Date Format Strings Cheatsheet",
    "text": "Date Format Strings Cheatsheet\nCommon format codes for dates and times:\n\n%Y: Year with century as a decimal number (e.g., 2023)\n%y: Year without century as a zero-padded decimal number (e.g., 23)\n%m: Month as a zero-padded decimal number (01-12)\n%d: Day of the month as a zero-padded decimal number (01-31)\n%H: Hour (24-hour clock) as a zero-padded decimal number (00-23)\n%M: Minute as a zero-padded decimal number (00-59)\n%S: Second as a zero-padded decimal number (00-59)\n%f: Microsecond as a decimal number, zero-padded on the left (000000-999999)\n\nAdditional useful codes:\n\n%b: Month as locale’s abbreviated name (e.g., Jan, Feb)\n%B: Month as locale’s full name (e.g., January, February)\n%a: Weekday as locale’s abbreviated name (e.g., Sun, Mon)\n%A: Weekday as locale’s full name (e.g., Sunday, Monday)\n%j: Day of the year as a zero-padded decimal number (001-366)\n%U: Week number of the year (Sunday as the first day of the week)\n%W: Week number of the year (Monday as the first day of the week)\n\nCommon combinations:\n\n%Y-%m-%d: ISO date format (e.g., 2023-05-15)\n%d/%m/%Y: Common date format in some countries (e.g., 15/05/2023)\n%Y-%m-%d %H:%M:%S: ISO date and time format (e.g., 2023-05-15 14:30:00)\n\nRemember, when using these format strings with pandas, you typically use them with functions like pd.to_datetime() or when setting the date_format parameter in read_csv()."
  },
  {
    "objectID": "course-materials/interactive-sessions/6c_dates.html#using-dates-as-dataframe-index",
    "href": "course-materials/interactive-sessions/6c_dates.html#using-dates-as-dataframe-index",
    "title": "Interactive Session 6C",
    "section": "Using Dates as DataFrame Index",
    "text": "Using Dates as DataFrame Index\nSetting the date column as the index can make time-based operations more intuitive:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Set 'date' as the index\ndf = df.set_index('date')\nprint(df.head())\n\n\n            temperature   rainfall\ndate                              \n2023-01-01    17.483571   1.265410\n2023-01-02    14.308678  16.514351\n2023-01-03    18.238443   0.061145\n2023-01-04    22.615149  17.512635\n2023-01-05    13.829233   0.220595\n\n\nOnce the index is set to a datetime, you can use the .loc selector to locate specific data in the dataframe:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Accessing data for a specific date\nprint(df.loc['2023-01-15'])\n\n\ntemperature    6.375411\nrainfall       3.030639\nName: 2023-01-15 00:00:00, dtype: float64\n\n\n\nResampling Time Series Data\nWith dates as the index, we can easily resample our data using the resample command and a resampling interval.\n\n\n\n\n\n\nCommon Datetime Arguments for resample\n\n\n\n\n'D': Calendar day\n'W': Week (Sunday)\n'W-MON': Week (Monday)\n'ME': Month end\n'MS': Month start\n'QE': Quarter end\n'QS': Quarter start\n'YE' or 'AE': Year end\n'YS' or 'AS': Year start\n'H': Hourly\n'T' or 'min': Minutely\n'S': Secondly\n\nNote: You can also use multiples, e.g., 2D for every 2 days, 4H for every 4 hours.\n\n\n\n\n\n\n\n\nChanges in monthly resampling options\n\n\n\nThe M, ME, and MS options relate to monthly resampling:\n\nM (Deprecated): This used to represent the end of the month. It’s being phased out due to ambiguity.\nME (Month End): This is the direct replacement for M. It explicitly represents the last day of each month. When you resample with ME, the resulting timestamps will be on the last day of each month.\nMS (Month Start): This represents the first day of each month. When you resample with MS, the resulting timestamps will be on the first day of each month.\n\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Resample to monthly average\nmonthly_avg = df.resample('ME').mean()\nprint(monthly_avg.head())\n\n# Plot monthly average temperature\nplt.figure(figsize=(12, 6))\nmonthly_avg['temperature'].plot()\nplt.title('Monthly Average Temperature')\nplt.xlabel('Date')\nplt.ylabel('Temperature (°C)')\nplt.show()\n\n\n            temperature  rainfall\ndate                             \n2023-01-31    13.992562  6.531653\n2023-02-28    14.284158  7.272248\n2023-03-31    15.219692  6.984496\n2023-04-30    14.898742  4.785105\n2023-05-31    14.580927  3.228182\n\n\n\n\n\n\n\n\n\nYour turn: Try resampling the data to get weekly maximum rainfall. Plot the results."
  },
  {
    "objectID": "course-materials/interactive-sessions/6c_dates.html#selecting-and-filtering-by-date-ranges",
    "href": "course-materials/interactive-sessions/6c_dates.html#selecting-and-filtering-by-date-ranges",
    "title": "Interactive Session 6C",
    "section": "Selecting and Filtering by Date Ranges",
    "text": "Selecting and Filtering by Date Ranges\nPandas makes it easy to select data for specific date ranges. You can use list and array style slicing to get a range of dates from a datetime index.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Select data for the first quarter of 2023\nq1_data = df['2023-01-01':'2023-03-31']\nprint(q1_data.head())\n\n\n            temperature   rainfall\ndate                              \n2023-01-01    17.483571   1.265410\n2023-01-02    14.308678  16.514351\n2023-01-03    18.238443   0.061145\n2023-01-04    22.615149  17.512635\n2023-01-05    13.829233   0.220595\n\n\nDatetime objects contain a number of methods and attributes that provide information about them.\n\n\n\n\n\n\nDatetime Attributes:\n\n\n\n\n.year: Year of the datetime\n.month: Month of the datetime (1-12)\n.day: Day of the month\n.hour: Hour (0-23)\n.minute: Minute (0-59)\n.second: Second (0-59)\n.weekday(): Day of the week (0-6, where 0 is Monday)\n\n\n\nYou can use these attributes to easily filter datetime indicies:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Filter for summer months (June, July, August)\nsummer_data = df[(df.index.month &gt;= 6) & (df.index.month &lt;= 8)]\nprint(summer_data.head())\n\n\n            temperature  rainfall\ndate                             \n2023-06-01    16.732241  6.896786\n2023-06-02    11.599876  1.019898\n2023-06-03    16.161268  1.174495\n2023-06-04    16.465362  2.313926\n2023-06-05    11.428243  3.313313\n\n\n\nUseful Datetime Methods:\n\n.date(): Returns the date part of the datetime\n.time(): Returns the time part of the datetime\n.strftime(format): Converts datetime to string according to given format\n.isoformat(): Returns a string representation of the date in ISO 8601 format\n\n\n\nMore examples of datetime selections:\n\n\nCode\n# Select data for a specific year\ndf_2023 = df[df.index.year == 2023]\nprint(\"Shape of 2023 data:\", df_2023.shape)\n\n\nShape of 2023 data: (365, 2)\n\n\n\n\nCode\n# Select data for a specific month\ndf_june = df[df.index.month == 6]\nprint(\"Shape of June data:\", df_june.shape)\n\n\nShape of June data: (30, 2)\n\n\n\n\nCode\n# Select data for weekdays only (weekdays are zero-indexed!)\ndf_weekdays = df[df.index.weekday &lt; 5]\nprint(\"Shape of weekday data:\", df_weekdays.shape)\n\n\nShape of weekday data: (260, 2)\n\n\n\n\nCode\n# Select data for a specific date range\ndf_q2 = df['2023-04-01':'2023-06-30']\nprint(\"Shape of Q2 data:\", df_q2.shape)\n\n\nShape of Q2 data: (91, 2)\n\n\n\n\nCode\n# Resample to monthly frequency\ndf_monthly = df.resample('MS').mean()\nprint(\"Shape of monthly data:\", df_monthly.shape)\n\n\nShape of monthly data: (12, 2)\n\n\n\n\nCode\n# Demonstrate some useful datetime methods\nprint(\"\\nFirst date in ISO format:\", df.index[0].isoformat())\n\nprint(\"Last date formatted:\", df.index[-1].strftime('%B %d, %Y'))\n\n\n\nFirst date in ISO format: 2023-01-01T00:00:00\nLast date formatted: December 31, 2023\n\n\nYour turn: Select data for all Mondays in the dataset. Calculate the average temperature for Mondays.\n\n✏️ Try it. Add the cell below to your notebook and write some code to solve the problem above.\n\n\n\nCode\n# Add your code here!"
  },
  {
    "objectID": "course-materials/interactive-sessions/6c_dates.html#key-points",
    "href": "course-materials/interactive-sessions/6c_dates.html#key-points",
    "title": "Interactive Session 6C",
    "section": "Key Points",
    "text": "Key Points\n\nUse pd.to_datetime() to parse dates in various formats.\nSetting dates as the DataFrame index enables powerful time-based operations.\nResampling allows you to change the frequency of your time series data.\nPandas provides flexible ways to select and filter data based on date ranges."
  },
  {
    "objectID": "course-materials/interactive-sessions/6c_dates.html#resources",
    "href": "course-materials/interactive-sessions/6c_dates.html#resources",
    "title": "Interactive Session 6C",
    "section": "Resources",
    "text": "Resources\n\nPandas Time Series documentation\nDatetime documentation\n[EDS 217 Cheatsheet: Working with Dates in Pandas]\n\nRemember, working with dates and time series is a fundamental skill in environmental data science. Practice these concepts with your own datasets to become proficient! ::: {.center-text .body-text-xl .teal-text} End interactive session 6C :::"
  },
  {
    "objectID": "course-materials/interactive-sessions/5b_cleaning_data.html#getting-started",
    "href": "course-materials/interactive-sessions/5b_cleaning_data.html#getting-started",
    "title": "Interactive Session 5B",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "course-materials/interactive-sessions/5b_cleaning_data.html#introduction-to-data-cleaning",
    "href": "course-materials/interactive-sessions/5b_cleaning_data.html#introduction-to-data-cleaning",
    "title": "Interactive Session 5B",
    "section": "Introduction to Data Cleaning",
    "text": "Introduction to Data Cleaning\nData cleaning is a crucial step in the data science workflow. It involves identifying and correcting errors, inconsistencies, and inaccuracies in datasets to ensure the quality and reliability of your analysis.\nIn this session, we’ll explore common issues in dataframes and learn how to address them using pandas."
  },
  {
    "objectID": "course-materials/interactive-sessions/5b_cleaning_data.html#instructions",
    "href": "course-materials/interactive-sessions/5b_cleaning_data.html#instructions",
    "title": "Interactive Session 5B",
    "section": "Instructions",
    "text": "Instructions\nWe will work through this material together, writing a new notebook as we go.\n\n✏️     This symbol designates code you should add to your notebook and run.\n\n🤓 Where useful, this session contains links to Pandas Tutor, which helps you to visualize the chained functions in the accompanying code block.\n\n\nLet’s start by importing pandas and creating a sample dataframe with some issues we’ll need to clean:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Create a sample dataframe with issues\ndata = {\n    'species': ['Oak', 'Pine', 'Maple', 'Oak', 'Pine', None],\n    'height_m': [5.2, 12.0, '7.5', 5.2, 15.0, 8.1],\n    'diameter_cm': [20, 35, 25, 20, 40, np.nan],\n    'location': ['Park A', 'Park B', 'Park A', 'Park A', 'Park B', 'Park C '],\n    'date_planted': ['2020-01-15', '2019-05-20', '2020-03-10', '2020-01-15', '2018-11-30', '2021-07-05']\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n\n\n  species height_m  diameter_cm location date_planted\n0     Oak      5.2         20.0   Park A   2020-01-15\n1    Pine     12.0         35.0   Park B   2019-05-20\n2   Maple      7.5         25.0   Park A   2020-03-10\n3     Oak      5.2         20.0   Park A   2020-01-15\n4    Pine     15.0         40.0   Park B   2018-11-30\n5    None      8.1          NaN  Park C    2021-07-05"
  },
  {
    "objectID": "course-materials/interactive-sessions/5b_cleaning_data.html#handling-missing-values",
    "href": "course-materials/interactive-sessions/5b_cleaning_data.html#handling-missing-values",
    "title": "Interactive Session 5B",
    "section": "Handling Missing Values",
    "text": "Handling Missing Values\n\nIdentifying Missing Values\nFirst, let’s check for missing values in our dataframe. For this we use the isnull() method on the dataframe.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(df.isnull())\n\n\n   species  height_m  diameter_cm  location  date_planted\n0    False     False        False     False         False\n1    False     False        False     False         False\n2    False     False        False     False         False\n3    False     False        False     False         False\n4    False     False        False     False         False\n5     True     False         True     False         False\n\n\nYou can see that the isnull() command returns a Booelan (True or False) value for each item in the dataframe. If the location (row, column) is empty, then the isnull() command will return True, otherwise it returns False.\nWe can apply the sum() method to the result of df.isnull() to see what columns have empty values in them.\n\n\n\n\n\n\nImportant\n\n\n\nThe axis argument is often used in pandas and numpy to indicate how an aggregation (e.g. sum()) should be applied. You should read this argument as an answer to the question:\n\nWhat should I apply this aggregation across, rows (axis 0) or columns (axis 1)?\n\ndf.sum(axis=0) adds up all the rows and returns a single sum for each column.\ndf.sum(axis=1) adds up all the columns and returns a single sum for each row.\nGenerally, aggregations over all rows are more useful than aggregations across all columns, so the default for pandas and numpy aggregations is to apply aggregations and dataframe operations assuming axis=0. However, as we’ll see, other commands default to axis=1.\nSome commands allow you to use alias string arguments (rows and columns), but this isn’t universal across the libarary.\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Use the axis argument as an integer (0)\nnull_values = df.isnull()\nprint(\"Using `axis=0`:\\n\",\n        null_values.sum(axis=0))\n\n# Show that this is the same as using the `axis='rows'` argument:\nprint(\"\\nUsing `axis='rows':\\n\",\n        null_values.sum(axis='rows'))\n\n# And that this is the same as the default behavior:\nprint(\"\\nUsing default arguments:\\n\",\n        null_values.sum())\n\n\nUsing `axis=0`:\n species         1\nheight_m        0\ndiameter_cm     1\nlocation        0\ndate_planted    0\ndtype: int64\n\nUsing `axis='rows':\n species         1\nheight_m        0\ndiameter_cm     1\nlocation        0\ndate_planted    0\ndtype: int64\n\nUsing default arguments:\n species         1\nheight_m        0\ndiameter_cm     1\nlocation        0\ndate_planted    0\ndtype: int64\n\n\nAs we requested, this command sums up all the rows in each column of null_values. Any False is a 0 and any True is a 1, so the result is the number of null values in each column of the dataframe.\n\nMethod chaining allows us to do both the finding of null values and the summing of values for all rows in each column with a single line of code\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Use method chaining to make our code more concise.\ndf.isnull().sum(axis='rows')\n\n\nspecies         1\nheight_m        0\ndiameter_cm     1\nlocation        0\ndate_planted    0\ndtype: int64\n\n\n🤓 Pandas Tutor\n\n\nDropping Missing Values\nWe can drop rows with missing values using the dropna() function:\n\n\nCode\ndf_dropped = df.dropna()\nprint(df_dropped)\n\n\n  species height_m  diameter_cm location date_planted\n0     Oak      5.2         20.0   Park A   2020-01-15\n1    Pine     12.0         35.0   Park B   2019-05-20\n2   Maple      7.5         25.0   Park A   2020-03-10\n3     Oak      5.2         20.0   Park A   2020-01-15\n4    Pine     15.0         40.0   Park B   2018-11-30\n\n\nNotice how we didn’t need to specify an axis - by default, dropna() operates on each row and removes and rows that are any missing values (the default is axis='rows').\n\n\nFilling Missing Values (Imputation)\nWhen you drop data (using methods like dropna() or drop()), you’re permanently removing information from your dataset.\nThis can potentially lead to:\n\nLoss of important insights\nBiased results\nReduced statistical power\nSmaller sample size, which can affect the reliability of your analysis\n\nImputation is the process of replacing missing values with substituted values. Instead of dropping rows or columns with missing data, you fill in the gaps.\n\nCommon imputation techniques include:\n\nMean/median/mode imputation\nForward fill or backward fill\nInterpolation\nUsing machine learning models to predict missing values\n\n\n\nOther techniques:\n\nCreating a “missing” category for categorical variables\nUsing algorithms that can handle missing data (like some decision tree-based methods)\nMultiple imputation for more rigorous statistical analysis\n\nWhen to consider alternatives:\n\nWhen missing data is not completely at random (MCAR)\nWhen you have a small dataset and can’t afford to lose samples\nWhen the missing data might contain important information about your problem\n\nYou can use any of the pandas Series aggregation commands to fill missing values instead of dropping the data.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Fill missing values with a specific value\ndf['species'] = df['species'].fillna('unknown')\n\nprint(df)\n\n\n   species height_m  diameter_cm location date_planted\n0      Oak      5.2         20.0   Park A   2020-01-15\n1     Pine     12.0         35.0   Park B   2019-05-20\n2    Maple      7.5         25.0   Park A   2020-03-10\n3      Oak      5.2         20.0   Park A   2020-01-15\n4     Pine     15.0         40.0   Park B   2018-11-30\n5  unknown      8.1          NaN  Park C    2021-07-05\n\n\nWe can even use aggregations to fill with values derived from our dataframe.\nFor example, let’s replace missing values of diameter_cm with the average value across all the rows.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Fill missing numeric values with the mean of the column\ndf['diameter_cm'] = df['diameter_cm'].fillna(df['diameter_cm'].mean())\n\n\n🤓 Pandas Tutor"
  },
  {
    "objectID": "course-materials/interactive-sessions/5b_cleaning_data.html#dealing-with-duplicates",
    "href": "course-materials/interactive-sessions/5b_cleaning_data.html#dealing-with-duplicates",
    "title": "Interactive Session 5B",
    "section": "Dealing with Duplicates",
    "text": "Dealing with Duplicates\n\nIdentifying and Removing Duplicate Rows\nLet’s check for and remove duplicate rows:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Check for duplicates\nprint(df.duplicated())\n\n\n0    False\n1    False\n2    False\n3     True\n4    False\n5    False\ndtype: bool\n\n\nIt looks like row 3 is a duplicate (it is the same as row 0). As before, we can see how many rows are duplicated by applying the sum command to the result of df.duplicated()\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\ndf.duplicated().sum()\n\n\nnp.int64(1)\n\n\n🤓 Pandas Tutor\nThe drop_duplicates() method returns a new dataframe that only contains the first row of any duplicated rows.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\ndf_no_duplicates = df.drop_duplicates()\nprint(df_no_duplicates)\n\n\n   species height_m  diameter_cm location date_planted\n0      Oak      5.2         20.0   Park A   2020-01-15\n1     Pine     12.0         35.0   Park B   2019-05-20\n2    Maple      7.5         25.0   Park A   2020-03-10\n4     Pine     15.0         40.0   Park B   2018-11-30\n5  unknown      8.1         28.0  Park C    2021-07-05\n\n\nThe extra entry for Oak no longer appears in df_no_duplicates.\n\nWhat if we wanted to simply get rid of the duplicates in our original df without having to make an entirely new dataframe? the inplace option allows for this with many pandas methods:\ndf.drop_duplicates(inplace=True)\n\n\n\n\n\n\nImportant\n\n\n\nWhile inplace=True can be useful when making changes to a dataframe without having to worry about creating a copy, you can’t do method chaining when using this argument.\n\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Make a copy of our dataframe\ndf2 = df.copy()\n# Remove the duplicates from df2 without making a new dataframe (save results back into df2)\ndf2.drop_duplicates(inplace=True)\nprint(df2)\n\n\n   species height_m  diameter_cm location date_planted\n0      Oak      5.2         20.0   Park A   2020-01-15\n1     Pine     12.0         35.0   Park B   2019-05-20\n2    Maple      7.5         25.0   Park A   2020-03-10\n4     Pine     15.0         40.0   Park B   2018-11-30\n5  unknown      8.1         28.0  Park C    2021-07-05\n\n\n\n\nHandling Duplicates Based on Specific Columns\nWe can also remove duplicates based on specific columns, in this case removing any rows that share the same species and location.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\ndf_unique_species = df.drop_duplicates(subset=['species', 'location'])\nprint(df_unique_species)\n\n\n   species height_m  diameter_cm location date_planted\n0      Oak      5.2         20.0   Park A   2020-01-15\n1     Pine     12.0         35.0   Park B   2019-05-20\n2    Maple      7.5         25.0   Park A   2020-03-10\n5  unknown      8.1         28.0  Park C    2021-07-05\n\n\nAlthough our two Pines weren’t duplciates (their height_m, diameter_cm, and date_planted were different), we still dropped them from the dataframe based on the subset of columns (species and location)\n🤓 Pandas Tutor"
  },
  {
    "objectID": "course-materials/interactive-sessions/5b_cleaning_data.html#data-type-conversion-and-consistency",
    "href": "course-materials/interactive-sessions/5b_cleaning_data.html#data-type-conversion-and-consistency",
    "title": "Interactive Session 5B",
    "section": "Data Type Conversion and Consistency",
    "text": "Data Type Conversion and Consistency\n\nChecking and Changing Data Types\nOften datasets - especially those collected by surveys or forms - contain a mixture of data types (i.e. some strings mixed in with mostly numbers)\nLet’s check the data types of our columns and convert them as needed:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(df.dtypes)\n\n\nspecies          object\nheight_m         object\ndiameter_cm     float64\nlocation         object\ndate_planted     object\ndtype: object\n\n\nThe object datatype is a generic term meaning “something, I don’t know what” in python (remember, in python everything is an object).\nGenerally, we want our data types to be something more specific, like a floating point number, an integer, a string, or a date. We can use the astype() method to coerce our data into a specific kind of thing.\n\nIn older versions of pandas, string columns were always still listed as type object. They are functionally str objects, but pandas isn’t storing them in any special “pandas” way, so they are just generic python objects. Newer versions of pandas allow you to create string (note: not the same as str) data types. They are optimized for use in pandas, although you will rarely see any difference in performance, it’s good practice to use them when you can.\n\nLet’s convert height_m to float.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Convert 'height_m' to float\ndf['height_m'] = df['height_m'].astype(float)\nprint(df.dtypes)\n\n\nspecies          object\nheight_m        float64\ndiameter_cm     float64\nlocation         object\ndate_planted     object\ndtype: object\n\n\nConverting generic objects to datetime is more complicated. In fact, we’ll have an entire session later this class on working with dates. Pandas has a helper function - pd.to_datetime() - that tries to infer dates from values in columns.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Convert 'date_planted' to datetime\ndf['date_planted'] = pd.to_datetime(df['date_planted'])\n\nprint(df.dtypes)\n\n\nspecies                 object\nheight_m               float64\ndiameter_cm            float64\nlocation                object\ndate_planted    datetime64[ns]\ndtype: object"
  },
  {
    "objectID": "course-materials/interactive-sessions/5b_cleaning_data.html#string-manipulation-and-formatting",
    "href": "course-materials/interactive-sessions/5b_cleaning_data.html#string-manipulation-and-formatting",
    "title": "Interactive Session 5B",
    "section": "String Manipulation and Formatting",
    "text": "String Manipulation and Formatting\nWe can use string methods to clean text data. We access these methods using the .str attribute that is part of every pandas Series.\n\n\n\n\n\n\nNote\n\n\n\nRemember, every column in a DataFrame is a Series\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(\"'unkown' should be capitalized\")\nprint(df)\n\n# Capitalize species names (unknown -&gt; Unknown)\ndf['species'] = df['species'].str.capitalize()\n\nprint(\"\\nFixed it!\")\nprint(df)\n\n\n'unkown' should be capitalized\n   species  height_m  diameter_cm location date_planted\n0      Oak       5.2         20.0   Park A   2020-01-15\n1     Pine      12.0         35.0   Park B   2019-05-20\n2    Maple       7.5         25.0   Park A   2020-03-10\n3      Oak       5.2         20.0   Park A   2020-01-15\n4     Pine      15.0         40.0   Park B   2018-11-30\n5  unknown       8.1         28.0  Park C    2021-07-05\n\nFixed it!\n   species  height_m  diameter_cm location date_planted\n0      Oak       5.2         20.0   Park A   2020-01-15\n1     Pine      12.0         35.0   Park B   2019-05-20\n2    Maple       7.5         25.0   Park A   2020-03-10\n3      Oak       5.2         20.0   Park A   2020-01-15\n4     Pine      15.0         40.0   Park B   2018-11-30\n5  Unknown       8.1         28.0  Park C    2021-07-05\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(\"'Park C ' should be 'Park C'\")\nprint(df)\n\n# Remove leading/trailing whitespace from location\n# \"Park C \" -&gt; \"Park C\"\ndf['location'] = df['location'].str.strip()\n\nprint(\"\\nFixed it!\")\nprint(df)\n\n\n'Park C ' should be 'Park C'\n   species  height_m  diameter_cm location date_planted\n0      Oak       5.2         20.0   Park A   2020-01-15\n1     Pine      12.0         35.0   Park B   2019-05-20\n2    Maple       7.5         25.0   Park A   2020-03-10\n3      Oak       5.2         20.0   Park A   2020-01-15\n4     Pine      15.0         40.0   Park B   2018-11-30\n5  Unknown       8.1         28.0  Park C    2021-07-05\n\nFixed it!\n   species  height_m  diameter_cm location date_planted\n0      Oak       5.2         20.0   Park A   2020-01-15\n1     Pine      12.0         35.0   Park B   2019-05-20\n2    Maple       7.5         25.0   Park A   2020-03-10\n3      Oak       5.2         20.0   Park A   2020-01-15\n4     Pine      15.0         40.0   Park B   2018-11-30\n5  Unknown       8.1         28.0   Park C   2021-07-05"
  },
  {
    "objectID": "course-materials/interactive-sessions/5b_cleaning_data.html#wrap-up-and-best-practices",
    "href": "course-materials/interactive-sessions/5b_cleaning_data.html#wrap-up-and-best-practices",
    "title": "Interactive Session 5B",
    "section": "Wrap-up and Best Practices",
    "text": "Wrap-up and Best Practices\nIn this session, we’ve covered essential techniques for cleaning dataframes in pandas: - Handling missing values - Dealing with duplicates - Converting data types - String manipulation and formatting\nRemember these best practices:\n\nAlways examine your data before and after cleaning steps.\nRemember that the default for most operations is to act across all rows (axis=0).\nDocument your cleaning steps for reproducibility.\nBe cautious when dropping data - sometimes imputation or other techniques might be more appropriate.\n\nFor more advanced cleaning techniques and in-depth explanations, refer to the pandas documentation Pandas Documentation, the master Pandas Cheat Sheet, or our class Cleaning Data Cheatsheet\n\nEnd interactive session 5B"
  },
  {
    "objectID": "course-materials/interactive-sessions/2a_lists.html",
    "href": "course-materials/interactive-sessions/2a_lists.html",
    "title": "Interactive Session 2A",
    "section": "",
    "text": "Unsplash list image\nPython has four collection data types, the most common of which is the list. This session introduces lists and a few of the important list operations. We will also cover indexing, a key feature of programming."
  },
  {
    "objectID": "course-materials/interactive-sessions/2a_lists.html#getting-started",
    "href": "course-materials/interactive-sessions/2a_lists.html#getting-started",
    "title": "Interactive Session 2A",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_2A_Lists.ipynb\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day 2: Session A - Lists\n\n[Link to session webpage](https://eds-217-essential-python.github.io/course-materials/interactive-sessions/2a_lists.html)\n\nDate: 09/04/2024\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "course-materials/interactive-sessions/2a_lists.html#session-1.2-topics",
    "href": "course-materials/interactive-sessions/2a_lists.html#session-1.2-topics",
    "title": "Interactive Session 2A",
    "section": "Session 1.2 Topics",
    "text": "Session 1.2 Topics"
  },
  {
    "objectID": "course-materials/interactive-sessions/2a_lists.html#instructions",
    "href": "course-materials/interactive-sessions/2a_lists.html#instructions",
    "title": "Interactive Session 2A",
    "section": "Instructions",
    "text": "Instructions\n\n\n\n\n\n\nNote\n\n\n\n🐍     This symbol designates an important note about Python structure, syntax, or another quirk.\n\n\n\n✏️     This symbol designates code you should add to your notebook and run."
  },
  {
    "objectID": "course-materials/interactive-sessions/2a_lists.html#lists",
    "href": "course-materials/interactive-sessions/2a_lists.html#lists",
    "title": "Interactive Session 2A",
    "section": "Lists",
    "text": "Lists\nA list is a Python object used to contain multiple values. Lists are ordered and changeable. They are defined as follows:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Define list variables\nnum_list = [4, 23, 654, 2, 0, -12, 4391]\nstr_list = ['energy', 'water', 'carbon']\n\n\nWhile you can create lists containing mixed data types, this is not usually recommended.\nThe len() command returns the length of the list.\n\n\nCode\nlen(str_list)\n\n\n3\n\n\nThe min() and max() commands are used to find the minimum and maximum values in a list. For a list of strings, this corresponds to the alphabetically first and last elements.\n\n\nCode\nmin(str_list)\n\n\n'carbon'\n\n\n\n\nCode\nmax(str_list)\n\n\n'water'\n\n\n\n✏️ Try it. Use the len(), min(), and max() commands to find the length, minimum, and maximum of num_list.\n\n\nIndexing\nThe index is used to reference a value in an iterable object by its position. To access an element in a list by its index, use square brackets [].\n\n\n\n\n\n\nNote\n\n\n\n🐍 Python is zero-indexed. This means that the first element in the list is 0, the second is 1, and so on. The last element in a list with \\(n\\) elements is \\(n - 1\\).\n\n\nnum_list = [4, 23, 654, 2, 0, -12, 4391]\n\n\nCode\nnum_list[2]\n\n\n654\n\n\nYou can also access an element based on its position from the end of the list.\nnum_list = [4, 23, 654, 2, 0, -12, 4391]\n\n\nCode\nnum_list[-2]\n\n\n-12\n\n\n\n✏️ Try it. Find the 2nd element in str_list in two different ways. Remember that Python is zero-indexed!\n\n\n\nSlicing\nAccessing a range of values in a list is called slicing. A slice specifies a start and an endpoint, generating a new list based on the indices. The indices are separated by a :.\n\n\n\n\n\n\nNote\n\n\n\n🐍 The endpoint index in the slice is exclusive. To slice to the end of the list, omit an endpoint.\n\n\nnum_list = [4, 23, 654, 2, 0, -12, 4391]\nThis code returns the 2nd and 3rd elements of the num_list\n\n\nCode\nnum_list[2:4]\n\n\n[654, 2]\n\n\nThis code would return everything from the 4th element to the end of the list:\n\n\nCode\nnum_list[3:]\n\n\n[2, 0, -12, 4391]\n\n\n\n✏️ Try it. Before running each of the following commands in a new cell in your notebook, try to determine what output you expect\n\nnum_list = [4, 23, 654, 2, 0, -12, 4391]\n\n\nCode\nnum_list[2:6]\n\n\n\n\nCode\nnum_list[0:4]   \n\n\n\n\nCode\nnum_list[:4]    \n\n\n\n\nCode\nnum_list[-6:-1] \n\n\nAlthough less common, it is also possible to specify a step size, i.e. [start:stop:step]. A step size of 1 would select every element, 2 would select every other element, etc…\n\n\nCode\nnum_list[0:4:2]  \n\n\n[4, 654]\n\n\n\n\nCode\nnum_list[::2]\n\n\n[4, 654, 0, 4391]\n\n\nA step of -1 returns the list in reverse.\n\n\nCode\nnum_list[::-1]\n\n\n[4391, -12, 0, 2, 654, 23, 4]\n\n\n\n\nString indicies\nLike lists, strings can also be indexed using the same notation. This can be useful for many applications, such as selecting files in a certain folder for import based on their names or extension.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nword_str = 'antidisestablishmentarianism'\n\n\n\n\nCode\nword_str[14]\n\n\n's'\n\n\n\n\nCode\nword_str[::3]\n\n\n'aistlhnrnm'\n\n\n\n\nList Operations\nElements can be added to a list using the command list.append().\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\ncolors = ['red', 'blue', 'green', 'black', 'white', 'pink']\n\n\nYou can add an element to a list in a specific position using the command list.insert(i, x) where i is the position where the element x should be added.\n\n✏️  Try it.  Add ‘purple’ to the list colors between ‘green’ and ‘black’.\n\nThere are multiple ways to remove elements from a list. The commands list.pop() and del remove elements based on indices.\ncolors.pop()       # removes the last element\ncolors.pop(2)      # removes the third element\ndel colors[2]      # removes the third element\ndel colors[2:4]    # removes the third and fourth elements\nThe command list.remove() removes an element based on its value.\ncolors.remove('red')\nprint(colors)\n&gt;&gt;&gt; ['blue', 'green', 'black', 'purple', 'white', 'pink']\n\n\nLet’s reset our color list:\n\n\nCode\ncolors = ['red', 'blue', 'green', 'purple', 'black', 'white', 'pink']\n\n\n\n✏️  Try it.  Remove pink and purple from colors, using del for one of the strings and list.remove() for the other.\n\n\n\nSorting Lists\nYou can sort the elements in a list (numerically or alphabetically) in two ways. The first uses the command list.sort().\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nrand_list = [5.1 , 3.42 , 3.333 , 100.4 , 0.5 , 26.0 , 7.44 , 5.8 , 39.0]\nrand_list.sort()\nprint(rand_list)\n\n\n[0.5, 3.333, 3.42, 5.1, 5.8, 7.44, 26.0, 39.0, 100.4]\n\n\nSetting reverse=True within this command sorts the list in reverse order:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nrand_list = [5.1 , 3.42 , 3.333 , 100.4 , 0.5 , 26.0 , 7.44 , 5.8 , 39.0]\nrand_list.sort(reverse=True)\nprint(rand_list)\n\n\n[100.4, 39.0, 26.0, 7.44, 5.8, 5.1, 3.42, 3.333, 0.5]\n\n\nSo far, all of the list commands we’ve used have been in-place operators. This means that they perform the operation to the variable in place without requiring a new variable to be assigned. By contrast, standard operators do not change the original list variable. A new variable must be set in order to retain the operation.\n\n✏️  Try it.  Verify that rand_list was, in fact, sorted in place by using the min() and max() functions to determine the minmum and maximum values in the list and printing the first and last values in the list.\n\n\n\nCode\n# Step 1: Use f-strings to print the min and max values in rand_list.\n\n# Step 2: Use f-strings to print the first and last values in rand_list.\n\n\nThe other method of sorting a list is to use the sorted() command, which does not change the original list. Instead, the sorted list must be assigned to a new variable.\n\n\nCode\nrand_list = [5.1 , 3.42 , 3.333 , 100.4 , 0.5 , 26.0 , 7.44 , 5.8 , 39.0]\nsorted_list = sorted(rand_list)\nprint(rand_list[0])\nprint(sorted_list[0])\n\n\n5.1\n0.5\n\n\nTo avoid changing the original variable when using an in-place operator, it is wise to create a copy. There are multiple ways to create copies of lists, but it is important to know the difference between a true copy and a view.\n\n\n\n\n\n\nImportant\n\n\n\nThe difference between a copy and a view is a critical topic in Python and something we will come back to later in the class when working with DataFrames. Python only makes a copy of data when it is necessary, so make sure you understand the difference!\n\n\nA view of a list can be created as follows:\n\n\nCode\nstr_list = ['energy', 'water', 'carbon']\nstr_list_view = str_list\n\n\nAny in-place operation performed on str_list_view will also be applied to str_list. For example, look what happens when you run this code:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Sorting the str_list_view...\nstr_list_view.sort()\n\n# Actually sorts the original str_list!\nprint(str_list)\n\n\n['carbon', 'energy', 'water']\n\n\n\n\n\n\n\n\nImportant\n\n\n\nstr_list_view was just a “view” into to the str_list variable, but a copy!\n\n\nTo avoid this, create a copy of str_list using any of the following methods:\n\n\nCode\nstr_list_copy = str_list.copy()\n# or\nstr_list_copy = str_list[:]\n# or\nstr_list_copy = list(str_list)\n\n\nIn addition to adding single elements to a list using list.append() or list.insert(), multiple elements can be added to a list at the same time by adding multiple lists together.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nrainbow  = ['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet']\nshades = ['coral', 'chartreuse', 'cyan', 'navy']\nprint( rainbow + shades )\n\n\n['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet', 'coral', 'chartreuse', 'cyan', 'navy']\n\n\nSingle lists can be repeated by multiplying by an integer.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nstr_list2 = str_list * 2\nnum_list4 = num_list * 4\nprint( str_list2 )\nprint( num_list4 )\n\n\n['carbon', 'energy', 'water', 'carbon', 'energy', 'water']\n[4, 23, 654, 2, 0, -12, 4391, 4, 23, 654, 2, 0, -12, 4391, 4, 23, 654, 2, 0, -12, 4391, 4, 23, 654, 2, 0, -12, 4391]\n\n\n\n\nGenerating sequential lists\nSequential lists are valuable tools, particularly for iteration, which we will explore in later sessions. The range() function is used to create an iterable object based on the size of an integer argument.\nrange(4)\n&gt;&gt;&gt; range(0, 4)\nTo construct a sequential list from the range() object, use the list() function.\nlist(range(4))\n&gt;&gt;&gt; [0, 1, 2, 3]\nUsing multiple integer arguments, the range() function can be used to generate sequential lists between two bounds: range(start, stop [, step]).\n\n\n\n\n\n\nNote\n\n\n\n🐍 Like indexing, all Python functions using  start  and  stop  arguments, the  stop  value is  exclusive .\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nrange_10 = list(range(1,11))\nodds_10 = list(range(1,11,2))\nprint(range_10)\nprint(odds_10)\n\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n[1, 3, 5, 7, 9]\n\n\n\nEnd interactive session 2A"
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#getting-started",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#getting-started",
    "title": "Interactive Session 6A",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#learning-objectives",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#learning-objectives",
    "title": "Interactive Session 6A",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe goals of this session are to:\n\nBe able to use basic sorting methods to organize dataframes\nUnderstand the concept of grouping data in Pandas\nUse the groupby() function to create groups\nApply aggregate functions to grouped data\nPerform multi-level grouping\nReshape data using pivot tables"
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#first-steps",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#first-steps",
    "title": "Interactive Session 6A",
    "section": "First Steps",
    "text": "First Steps\nAs always, let’s start our notebook by loading our necessary libaries.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nimport pandas as pd"
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#basic-sorting-in-pandas",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#basic-sorting-in-pandas",
    "title": "Interactive Session 6A",
    "section": "Basic Sorting in Pandas",
    "text": "Basic Sorting in Pandas\nBefore we dive into grouping and aggregation, let’s cover some fundamental sorting operations in Pandas. Sorting your data can provide valuable insights and is often a precursor to more complex data manipulations.\n\nSorting by a Single Column\nTo sort a DataFrame by a single column, we use the sort_values() method:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Create a sample DataFrame\ndf = pd.DataFrame({\n    'date': pd.date_range(start='2023-01-01', periods=5),\n    'temperature': [20, 18, 22, 19, 21],\n    'precipitation': [0, 5, 2, 8, 3]\n})\n\nprint(\"Original DataFrame:\")\nprint(df)\n\n\nOriginal DataFrame:\n        date  temperature  precipitation\n0 2023-01-01           20              0\n1 2023-01-02           18              5\n2 2023-01-03           22              2\n3 2023-01-04           19              8\n4 2023-01-05           21              3\n\n\n\nSorting by temperature (default is in ascending order)\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Sort by temperature\ndf_sorted = df.sort_values('temperature')\nprint(\"\\nSorted by temperature (ascending):\")\nprint(df_sorted)\n\n\n\nSorted by temperature (ascending):\n        date  temperature  precipitation\n1 2023-01-02           18              5\n3 2023-01-04           19              8\n0 2023-01-01           20              0\n4 2023-01-05           21              3\n2 2023-01-03           22              2\n\n\nBy default, sort_values() sorts in ascending order. To sort in descending order, use the ascending parameter:\n\n\nSorting in descending order\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Sort by temperature in descending order\ndf_sorted_desc = df.sort_values('temperature', ascending=False)\nprint(\"Sorted by temperature (descending):\")\nprint(df_sorted_desc)\n\n\nSorted by temperature (descending):\n        date  temperature  precipitation\n2 2023-01-03           22              2\n4 2023-01-05           21              3\n0 2023-01-01           20              0\n3 2023-01-04           19              8\n1 2023-01-02           18              5\n\n\n\n\n\nSorting by Multiple Columns\nYou can sort by multiple columns by passing a list of column names:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Sort by precipitation (ascending) and then temperature (descending)\ndf_multi_sorted = df.sort_values(['precipitation', 'temperature'], ascending=[True, False])\nprint(\"Sorted by precipitation (asc) and temperature (desc):\")\nprint(df_multi_sorted)\n\n\nSorted by precipitation (asc) and temperature (desc):\n        date  temperature  precipitation\n0 2023-01-01           20              0\n2 2023-01-03           22              2\n4 2023-01-05           21              3\n1 2023-01-02           18              5\n3 2023-01-04           19              8\n\n\nIn this example, we first sort by precipitation in ascending order, and then by temperature in descending order for any rows with the same precipitation value.\n\n\nSorting Index\nIf you want to sort by the index instead of a column, use sort_index():\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Sort by index\ndf_index_sorted = df.sort_index(ascending=False)\nprint(\"Sorted by index (descending):\")\nprint(df_index_sorted)\n\n\nSorted by index (descending):\n        date  temperature  precipitation\n4 2023-01-05           21              3\n3 2023-01-04           19              8\n2 2023-01-03           22              2\n1 2023-01-02           18              5\n0 2023-01-01           20              0\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember, by default sorting operations return a new DataFrame and don’t modify the original unless you use inplace=True."
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#introduction-to-grouping",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#introduction-to-grouping",
    "title": "Interactive Session 6A",
    "section": "Introduction to Grouping",
    "text": "Introduction to Grouping\nGrouping data is a powerful technique in data analysis that allows us to split a DataFrame into groups based on some criteria, apply a function to each group independently, and combine the results. This is particularly useful when we want to calculate summary statistics for different categories within our data.\nLet’s start with a simple example using a dataset of environmental measurements across different locations and times.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Create a sample DataFrame\ndata = {\n    'location': ['A', 'B', 'A', 'B', 'A', 'B'],\n    'date': pd.date_range(start='2023-01-01', periods=6),\n    'temperature': [20, 22, 19, 24, 21, 23],\n    'humidity': [50, 48, 52, 45, 49, 47]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n\n\n  location       date  temperature  humidity\n0        A 2023-01-01           20        50\n1        B 2023-01-02           22        48\n2        A 2023-01-03           19        52\n3        B 2023-01-04           24        45\n4        A 2023-01-05           21        49\n5        B 2023-01-06           23        47"
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#using-groupby",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#using-groupby",
    "title": "Interactive Session 6A",
    "section": "Using groupby()",
    "text": "Using groupby()\nThe groupby() function is the core of grouping operations in Pandas. It allows us to split the data into groups based on one or more columns.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Group by location\ngrouped = df.groupby('location')\n\n# Calculate mean for each group\nprint(grouped.mean())\n\n\n               date  temperature   humidity\nlocation                                   \nA        2023-01-03         20.0  50.333333\nB        2023-01-04         23.0  46.666667"
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#aggregating-data",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#aggregating-data",
    "title": "Interactive Session 6A",
    "section": "Aggregating Data",
    "text": "Aggregating Data\nWe can apply various aggregation functions to our grouped data. Some common ones include mean(), sum(), count(), min(), max(), etc.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Multiple aggregations\nprint(grouped.agg(['mean', 'min', 'max']))\n\n\n               date                       temperature           humidity      \\\n               mean        min        max        mean min max       mean min   \nlocation                                                                       \nA        2023-01-03 2023-01-01 2023-01-05        20.0  19  21  50.333333  49   \nB        2023-01-04 2023-01-02 2023-01-06        23.0  22  24  46.666667  45   \n\n              \n         max  \nlocation      \nA         52  \nB         48  \n\n\nNow it’s your turn! Try to calculate the standard deviation of temperature and humidity for each location.\n\n✏️ Try it. Add the cell below to your notebook and then provide your code.\n\n\n\nCode\n# Your code here"
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#multi-level-grouping",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#multi-level-grouping",
    "title": "Interactive Session 6A",
    "section": "Multi-level Grouping",
    "text": "Multi-level Grouping\nWe can group by multiple columns to create a hierarchical index.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Add a 'year' column\ndf['year'] = df['date'].dt.year\n\n# Group by location and year\nmulti_grouped = df.groupby(['location', 'year'])\n\nprint(multi_grouped.mean())\n\n\n                    date  temperature   humidity\nlocation year                                   \nA        2023 2023-01-03         20.0  50.333333\nB        2023 2023-01-04         23.0  46.666667\n\n\n\nUnderstanding Groupby Objects\nAfter using the groupby() function, it’s important to understand what kind of object we’re working with and how it differs from a regular DataFrame. Let’s explore this in more detail.\n\nThe Groupby Object\nWhen you apply the groupby() function to a DataFrame, the result is a DataFrameGroupBy object. This object is not a DataFrame itself, but rather a special object that contains information about the groups.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Create a groupby object\ngrouped = df.groupby('location')\n\n# Check the type of the grouped object\nprint(type(grouped))\n\n# Try to view the grouped object\nprint(grouped)\n\n\n&lt;class 'pandas.core.groupby.generic.DataFrameGroupBy'&gt;\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x112300810&gt;\n\n\nAs you can see, simply printing the groupby object doesn’t show us the data. Instead, it gives us information about the groupby operation.\n\n\n\nAccessing Group Data\nTo actually see the data in each group, we need to iterate over the groups or use aggregation functions.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Iterate over groups\nfor name, group in grouped:\n    print(f\"Group: {name}\")\n    print(group)\n    print()\n\n# Using an aggregation function\nprint(grouped.mean())\n\n\nGroup: A\n  location       date  temperature  humidity  year\n0        A 2023-01-01           20        50  2023\n2        A 2023-01-03           19        52  2023\n4        A 2023-01-05           21        49  2023\n\nGroup: B\n  location       date  temperature  humidity  year\n1        B 2023-01-02           22        48  2023\n3        B 2023-01-04           24        45  2023\n5        B 2023-01-06           23        47  2023\n\n               date  temperature   humidity    year\nlocation                                           \nA        2023-01-03         20.0  50.333333  2023.0\nB        2023-01-04         23.0  46.666667  2023.0\n\n\n\n\nKey Differences from DataFrames\n\nStructure: A groupby object is not a table-like structure like a DataFrame. It’s more like a container of groups.\nDirect Access: You can’t access columns or rows of a groupby object directly like you can with a DataFrame.\nLazy Evaluation: Groupby operations are lazy - they don’t actually compute anything until you call an aggregation function or iterate over the groups.\nAggregation Required: To get meaningful results from a groupby object, you typically need to apply some kind of aggregation function (like mean(), sum(), count(), etc.).\n\n\n\nConverting Groupby Results to DataFrame\nAfter applying an aggregation function to a groupby object, the result is typically a DataFrame:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Result of aggregation is a DataFrame\nresult = grouped.mean()\nprint(type(result))\n\n# We can now use DataFrame methods on this result\nprint(result.reset_index())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n  location       date  temperature   humidity    year\n0        A 2023-01-03         20.0  50.333333  2023.0\n1        B 2023-01-04         23.0  46.666667  2023.0\n\n\n\nPractice\nTry grouping the data by both ‘location’ and ‘year’, then calculate the maximum temperature for each group. What type of object do you get? How can you reset the index to make it a regular DataFrame?\n\n✏️ Try it. Add the cell below to your notebook and then provide your code.\n\n\n\nCode\n# Your code here\n\n\n\n\nKey Groupby Points\n\nA groupby object is not a DataFrame, but a special object containing group information.\nTo view data in a groupby object, you need to iterate over it or apply aggregation functions.\nGroupby operations are lazy and require aggregation to produce results.\nThe result of aggregating a groupby object is typically a DataFrame or Series, which you can then manipulate further."
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#reshaping-dataframes-with-pivot-tables",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#reshaping-dataframes-with-pivot-tables",
    "title": "Interactive Session 6A",
    "section": "Reshaping DataFrames with Pivot Tables",
    "text": "Reshaping DataFrames with Pivot Tables\nPivot tables provide a way to reshape data and calculate aggregations in one step.\n\nHow Pivot Tables Work\n\nReshaping Data: Pivot tables reshape data by turning unique values from one column into multiple columns.\nAggregation: They perform aggregations on a specified value column for each unique group created by the new columns.\nIndex and Columns: You specify which column to use as the new index, which to use as new columns, and which to aggregate.\n\nThe idea is very similar to the df.pivot command: \nThe main difference between df.pivot and df.pivot_table is that df.pivot_table includes aggregation.\nLet’s see an example:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Create a sample DataFrame\ndata = {\n    'date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02'],\n    'city': ['New York', 'Los Angeles', 'New York', 'Los Angeles'],\n    'temperature': [32, 68, 28, 72]\n}\ndf = pd.DataFrame(data)\nprint(\"Original DataFrame:\")\nprint(df)\n\n\nOriginal DataFrame:\n         date         city  temperature\n0  2023-01-01     New York           32\n1  2023-01-01  Los Angeles           68\n2  2023-01-02     New York           28\n3  2023-01-02  Los Angeles           72\n\n\n\nUsing df.pivot to rotate the dataframe:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\npivot = pd.pivot(df, values='temperature', index='date', columns='city')\nprint(\"\\nPivot:\")\nprint(pivot)\n\n\n\nPivot:\ncity        Los Angeles  New York\ndate                             \n2023-01-01           68        32\n2023-01-02           72        28\n\n\n\n\nUsing df.pivot_table to create a pivot table:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Create a pivot table\npivot_table = df.pivot_table(values='temperature', index='date', columns='city', aggfunc='mean')\nprint(\"\\nPivot Table:\")\nprint(pivot_table)\n\n\n\nPivot Table:\ncity        Los Angeles  New York\ndate                             \n2023-01-01         68.0      32.0\n2023-01-02         72.0      28.0\n\n\nIn this example: - ‘date’ becomes the index - ‘city’ values become new columns - ‘temperature’ values are aggregated (mean) for each date-city combination\n\n\n\n\n\n\nNote\n\n\n\nIn this example, the result of our pivot and pivot_table commands are essentially the same. Why is that the case? When would we expect different results from these two commands?\n\n\n\n\n\nKey Features of Pivot Tables\n\nHandling Duplicates: If there are multiple values for a given index-column combination, an aggregation function (like mean, sum, count) must be specified.\nMissing Data: Pivot tables can reveal missing data, often filling these gaps with NaN.\nMulti-level Index: You can create multi-level indexes and columns for more complex reorganizations.\nFlexibility: You can pivot on multiple columns and use multiple value columns.\n\nPivot tables are especially useful for: - Summarizing large datasets - Creating cross-tabulations - Preparing data for visualization - Identifying patterns or trends across categories\nRemember, while pivot tables are powerful, they work best with well-structured data and clear categorical variables.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Create a pivot table\npivot = df.pivot_table(values='temperature', index='city', columns='date', aggfunc='mean')\nprint(pivot)\n\n\ndate         2023-01-01  2023-01-02\ncity                               \nLos Angeles        68.0        72.0\nNew York           32.0        28.0\n\n\nTry creating a pivot table that shows the maximum humidity for each city and date.\n\n✏️ Try it. Add the cell below to your notebook and then provide your code.\n\n\n\nCode\n# Your code here"
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#key-points",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#key-points",
    "title": "Interactive Session 6A",
    "section": "Key Points",
    "text": "Key Points\n\nGrouping allows us to split data based on categories and perform operations on each group.\nThe groupby() function is the primary tool for grouping in Pandas.\nWe can apply various aggregation functions to grouped data.\nMulti-level grouping creates a hierarchical index.\nPivot tables offer a way to reshape and aggregate data simultaneously."
  },
  {
    "objectID": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#resources",
    "href": "course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#resources",
    "title": "Interactive Session 6A",
    "section": "Resources",
    "text": "Resources\n\nPandas Groupby Documentation\nPandas Pivot Table Documentation\n\nDon’t forget to check out our EDS 217 Cheatsheet on Grouping and Aggregating for quick reference!\n\nEnd interactive session 6A"
  }
]